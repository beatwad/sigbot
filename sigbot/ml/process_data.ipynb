{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'signal_stat/buy_stat_1h.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 136>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W1sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W1sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m \u001b[39m# Buy\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W1sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39m# dataset with the signal statistics\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W1sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_pickle(\u001b[39m'\u001b[39;49m\u001b[39msignal_stat/buy_stat_1h.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W1sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m \u001b[39m# dataset for model train\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W1sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m train_buy \u001b[39m=\u001b[39m create_train_df(df, \u001b[39m'\u001b[39m\u001b[39mbuy\u001b[39m\u001b[39m'\u001b[39m, configs, target_offset, first, last, step)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/pickle.py:189\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m4    4    9\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m excs_to_catch \u001b[39m=\u001b[39m (\u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mModuleNotFoundError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m)\n\u001b[0;32m--> 189\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    190\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    191\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    192\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    193\u001b[0m     is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    194\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    195\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    196\u001b[0m     \u001b[39m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[39m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[39m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[39m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    202\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    873\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'signal_stat/buy_stat_1h.pkl'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from indicators import indicators\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from config.config import ConfigFactory\n",
    "\n",
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"1h_4h\"\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "def get_file(ticker):\n",
    "    ''' Find files buy ticker names, file names can be in different formats '''\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def add_indicators(df, ttype, configs):\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, '', '', 0)\n",
    "    # add RSI\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, '', '', 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df = trend.get_indicator(df, '', '', 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df = macd.get_indicator(df, '', '', 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, '', '', 0)\n",
    "    # add SMA\n",
    "    # sma = indicators.SMA(ttype, configs)\n",
    "    # df = sma.get_indicator(df, '', '', 0)\n",
    "    return df\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step):\n",
    "    ''' Create train dataset from signal statistics and ticker candle data'''\n",
    "    train_df = pd.DataFrame()\n",
    "    tickers = df['ticker'].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df['ticker'] == ticker]\n",
    "        times = signal_df['time']\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        tmp_df_1h = add_indicators(tmp_df_1h, ttype, configs)\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc('pattern')]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h['time'] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + step, step):\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h['time'] == time_prev, :].reset_index(drop=True)\n",
    "                    row_tmp.columns = [c + f'_prev_{i}' for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row['ticker'] = ticker\n",
    "                row['pattern'] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "            \n",
    "            # add target\n",
    "            time_next = t + timedelta(hours=target_offset)\n",
    "            if ttype == 'buy':\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'high'].reset_index(drop=True)\n",
    "            else:\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'low'].reset_index(drop=True)\n",
    "\n",
    "            target.name = 'target'\n",
    "            rows = pd.concat([row, target], axis=1)\n",
    "            \n",
    "            # add data to the dataset\n",
    "            if train_df.shape[0] == 0:\n",
    "                train_df = rows\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, rows])\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "# for how long time (in hours) we want to predict\n",
    "target_offset = 24\n",
    "# first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "first = 4\n",
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 48\n",
    "# step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "step = 4\n",
    "\n",
    "# Buy\n",
    "# dataset with the signal statistics\n",
    "df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "# dataset for model train\n",
    "train_buy = create_train_df(df, 'buy', configs, target_offset, first, last, step)\n",
    "train_buy = train_buy.dropna()\n",
    "\n",
    "# Sell\n",
    "# dataset with the signal statistics\n",
    "df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "# dataset for model train\n",
    "train_sell = create_train_df(df, 'sell', configs, target_offset, first, last, step)\n",
    "train_sell = train_sell.dropna()\n",
    "\n",
    "train_df = pd.concat([train_buy, train_sell]).sort_values('time').reset_index(drop=True)\n",
    "display(train_df.head())\n",
    "display(train_df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "test_size=0.2\n",
    "\n",
    "x_data = torch.tensor(train_df.drop(['target', 'time', 'ticker', 'pattern'], axis=1).values, dtype=torch.float32)\n",
    "y_data = torch.tensor(train_df['target'].values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_data), type(y_data))\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SigModel(\n",
       "  (layers): Sequential(\n",
       "    (lin1): Linear(in_features=263, out_features=64, bias=True)\n",
       "    (relu1): ReLU()\n",
       "    (do1): Dropout(p=0.25, inplace=False)\n",
       "    (lin2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (relu2): ReLU()\n",
       "    (do2): Dropout(p=0.25, inplace=False)\n",
       "    (lin3): Linear(in_features=128, out_features=96, bias=True)\n",
       "    (relu3): ReLU()\n",
       "    (do3): Dropout(p=0.25, inplace=False)\n",
       "    (lin4): Linear(in_features=96, out_features=32, bias=True)\n",
       "    (relu4): ReLU()\n",
       "    (do4): Dropout(p=0.25, inplace=False)\n",
       "    (lin5): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigModel, self).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(263, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "    \n",
    "model = SigModel().to(device)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad(set_to_none=True) # ~ model.zero_grad()\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train)\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train).item()\n",
    "    \n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights according to gradient value\n",
    "    optimizer.step()\n",
    "\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid)\n",
    "        val_loss = criterion(val_preds, y_valid).item()\n",
    "    \n",
    "    return train_loss, val_loss\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Send data to the device\n",
    "x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "# Empty loss lists to track values\n",
    "epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss\n",
    "learning_rate = 0.003\n",
    "optmizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loop through the data\n",
    "for epoch in range(epochs):\n",
    "    train_loss, val_loss = train_epoch(model, train_loader, criterion, optmizer)\n",
    "\n",
    "    # Print progress a total of 20 times\n",
    "    if epoch % int(epochs / 20) == 0:\n",
    "        print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f} | Validation Loss: {val_loss:.5f}')\n",
    "\n",
    "        epoch_count.append(epoch)\n",
    "        train_loss_values.append(train_loss.detach().numpy())\n",
    "        valid_loss_values.append(val_loss.detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "plt.title('Training & Validation Loss Curves')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat \u001b[1m\u001b[34m#1\n",
      "[100]\ttraining's binary_logloss: 0.666445\tvalid_1's binary_logloss: 0.685064\n",
      "[200]\ttraining's binary_logloss: 0.653676\tvalid_1's binary_logloss: 0.682842\n",
      "[300]\ttraining's binary_logloss: 0.639283\tvalid_1's binary_logloss: 0.6797\n",
      "[400]\ttraining's binary_logloss: 0.626068\tvalid_1's binary_logloss: 0.677556\n",
      "[500]\ttraining's binary_logloss: 0.611998\tvalid_1's binary_logloss: 0.674979\n",
      "[600]\ttraining's binary_logloss: 0.605685\tvalid_1's binary_logloss: 0.673847\n",
      "[700]\ttraining's binary_logloss: 0.59495\tvalid_1's binary_logloss: 0.67257\n",
      "[800]\ttraining's binary_logloss: 0.587397\tvalid_1's binary_logloss: 0.67147\n",
      "[900]\ttraining's binary_logloss: 0.577149\tvalid_1's binary_logloss: 0.670331\n",
      "[1000]\ttraining's binary_logloss: 0.569968\tvalid_1's binary_logloss: 0.669587\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| loss: \u001b[1m\u001b[34m0.66959\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.666793\tvalid_1's binary_logloss: 0.682736\n",
      "[200]\ttraining's binary_logloss: 0.65397\tvalid_1's binary_logloss: 0.678806\n",
      "[300]\ttraining's binary_logloss: 0.639669\tvalid_1's binary_logloss: 0.674723\n",
      "[400]\ttraining's binary_logloss: 0.62666\tvalid_1's binary_logloss: 0.671453\n",
      "[500]\ttraining's binary_logloss: 0.612984\tvalid_1's binary_logloss: 0.668915\n",
      "[600]\ttraining's binary_logloss: 0.607452\tvalid_1's binary_logloss: 0.66793\n",
      "[700]\ttraining's binary_logloss: 0.597264\tvalid_1's binary_logloss: 0.666214\n",
      "[800]\ttraining's binary_logloss: 0.590612\tvalid_1's binary_logloss: 0.665309\n",
      "[900]\ttraining's binary_logloss: 0.581504\tvalid_1's binary_logloss: 0.66481\n",
      "[1000]\ttraining's binary_logloss: 0.574728\tvalid_1's binary_logloss: 0.664409\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| loss: \u001b[1m\u001b[34m0.66441\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.66746\tvalid_1's binary_logloss: 0.682265\n",
      "[200]\ttraining's binary_logloss: 0.655029\tvalid_1's binary_logloss: 0.677988\n",
      "[300]\ttraining's binary_logloss: 0.640831\tvalid_1's binary_logloss: 0.674263\n",
      "[400]\ttraining's binary_logloss: 0.62785\tvalid_1's binary_logloss: 0.670998\n",
      "[500]\ttraining's binary_logloss: 0.614009\tvalid_1's binary_logloss: 0.66681\n",
      "[600]\ttraining's binary_logloss: 0.608319\tvalid_1's binary_logloss: 0.665714\n",
      "[700]\ttraining's binary_logloss: 0.59802\tvalid_1's binary_logloss: 0.663525\n",
      "[800]\ttraining's binary_logloss: 0.590617\tvalid_1's binary_logloss: 0.66212\n",
      "[900]\ttraining's binary_logloss: 0.581368\tvalid_1's binary_logloss: 0.660651\n",
      "[1000]\ttraining's binary_logloss: 0.574616\tvalid_1's binary_logloss: 0.660102\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| loss: \u001b[1m\u001b[34m0.66010\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.667427\tvalid_1's binary_logloss: 0.682427\n",
      "[200]\ttraining's binary_logloss: 0.655495\tvalid_1's binary_logloss: 0.678842\n",
      "[300]\ttraining's binary_logloss: 0.641621\tvalid_1's binary_logloss: 0.675486\n",
      "[400]\ttraining's binary_logloss: 0.627673\tvalid_1's binary_logloss: 0.671859\n",
      "[500]\ttraining's binary_logloss: 0.613272\tvalid_1's binary_logloss: 0.669012\n",
      "[600]\ttraining's binary_logloss: 0.606952\tvalid_1's binary_logloss: 0.667978\n",
      "[700]\ttraining's binary_logloss: 0.596298\tvalid_1's binary_logloss: 0.666604\n",
      "[800]\ttraining's binary_logloss: 0.58854\tvalid_1's binary_logloss: 0.665903\n",
      "[900]\ttraining's binary_logloss: 0.577869\tvalid_1's binary_logloss: 0.664814\n",
      "[1000]\ttraining's binary_logloss: 0.570627\tvalid_1's binary_logloss: 0.663991\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| loss: \u001b[1m\u001b[34m0.66399\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.66728\tvalid_1's binary_logloss: 0.680892\n",
      "[200]\ttraining's binary_logloss: 0.654398\tvalid_1's binary_logloss: 0.675887\n",
      "[300]\ttraining's binary_logloss: 0.639729\tvalid_1's binary_logloss: 0.671025\n",
      "[400]\ttraining's binary_logloss: 0.626594\tvalid_1's binary_logloss: 0.66759\n",
      "[500]\ttraining's binary_logloss: 0.613475\tvalid_1's binary_logloss: 0.664394\n",
      "[600]\ttraining's binary_logloss: 0.607595\tvalid_1's binary_logloss: 0.663911\n",
      "[700]\ttraining's binary_logloss: 0.597489\tvalid_1's binary_logloss: 0.662263\n",
      "[800]\ttraining's binary_logloss: 0.589706\tvalid_1's binary_logloss: 0.660844\n",
      "[900]\ttraining's binary_logloss: 0.579497\tvalid_1's binary_logloss: 0.659459\n",
      "[1000]\ttraining's binary_logloss: 0.573034\tvalid_1's binary_logloss: 0.65867\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| loss: \u001b[1m\u001b[34m0.65867\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "\u001b[1m\u001b[31m Outer Holdout avg score: \u001b[0m log_loss: \u001b[1m\u001b[31m0.66317\u001b[0m\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "class CFG:\n",
    "    n_repeats = 1\n",
    "    n_folds = 5\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target'] >= df['close']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.01,\n",
    "          'early_stopping_round': 100,\n",
    "          'max_depth': 7,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree': 0.75,\n",
    "          'num_leaves': 32,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(train_df, permut=True, boruta=True)\n",
    "\n",
    "# perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "# boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "# feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "# res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "# res.groupby('Feature')['rank'].sum().sort_values().head(30).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature\n",
       "stoch_slowk_dir_prev_4        4.0\n",
       "stoch_slowd_dir_prev_4        9.5\n",
       "stoch_slowd_dir_prev_48      12.0\n",
       "macdhist_prev_40             13.0\n",
       "stoch_diff_prev_48           17.0\n",
       "stoch_slowk                  55.0\n",
       "rsi_prev_12                  87.5\n",
       "linear_reg_angle            145.0\n",
       "linear_reg_angle_prev_28    148.0\n",
       "stoch_slowd_prev_40         159.0\n",
       "stoch_slowk_prev_16         171.0\n",
       "rsi_prev_44                 173.0\n",
       "linear_reg_angle_prev_16    174.0\n",
       "linear_reg_prev_8           177.0\n",
       "stoch_slowd                 182.0\n",
       "rsi                         183.0\n",
       "linear_reg_angle_prev_24    185.0\n",
       "stoch_diff_prev_32          186.0\n",
       "stoch_diff                  186.0\n",
       "stoch_slowk_dir_prev_32     188.0\n",
       "stoch_slowk_dir_prev_48     189.0\n",
       "rsi_prev_48                 192.0\n",
       "rsi_prev_24                 196.0\n",
       "linear_reg_angle_prev_44    198.0\n",
       "rsi_prev_8                  203.0\n",
       "macdsignal_dir_prev_40      206.0\n",
       "stoch_slowk_prev_48         209.0\n",
       "stoch_slowd_prev_36         212.0\n",
       "linear_reg_angle_prev_20    213.0\n",
       "volume_prev_4               213.0\n",
       "Name: rank, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "res.groupby('Feature')['rank'].sum().sort_values().head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 30 features\n",
      "[100]\tvalid_0's binary_logloss: 0.680215\n",
      "[200]\tvalid_0's binary_logloss: 0.675149\n",
      "[300]\tvalid_0's binary_logloss: 0.671369\n",
      "[400]\tvalid_0's binary_logloss: 0.669378\n",
      "[500]\tvalid_0's binary_logloss: 0.667129\n",
      "[600]\tvalid_0's binary_logloss: 0.667367\n",
      "[700]\tvalid_0's binary_logloss: 0.66606\n",
      "[800]\tvalid_0's binary_logloss: 0.66536\n",
      "[900]\tvalid_0's binary_logloss: 0.664277\n",
      "Logloss: 0.6642772955253781, Confident objects accuracy: 0.746875\n",
      "[100]\tvalid_0's binary_logloss: 0.675612\n",
      "[200]\tvalid_0's binary_logloss: 0.670721\n",
      "[300]\tvalid_0's binary_logloss: 0.667151\n",
      "[400]\tvalid_0's binary_logloss: 0.664309\n",
      "[500]\tvalid_0's binary_logloss: 0.661912\n",
      "[600]\tvalid_0's binary_logloss: 0.661403\n",
      "[700]\tvalid_0's binary_logloss: 0.659943\n",
      "[800]\tvalid_0's binary_logloss: 0.659438\n",
      "[900]\tvalid_0's binary_logloss: 0.658975\n",
      "Logloss: 0.6589753823927194, Confident objects accuracy: 0.7472826086956522\n",
      "[100]\tvalid_0's binary_logloss: 0.676342\n",
      "[200]\tvalid_0's binary_logloss: 0.671086\n",
      "[300]\tvalid_0's binary_logloss: 0.666605\n",
      "[400]\tvalid_0's binary_logloss: 0.66437\n",
      "[500]\tvalid_0's binary_logloss: 0.66079\n",
      "[600]\tvalid_0's binary_logloss: 0.659961\n",
      "[700]\tvalid_0's binary_logloss: 0.657541\n",
      "[800]\tvalid_0's binary_logloss: 0.657322\n",
      "[900]\tvalid_0's binary_logloss: 0.656615\n",
      "Logloss: 0.6566154563890136, Confident objects accuracy: 0.7815126050420168\n",
      "[100]\tvalid_0's binary_logloss: 0.675993\n",
      "[200]\tvalid_0's binary_logloss: 0.671729\n",
      "[300]\tvalid_0's binary_logloss: 0.667826\n",
      "[400]\tvalid_0's binary_logloss: 0.665514\n",
      "[500]\tvalid_0's binary_logloss: 0.662946\n",
      "[600]\tvalid_0's binary_logloss: 0.661851\n",
      "[700]\tvalid_0's binary_logloss: 0.661014\n",
      "[800]\tvalid_0's binary_logloss: 0.660387\n",
      "[900]\tvalid_0's binary_logloss: 0.659098\n",
      "Logloss: 0.6590977197409298, Confident objects accuracy: 0.7580645161290323\n",
      "[100]\tvalid_0's binary_logloss: 0.674376\n",
      "[200]\tvalid_0's binary_logloss: 0.668795\n",
      "[300]\tvalid_0's binary_logloss: 0.66438\n",
      "[400]\tvalid_0's binary_logloss: 0.660906\n",
      "[500]\tvalid_0's binary_logloss: 0.658513\n",
      "[600]\tvalid_0's binary_logloss: 0.657878\n",
      "[700]\tvalid_0's binary_logloss: 0.65656\n",
      "[800]\tvalid_0's binary_logloss: 0.655138\n",
      "[900]\tvalid_0's binary_logloss: 0.654171\n",
      "Logloss: 0.6541714406026568, Confident objects accuracy: 0.7831325301204819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6584281345050492"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7641921397379913"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "\n",
    "def model_train(df, task_type, how, n_folds): \n",
    "    oof = np.zeros([df['target'].shape[0], 1])\n",
    "    # features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    features = res.groupby('Feature')['rank'].sum().sort_values().head(30).index.to_list()\n",
    "\n",
    "    X, groups = df[features], df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(df['pattern'], drop_first=True)], axis=1)\n",
    "    \n",
    "    if task_type == 'cls':\n",
    "        y = df['target'] >= df['close']\n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "    else:\n",
    "        y = (df['target'] - df['close']) / df['close']\n",
    "        kf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    oe_enc = OrdinalEncoder()\n",
    "    groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "    print(f\"Training with {len(features)} features\")\n",
    "    \n",
    "    if how == 'lreg':\n",
    "        scaler = StandardScaler()\n",
    "        X[X.columns] = scaler.fit_transform(X)\n",
    "    \n",
    "    for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        models = list()\n",
    "        if how == 'lgbm':\n",
    "            if task_type == 'cls':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='mse', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "        elif how == 'lreg':\n",
    "            if task_type == 'cls':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "            else:\n",
    "                model = LinearRegression(positive=True)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            acc_score = confident_accuracy_score(y_val, val_preds[:,1])\n",
    "            print(f'Logloss: {val_score}, Confident objects accuracy: {acc_score}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "        else:\n",
    "            val_preds = model.predict(X_val)\n",
    "            val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            print('RMSE: {val_score}')\n",
    "            oof[val_idx, 0] = val_preds\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "    return oof, models\n",
    "\n",
    "def confident_accuracy_score(y, oof, low_bound=0.35, high_bound=0.65):\n",
    "    ''' Consider only high confident objects for accuracy score calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    pred_conf = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    y_conf = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    return accuracy_score(y_conf, pred_conf)   \n",
    "\n",
    "# task_type = 'reg'\n",
    "task_type = 'cls'\n",
    "\n",
    "# best params for classification\n",
    "if task_type == 'cls':\n",
    "    params = {\n",
    "            'boosting_type': 'dart',\n",
    "            'n_estimators': 900,\n",
    "            'learning_rate': 0.02,\n",
    "            #   'early_stopping_round': 50,\n",
    "            'max_depth': 10,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.9,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 26,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 255,\n",
    "            'reg_alpha': 1e-5,\n",
    "            'reg_lambda': 1e-7,\n",
    "            'objective': 'binary'\n",
    "            }\n",
    "else:\n",
    "    # best params for regression\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.021,\n",
    "            'early_stopping_round': 100,\n",
    "            'max_depth': 7,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.8,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 27,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 511,\n",
    "            'reg_alpha': 1e-4,\n",
    "            'reg_lambda': 1e-4,\n",
    "            'objective': 'regression'\n",
    "            }\n",
    "\n",
    "oof, models = model_train(train_df, task_type=task_type, how='lgbm', n_folds=5) # 0.061096263508601985 / logloss: 0.6226973017816237 acc: 0.6424792139077853\n",
    "# oof, models = model_train(train_df, task_type=task_type, how='lreg', n_folds=5) # 0.06958035063954768 / logloss: 0.6985539186132326 acc: 0.587892049598833\n",
    "\n",
    "if task_type == 'cls':\n",
    "    y = train_df['target'] >= train_df['close']\n",
    "    low_bound, high_bound = 0.35, 0.65\n",
    "    display(log_loss(y, oof))\n",
    "    display(confident_accuracy_score(y, oof, low_bound, high_bound))\n",
    "else:\n",
    "    y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "    display(mean_squared_error(y, oof, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6584281345050492\n",
    "\n",
    "# 0.7641921397379913"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
