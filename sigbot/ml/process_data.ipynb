{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_prev_48</th>\n",
       "      <th>linear_reg_angle_prev_48</th>\n",
       "      <th>macd_prev_48</th>\n",
       "      <th>macdsignal_prev_48</th>\n",
       "      <th>macdhist_prev_48</th>\n",
       "      <th>macd_dir_prev_48</th>\n",
       "      <th>macdsignal_dir_prev_48</th>\n",
       "      <th>atr_prev_48</th>\n",
       "      <th>close_smooth_prev_48</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-10 21:00:00</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>227818.000</td>\n",
       "      <td>58.380200</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>34.523810</td>\n",
       "      <td>0.205808</td>\n",
       "      <td>...</td>\n",
       "      <td>22.072420</td>\n",
       "      <td>3.511172</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.488404</td>\n",
       "      <td>-0.017765</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-15 15:00:00</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>250135.000</td>\n",
       "      <td>58.463239</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>53.174603</td>\n",
       "      <td>0.357436</td>\n",
       "      <td>...</td>\n",
       "      <td>16.768418</td>\n",
       "      <td>-3.329827</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.302160</td>\n",
       "      <td>-0.266478</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>1.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-21 19:00:00</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>49801.000</td>\n",
       "      <td>45.480088</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>61.904762</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>38.183518</td>\n",
       "      <td>2.280714</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.359671</td>\n",
       "      <td>1.111904</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.000008</td>\n",
       "      <td>1.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-24 12:00:00</td>\n",
       "      <td>68.10000</td>\n",
       "      <td>69.10000</td>\n",
       "      <td>68.0000</td>\n",
       "      <td>68.70000</td>\n",
       "      <td>1712.657</td>\n",
       "      <td>25.443832</td>\n",
       "      <td>13.872597</td>\n",
       "      <td>11.125943</td>\n",
       "      <td>0.187309</td>\n",
       "      <td>...</td>\n",
       "      <td>35.878666</td>\n",
       "      <td>10.537428</td>\n",
       "      <td>0.487438</td>\n",
       "      <td>0.885719</td>\n",
       "      <td>-0.398280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.079996</td>\n",
       "      <td>1.140238</td>\n",
       "      <td>73.870833</td>\n",
       "      <td>70.30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-24 20:00:00</td>\n",
       "      <td>0.01983</td>\n",
       "      <td>0.01986</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.01983</td>\n",
       "      <td>2034435.500</td>\n",
       "      <td>39.251422</td>\n",
       "      <td>48.677324</td>\n",
       "      <td>51.357005</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>...</td>\n",
       "      <td>9.486056</td>\n",
       "      <td>-10.186354</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.893395</td>\n",
       "      <td>0.167818</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.020048</td>\n",
       "      <td>0.01943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time      open      high      low     close       volume  \\\n",
       "0 2022-09-10 21:00:00   0.99990   0.99990   0.9998   0.99980   227818.000   \n",
       "1 2022-09-15 15:00:00   1.00000   1.00000   0.9999   1.00000   250135.000   \n",
       "2 2022-09-21 19:00:00   1.00000   1.00000   0.9999   0.99990    49801.000   \n",
       "3 2022-12-24 12:00:00  68.10000  69.10000  68.0000  68.70000     1712.657   \n",
       "4 2022-12-24 20:00:00   0.01983   0.01986   0.0198   0.01983  2034435.500   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  58.380200    35.714286    34.523810         0.205808  ...   \n",
       "1  58.463239    64.285714    53.174603         0.357436  ...   \n",
       "2  45.480088    57.142857    61.904762        -0.066667  ...   \n",
       "3  25.443832    13.872597    11.125943         0.187309  ...   \n",
       "4  39.251422    48.677324    51.357005         0.000399  ...   \n",
       "\n",
       "   linear_reg_prev_48  linear_reg_angle_prev_48  macd_prev_48  \\\n",
       "0           22.072420                  3.511172     -0.000010   \n",
       "1           16.768418                 -3.329827     -0.000006   \n",
       "2           38.183518                  2.280714     -0.000016   \n",
       "3           35.878666                 10.537428      0.487438   \n",
       "4            9.486056                -10.186354     -0.000034   \n",
       "\n",
       "   macdsignal_prev_48  macdhist_prev_48  macd_dir_prev_48  \\\n",
       "0           -0.000008         -0.000002          0.488404   \n",
       "1            0.000005         -0.000011         -0.302160   \n",
       "2           -0.000006         -0.000010          0.359671   \n",
       "3            0.885719         -0.398280          0.000000   \n",
       "4           -0.000007         -0.000027          0.893395   \n",
       "\n",
       "   macdsignal_dir_prev_48  atr_prev_48  close_smooth_prev_48    target  \n",
       "0               -0.017765     0.000108              0.999946   0.99980  \n",
       "1               -0.266478     0.000111              0.999804   1.00010  \n",
       "2                1.111904     0.000115              1.000008   1.00010  \n",
       "3               -0.079996     1.140238             73.870833  70.30000  \n",
       "4                0.167818     0.000150              0.020048   0.01943  \n",
       "\n",
       "[5 rows x 264 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11581, 264)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from indicators import indicators\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from config.config import ConfigFactory\n",
    "\n",
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"1h_4h\"\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "# Should we create new dataset or load previous\n",
    "collect = False\n",
    "\n",
    "def get_file(ticker):\n",
    "    ''' Find files buy ticker names, file names can be in different formats '''\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def add_indicators(df, ttype, configs):\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, '', '', 0)\n",
    "    # add RSI\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, '', '', 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df = trend.get_indicator(df, '', '', 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df = macd.get_indicator(df, '', '', 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, '', '', 0)\n",
    "    # add SMA\n",
    "    # sma = indicators.SMA(ttype, configs)\n",
    "    # df = sma.get_indicator(df, '', '', 0)\n",
    "    return df\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step):\n",
    "    ''' Create train dataset from signal statistics and ticker candle data'''\n",
    "    train_df = pd.DataFrame()\n",
    "    tickers = df['ticker'].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df['ticker'] == ticker]\n",
    "        times = signal_df['time']\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        tmp_df_1h = add_indicators(tmp_df_1h, ttype, configs)\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc('pattern')]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h['time'] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + step, step):\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h['time'] == time_prev, :].reset_index(drop=True)\n",
    "                    row_tmp.columns = [c + f'_prev_{i}' for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row['ticker'] = ticker\n",
    "                row['pattern'] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "            \n",
    "            # add target\n",
    "            time_next = t + timedelta(hours=target_offset)\n",
    "            if ttype == 'buy':\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'high'].reset_index(drop=True)\n",
    "            else:\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'low'].reset_index(drop=True)\n",
    "\n",
    "            target.name = 'target'\n",
    "            rows = pd.concat([row, target], axis=1)\n",
    "            \n",
    "            # add data to the dataset\n",
    "            if train_df.shape[0] == 0:\n",
    "                train_df = rows\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, rows])\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "if collect is True:\n",
    "    # for how long time (in hours) we want to predict\n",
    "    target_offset = 24\n",
    "    # first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "    first = 4\n",
    "    # last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "    last = 48\n",
    "    # step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "    step = 4\n",
    "\n",
    "    # Buy\n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "    # dataset for model train\n",
    "    train_buy = create_train_df(df, 'buy', configs, target_offset, first, last, step)\n",
    "    train_buy = train_buy.dropna()\n",
    "\n",
    "    # Sell\n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "    # dataset for model train\n",
    "    train_sell = create_train_df(df, 'sell', configs, target_offset, first, last, step)\n",
    "    train_sell = train_sell.dropna()\n",
    "\n",
    "    train_df = pd.concat([train_buy, train_sell]).sort_values('time').reset_index(drop=True)\n",
    "    train_df.to_pickle('signal_stat/train_df.pkl')\n",
    "else:\n",
    "    train_df = pd.read_pickle('signal_stat/train_df.pkl')\n",
    "\n",
    "display(train_df.head())\n",
    "display(train_df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "test_size=0.2\n",
    "\n",
    "x_data = train_df.drop(['target', 'time', 'ticker', 'pattern'], axis=1)\n",
    "y_data = train_df['target'] >= train_df['close']\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train[x_train.columns] = scaler.fit_transform(x_train)\n",
    "x_valid[x_valid.columns] = scaler.transform(x_valid)\n",
    "\n",
    "x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "x_valid = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_train), type(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigModel, self).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(260, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "        self.layers.add_module('sigmoid', torch.nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0 | Train Loss: 0.69777, Train Acc: 0.46848            Validation Loss: 0.73827, Val Acc: 0.48425                LR: 1e-06\n",
      "Epoch:  500 | Train Loss: 0.69701, Train Acc: 0.47042            Validation Loss: 0.78139, Val Acc: 0.48468                LR: 1e-06\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train).squeeze()\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train)\n",
    "    train_acc = (train_preds.round() == y_train).float().mean()\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad() # ~ model.zero_grad()\n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid).squeeze()\n",
    "        val_loss = criterion(val_preds, y_valid)\n",
    "        val_acc = (val_preds.round() == y_valid).float().mean()\n",
    "    \n",
    "    # update weights according to gradient value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# Initialize model\n",
    "model = SigModel().to(device)\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 10000\n",
    "\n",
    "# Send data to the device\n",
    "x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "# Empty loss lists to track values\n",
    "epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2000, threshold=1e-2)\n",
    "\n",
    "# Loop through the data\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc, val_loss, val_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "    # Print progress a total of 20 times\n",
    "    if epoch % int(epochs / 20) == 0:\n",
    "        print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.5f}\\\n",
    "            Validation Loss: {val_loss:.5f}, Val Acc: {val_acc:.5f}\\\n",
    "                LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]}')\n",
    "\n",
    "        epoch_count.append(epoch)\n",
    "        train_loss_values.append(train_loss.cpu().detach().numpy())\n",
    "        valid_loss_values.append(val_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "plt.title('Training & Validation Loss Curves')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat \u001b[1m\u001b[34m#1\n",
      "[100]\ttraining's binary_logloss: 0.664634\tvalid_1's binary_logloss: 0.677546\n",
      "[200]\ttraining's binary_logloss: 0.651138\tvalid_1's binary_logloss: 0.672401\n",
      "[300]\ttraining's binary_logloss: 0.636059\tvalid_1's binary_logloss: 0.667498\n",
      "[400]\ttraining's binary_logloss: 0.622097\tvalid_1's binary_logloss: 0.663483\n",
      "[500]\ttraining's binary_logloss: 0.608386\tvalid_1's binary_logloss: 0.660385\n",
      "[600]\ttraining's binary_logloss: 0.60259\tvalid_1's binary_logloss: 0.659508\n",
      "[700]\ttraining's binary_logloss: 0.592565\tvalid_1's binary_logloss: 0.657351\n",
      "[800]\ttraining's binary_logloss: 0.585903\tvalid_1's binary_logloss: 0.656394\n",
      "[900]\ttraining's binary_logloss: 0.576039\tvalid_1's binary_logloss: 0.654738\n",
      "[1000]\ttraining's binary_logloss: 0.569051\tvalid_1's binary_logloss: 0.653508\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| loss: \u001b[1m\u001b[34m0.65351\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.6635\tvalid_1's binary_logloss: 0.680149\n",
      "[200]\ttraining's binary_logloss: 0.650273\tvalid_1's binary_logloss: 0.674954\n",
      "[300]\ttraining's binary_logloss: 0.635465\tvalid_1's binary_logloss: 0.671507\n",
      "[400]\ttraining's binary_logloss: 0.621497\tvalid_1's binary_logloss: 0.668385\n",
      "[500]\ttraining's binary_logloss: 0.606999\tvalid_1's binary_logloss: 0.665654\n",
      "[600]\ttraining's binary_logloss: 0.601088\tvalid_1's binary_logloss: 0.664071\n",
      "[700]\ttraining's binary_logloss: 0.590281\tvalid_1's binary_logloss: 0.661721\n",
      "[800]\ttraining's binary_logloss: 0.582851\tvalid_1's binary_logloss: 0.660406\n",
      "[900]\ttraining's binary_logloss: 0.572483\tvalid_1's binary_logloss: 0.659106\n",
      "[1000]\ttraining's binary_logloss: 0.565275\tvalid_1's binary_logloss: 0.658427\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| loss: \u001b[1m\u001b[34m0.65843\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.662749\tvalid_1's binary_logloss: 0.680863\n",
      "[200]\ttraining's binary_logloss: 0.649355\tvalid_1's binary_logloss: 0.676989\n",
      "[300]\ttraining's binary_logloss: 0.63436\tvalid_1's binary_logloss: 0.673642\n",
      "[400]\ttraining's binary_logloss: 0.620439\tvalid_1's binary_logloss: 0.671132\n",
      "[500]\ttraining's binary_logloss: 0.606282\tvalid_1's binary_logloss: 0.669055\n",
      "[600]\ttraining's binary_logloss: 0.600056\tvalid_1's binary_logloss: 0.668408\n",
      "[700]\ttraining's binary_logloss: 0.589848\tvalid_1's binary_logloss: 0.667651\n",
      "[800]\ttraining's binary_logloss: 0.582331\tvalid_1's binary_logloss: 0.667549\n",
      "[900]\ttraining's binary_logloss: 0.572577\tvalid_1's binary_logloss: 0.66658\n",
      "[1000]\ttraining's binary_logloss: 0.565789\tvalid_1's binary_logloss: 0.666052\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| loss: \u001b[1m\u001b[34m0.66605\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.664235\tvalid_1's binary_logloss: 0.679627\n",
      "[200]\ttraining's binary_logloss: 0.65057\tvalid_1's binary_logloss: 0.675252\n",
      "[300]\ttraining's binary_logloss: 0.635415\tvalid_1's binary_logloss: 0.670544\n",
      "[400]\ttraining's binary_logloss: 0.621095\tvalid_1's binary_logloss: 0.667297\n",
      "[500]\ttraining's binary_logloss: 0.606894\tvalid_1's binary_logloss: 0.664199\n",
      "[600]\ttraining's binary_logloss: 0.600888\tvalid_1's binary_logloss: 0.663446\n",
      "[700]\ttraining's binary_logloss: 0.590344\tvalid_1's binary_logloss: 0.662146\n",
      "[800]\ttraining's binary_logloss: 0.582935\tvalid_1's binary_logloss: 0.661275\n",
      "[900]\ttraining's binary_logloss: 0.572898\tvalid_1's binary_logloss: 0.659343\n",
      "[1000]\ttraining's binary_logloss: 0.566077\tvalid_1's binary_logloss: 0.658666\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| loss: \u001b[1m\u001b[34m0.65867\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.662087\tvalid_1's binary_logloss: 0.680393\n",
      "[200]\ttraining's binary_logloss: 0.647966\tvalid_1's binary_logloss: 0.676152\n",
      "[300]\ttraining's binary_logloss: 0.632861\tvalid_1's binary_logloss: 0.672698\n",
      "[400]\ttraining's binary_logloss: 0.618863\tvalid_1's binary_logloss: 0.670064\n",
      "[500]\ttraining's binary_logloss: 0.606222\tvalid_1's binary_logloss: 0.667979\n",
      "[600]\ttraining's binary_logloss: 0.600318\tvalid_1's binary_logloss: 0.667279\n",
      "[700]\ttraining's binary_logloss: 0.590815\tvalid_1's binary_logloss: 0.66581\n",
      "[800]\ttraining's binary_logloss: 0.58456\tvalid_1's binary_logloss: 0.665419\n",
      "[900]\ttraining's binary_logloss: 0.575318\tvalid_1's binary_logloss: 0.664935\n",
      "[1000]\ttraining's binary_logloss: 0.569019\tvalid_1's binary_logloss: 0.664558\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| loss: \u001b[1m\u001b[34m0.66456\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "\u001b[1m\u001b[31m Outer Holdout avg score: \u001b[0m log_loss: \u001b[1m\u001b[31m0.66032\u001b[0m\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "class CFG:\n",
    "    n_repeats = 1\n",
    "    n_folds = 5\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target'] >= df['close']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.01,\n",
    "          'early_stopping_round': 100,\n",
    "          'max_depth': 7,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree': 0.75,\n",
    "          'num_leaves': 32,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(train_df, permut=True, boruta=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "res = res.groupby('Feature')['rank'].sum().sort_values()\n",
    "res.to_csv('feature_importance.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 124>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39m# best params for regression\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     params \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mboosting_type\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mgbdt\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m             }\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m oof, models \u001b[39m=\u001b[39m model_train(train_df, task_type\u001b[39m=\u001b[39;49mtask_type, how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlgbm\u001b[39;49m\u001b[39m'\u001b[39;49m, n_folds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m) \u001b[39m# 0.061096263508601985 / logloss: 0.6226973017816237 acc: 0.6424792139077853\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m \u001b[39m# oof, models = model_train(train_df, task_type=task_type, how='lreg', n_folds=5) # 0.06958035063954768 / logloss: 0.6985539186132326 acc: 0.587892049598833\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m \u001b[39mif\u001b[39;00m task_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;32m/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb Cell 17\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(df, task_type, how, n_folds)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m oof \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros([df[\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m features \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mFeature\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39msort_values()\u001b[39m.\u001b[39mhead(\u001b[39m200\u001b[39m)\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mto_list()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X, groups \u001b[39m=\u001b[39m df[features], df[\u001b[39m'\u001b[39m\u001b[39mticker\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m X \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([X, pd\u001b[39m.\u001b[39mget_dummies(df[\u001b[39m'\u001b[39m\u001b[39mpattern\u001b[39m\u001b[39m'\u001b[39m], drop_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "\n",
    "def model_train(df, task_type, how, n_folds): \n",
    "    oof = np.zeros([df['target'].shape[0], 1])\n",
    "    # features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    features = res.groupby('Feature')['rank'].sum().sort_values().head(200).index.to_list()\n",
    "\n",
    "    X, groups = df[features], df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(df['pattern'], drop_first=True)], axis=1)\n",
    "    \n",
    "    if task_type == 'cls':\n",
    "        y = df['target'] >= df['close']\n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "    else:\n",
    "        y = (df['target'] - df['close']) / df['close']\n",
    "        kf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    oe_enc = OrdinalEncoder()\n",
    "    groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "    print(f\"Training with {len(features)} features\")\n",
    "    \n",
    "    if how == 'lreg':\n",
    "        scaler = StandardScaler()\n",
    "        X[X.columns] = scaler.fit_transform(X)\n",
    "    \n",
    "    for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        models = list()\n",
    "        if how == 'lgbm':\n",
    "            if task_type == 'cls':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='mse', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "        elif how == 'lreg':\n",
    "            if task_type == 'cls':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "            else:\n",
    "                model = LinearRegression(positive=True)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            acc_score = confident_accuracy_score(y_val, val_preds[:,1])\n",
    "            print(f'Logloss: {val_score}, Confident objects accuracy: {acc_score}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "        else:\n",
    "            val_preds = model.predict(X_val)\n",
    "            val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            print('RMSE: {val_score}')\n",
    "            oof[val_idx, 0] = val_preds\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "    return oof, models\n",
    "\n",
    "def confident_accuracy_score(y, oof, low_bound=0.35, high_bound=0.65):\n",
    "    ''' Consider only high confident objects for accuracy score calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    pred_conf = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    y_conf = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    return accuracy_score(y_conf, pred_conf)   \n",
    "\n",
    "# task_type = 'reg'\n",
    "task_type = 'cls'\n",
    "\n",
    "# best params for classification\n",
    "if task_type == 'cls':\n",
    "    params = {\n",
    "            'boosting_type': 'dart',\n",
    "            'n_estimators': 900,\n",
    "            'learning_rate': 0.02,\n",
    "            #   'early_stopping_round': 50,\n",
    "            'max_depth': 10,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.9,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 26,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 255,\n",
    "            'reg_alpha': 1e-5,\n",
    "            'reg_lambda': 1e-7,\n",
    "            'objective': 'binary'\n",
    "            }\n",
    "else:\n",
    "    # best params for regression\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.021,\n",
    "            'early_stopping_round': 100,\n",
    "            'max_depth': 7,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.8,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 27,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 511,\n",
    "            'reg_alpha': 1e-4,\n",
    "            'reg_lambda': 1e-4,\n",
    "            'objective': 'regression'\n",
    "            }\n",
    "\n",
    "oof, models = model_train(train_df, task_type=task_type, how='lgbm', n_folds=5) # 0.061096263508601985 / logloss: 0.6226973017816237 acc: 0.6424792139077853\n",
    "# oof, models = model_train(train_df, task_type=task_type, how='lreg', n_folds=5) # 0.06958035063954768 / logloss: 0.6985539186132326 acc: 0.587892049598833\n",
    "\n",
    "if task_type == 'cls':\n",
    "    y = train_df['target'] >= train_df['close']\n",
    "    low_bound, high_bound = 0.3, 0.7\n",
    "    display(log_loss(y, oof))\n",
    "    display(confident_accuracy_score(y, oof, low_bound, high_bound))\n",
    "else:\n",
    "    y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "    display(mean_squared_error(y, oof, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6548368650163564\n",
    "\n",
    "# 0.8207831325301205"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
