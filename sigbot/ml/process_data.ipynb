{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from indicators import indicators\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from config.config import ConfigFactory\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score, precision_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    collect_data = False # create new dataset or load previous\n",
    "    select_features = False\n",
    "    train_NN = False\n",
    "    train_LGBM = True\n",
    "    n_repeats = 1\n",
    "    n_folds = 5\n",
    "    cls_target_ratio = 1.021"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_angle_prev_72</th>\n",
       "      <th>macd_prev_72</th>\n",
       "      <th>macdsignal_prev_72</th>\n",
       "      <th>macdhist_prev_72</th>\n",
       "      <th>macd_dir_prev_72</th>\n",
       "      <th>macdsignal_dir_prev_72</th>\n",
       "      <th>atr_prev_72</th>\n",
       "      <th>close_smooth_prev_72</th>\n",
       "      <th>target</th>\n",
       "      <th>ttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-10 21:00:00</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>227818.00</td>\n",
       "      <td>58.380200</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>34.523810</td>\n",
       "      <td>0.205808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.856226</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.318808</td>\n",
       "      <td>0.227337</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-15 15:00:00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>250135.00</td>\n",
       "      <td>58.463239</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>53.174603</td>\n",
       "      <td>0.357436</td>\n",
       "      <td>...</td>\n",
       "      <td>5.250146</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.229362</td>\n",
       "      <td>0.110533</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.999742</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-21 19:00:00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>49801.00</td>\n",
       "      <td>45.480088</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>61.904762</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>5.078633</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-1.898082</td>\n",
       "      <td>-0.192558</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>16.2600</td>\n",
       "      <td>16.2800</td>\n",
       "      <td>16.2200</td>\n",
       "      <td>16.2600</td>\n",
       "      <td>8091.48</td>\n",
       "      <td>34.890095</td>\n",
       "      <td>26.002932</td>\n",
       "      <td>23.959198</td>\n",
       "      <td>0.064161</td>\n",
       "      <td>...</td>\n",
       "      <td>10.131495</td>\n",
       "      <td>0.101978</td>\n",
       "      <td>0.165148</td>\n",
       "      <td>-0.063170</td>\n",
       "      <td>-0.103585</td>\n",
       "      <td>-0.067033</td>\n",
       "      <td>0.161075</td>\n",
       "      <td>16.473750</td>\n",
       "      <td>1</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>6.8900</td>\n",
       "      <td>6.9100</td>\n",
       "      <td>6.8900</td>\n",
       "      <td>6.9000</td>\n",
       "      <td>3986.13</td>\n",
       "      <td>30.533005</td>\n",
       "      <td>14.404762</td>\n",
       "      <td>18.392857</td>\n",
       "      <td>-0.204445</td>\n",
       "      <td>...</td>\n",
       "      <td>11.812509</td>\n",
       "      <td>0.028818</td>\n",
       "      <td>-0.003283</td>\n",
       "      <td>0.032101</td>\n",
       "      <td>-0.946146</td>\n",
       "      <td>-0.453481</td>\n",
       "      <td>0.088674</td>\n",
       "      <td>6.850417</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time     open     high      low    close     volume  \\\n",
       "0 2022-09-10 21:00:00   0.9999   0.9999   0.9998   0.9998  227818.00   \n",
       "1 2022-09-15 15:00:00   1.0000   1.0000   0.9999   1.0000  250135.00   \n",
       "2 2022-09-21 19:00:00   1.0000   1.0000   0.9999   0.9999   49801.00   \n",
       "3 2022-12-25 15:00:00  16.2600  16.2800  16.2200  16.2600    8091.48   \n",
       "4 2022-12-25 15:00:00   6.8900   6.9100   6.8900   6.9000    3986.13   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  58.380200    35.714286    34.523810         0.205808  ...   \n",
       "1  58.463239    64.285714    53.174603         0.357436  ...   \n",
       "2  45.480088    57.142857    61.904762        -0.066667  ...   \n",
       "3  34.890095    26.002932    23.959198         0.064161  ...   \n",
       "4  30.533005    14.404762    18.392857        -0.204445  ...   \n",
       "\n",
       "   linear_reg_angle_prev_72  macd_prev_72  macdsignal_prev_72  \\\n",
       "0                 -0.856226     -0.000020           -0.000010   \n",
       "1                  5.250146      0.000016            0.000011   \n",
       "2                  5.078633      0.000006           -0.000004   \n",
       "3                 10.131495      0.101978            0.165148   \n",
       "4                 11.812509      0.028818           -0.003283   \n",
       "\n",
       "   macdhist_prev_72  macd_dir_prev_72  macdsignal_dir_prev_72  atr_prev_72  \\\n",
       "0         -0.000009          0.318808                0.227337     0.000114   \n",
       "1          0.000006          0.229362                0.110533     0.000113   \n",
       "2          0.000010         -1.898082               -0.192558     0.000113   \n",
       "3         -0.063170         -0.103585               -0.067033     0.161075   \n",
       "4          0.032101         -0.946146               -0.453481     0.088674   \n",
       "\n",
       "   close_smooth_prev_72  target  ttype  \n",
       "0              1.000004       0    buy  \n",
       "1              0.999742       0    buy  \n",
       "2              0.999988       0    buy  \n",
       "3             16.473750       1   sell  \n",
       "4              6.850417       0   sell  \n",
       "\n",
       "[5 rows x 385 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(12043, 385)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"1h_4h\"\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "def get_file(ticker):\n",
    "    ''' Find files buy ticker names, file names can be in different formats '''\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def add_indicators(df, ttype, configs):\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, '', '', 0)\n",
    "    # add RSI\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, '', '', 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df = trend.get_indicator(df, '', '', 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df = macd.get_indicator(df, '', '', 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, '', '', 0)\n",
    "    # add SMA\n",
    "    # sma = indicators.SMA(ttype, configs)\n",
    "    # df = sma.get_indicator(df, '', '', 0)\n",
    "    return df\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step):\n",
    "    ''' Create train dataset from signal statistics and ticker candle data'''\n",
    "    train_df = pd.DataFrame()\n",
    "    tickers = df['ticker'].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df['ticker'] == ticker]\n",
    "        times = signal_df['time']\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        tmp_df_1h = add_indicators(tmp_df_1h, ttype, configs)\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc('pattern')]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h['time'] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + step, step):\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h['time'] == time_prev, :].reset_index(drop=True)\n",
    "                    row_tmp.columns = [c + f'_prev_{i}' for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row['ticker'] = ticker\n",
    "                row['pattern'] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "\n",
    "            row['target'] = 0\n",
    "            \n",
    "            if row['pattern'].values == 'STOCH_RSI':\n",
    "                if ttype == 'buy':\n",
    "                    row['ttype'] = 'sell'\n",
    "                else:\n",
    "                    row['ttype'] = 'buy'\n",
    "            else:\n",
    "                row['ttype'] = ttype\n",
    "            \n",
    "            # Ff ttype = buy and during the selected period high price was higher than close_price * target_ratio\n",
    "            # and earlier low price wasn't lower than close_price / target_ratio, than target is True, else target is False.\n",
    "            # Similarly for ttype = sell \n",
    "            close_price = tmp_df_1h.loc[tmp_df_1h['time'] == t, 'close'].values\n",
    "            \n",
    "            for i in range(1, target_offset + 1):\n",
    "                time_next = t + timedelta(hours=i)\n",
    "                target_buy = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'high'].reset_index(drop=True)\n",
    "                target_sell = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'low'].reset_index(drop=True)\n",
    "\n",
    "                try:\n",
    "                    target_buy = target_buy > close_price * CFG.cls_target_ratio\n",
    "                    target_sell = target_sell < close_price / CFG.cls_target_ratio\n",
    "                except ValueError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    if (row['ttype'].values == 'buy' and target_sell[0]) or (row['ttype'].values == 'sell' and target_buy[0]):\n",
    "                        break\n",
    "                    elif (row['ttype'].values == 'buy' and target_buy[0]) or (row['ttype'].values == 'sell' and target_sell[0]):\n",
    "                        row['target'] = 1\n",
    "                        break\n",
    "                except KeyError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "            \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "\n",
    "            # add data to the dataset\n",
    "            if train_df.shape[0] == 0:\n",
    "                train_df = row\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, row])\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "# for how long time (in hours) we want to predict\n",
    "target_offset = 24\n",
    "# first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "first = 4\n",
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 72\n",
    "# step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "step = 4\n",
    "\n",
    "if CFG.collect_data is True:\n",
    "    # Buy\n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "    # dataset for model train\n",
    "    train_buy = create_train_df(df, 'buy', configs, target_offset, first, last, step)\n",
    "    train_buy = train_buy.dropna()\n",
    "\n",
    "    # Sell\n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "    # dataset for model train\n",
    "    train_sell = create_train_df(df, 'sell', configs, target_offset, first, last, step)\n",
    "    train_sell = train_sell.dropna()\n",
    "\n",
    "    df = pd.concat([train_buy, train_sell]).sort_values('time').reset_index(drop=True)\n",
    "    df.to_pickle(f'signal_stat/train_df_{last}.pkl')\n",
    "else:\n",
    "    df = pd.read_pickle(f'signal_stat/train_df_{last}.pkl')\n",
    "\n",
    "display(df.head())\n",
    "display(df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check target correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1000\n",
    "\n",
    "# x = train_df.loc[(train_df.target == 1) & (train_df.ttype == 'buy'), ['ticker', 'ttype', 'pattern', 'time', 'close', 'target']]\n",
    "# y = x.iloc[i]\n",
    "# low_price, high_price = y['close'] / CFG.cls_target_ratio, y['close'] * CFG.cls_target_ratio,\n",
    "# print(y['ticker'], y['time'], y['ttype'])\n",
    "\n",
    "# tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{y[\"ticker\"]}_1h.pkl')\n",
    "# # tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{y[\"ticker\"][:-4]}-{y[\"ticker\"][-4:]}_1h.pkl')\n",
    "# # tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{y[\"ticker\"][:-4]}-{y[\"ticker\"][-4:]}-SWAP_4h.pkl')\n",
    "\n",
    "# tmp_df_1h['low_price'] = low_price\n",
    "# tmp_df_1h['high_price'] = high_price\n",
    "# idx = tmp_df_1h[tmp_df_1h['time'] == y['time']].index[0]\n",
    "\n",
    "# tmp_df_1h = tmp_df_1h.iloc[idx:idx+24][['time', 'close', 'high', 'high_price', 'low', 'low_price']]\n",
    "\n",
    "# if y['ttype'] == 'buy':\n",
    "#     tmp_df_1h['signal'] = tmp_df_1h['high'] > tmp_df_1h['high_price']\n",
    "#     tmp_df_1h['anti_signal'] = tmp_df_1h['low'] < tmp_df_1h['low_price']\n",
    "# else:\n",
    "#     tmp_df_1h['signal'] = tmp_df_1h['low'] < tmp_df_1h['low_price']\n",
    "#     tmp_df_1h['anti_signal'] = tmp_df_1h['high'] > tmp_df_1h['high_price']\n",
    "\n",
    "# tmp_df_1h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split by ticker group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.1, n_splits=2, random_state = 7)\n",
    "\n",
    "split = splitter.split(df, groups=df['ticker'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_df = df.iloc[train_inds]\n",
    "test_df = df.iloc[test_inds]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_size=0.2\n",
    "\n",
    "x_data = train_df.drop(['target', 'time', 'ticker', 'pattern', 'ttype'], axis=1)\n",
    "y_data = train_df['target'] >= train_df['close']\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train[x_train.columns] = scaler.fit_transform(x_train)\n",
    "x_valid[x_valid.columns] = scaler.transform(x_valid)\n",
    "\n",
    "x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "x_valid = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_train), type(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigModel, self).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(260, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "        self.layers.add_module('sigmoid', torch.nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train).squeeze()\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train)\n",
    "    train_acc = (train_preds.round() == y_train).float().mean()\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad() # ~ model.zero_grad()\n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid).squeeze()\n",
    "        val_loss = criterion(val_preds, y_valid)\n",
    "        val_acc = (val_preds.round() == y_valid).float().mean()\n",
    "    \n",
    "    # update weights according to gradient value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# Initialize model\n",
    "if CFG.train_NN:\n",
    "    model = SigModel().to(device)\n",
    "\n",
    "    # Number of epochs\n",
    "    epochs = 10000\n",
    "\n",
    "    # Send data to the device\n",
    "    x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "    y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "    train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    # Empty loss lists to track values\n",
    "    epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    learning_rate = 1e-6\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2000, threshold=1e-2)\n",
    "\n",
    "    # Loop through the data\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc, val_loss, val_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "        # Print progress a total of 20 times\n",
    "        if epoch % int(epochs / 20) == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.5f}\\\n",
    "                Validation Loss: {val_loss:.5f}, Val Acc: {val_acc:.5f}\\\n",
    "                    LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]}')\n",
    "\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(train_loss.cpu().detach().numpy())\n",
    "            valid_loss_values.append(val_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NN train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CFG.train_NN:\n",
    "    plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "    plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss Curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern', 'ttype']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target'] >= df['close']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.01,\n",
    "        #   'early_stopping_round': 100,\n",
    "          'max_depth': 7,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree': 0.75,\n",
    "          'num_leaves': 32,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "if CFG.select_features:\n",
    "    perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(df, permut=True, boruta=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.select_features:\n",
    "    perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "    boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "    feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "    res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "    res = res.groupby('Feature')['rank'].sum().sort_values().reset_index()\n",
    "    res.to_csv('feature_importance.csv')\n",
    "else:\n",
    "    res = pd.read_csv('feature_importance.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test split based on ticker groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 150 features\n",
      "Fold #1\n",
      "[100]\tvalid_0's binary_logloss: 0.680638\tvalid_0's average_precision: 0.618577\n",
      "[200]\tvalid_0's binary_logloss: 0.676357\tvalid_0's average_precision: 0.630403\n",
      "[300]\tvalid_0's binary_logloss: 0.673326\tvalid_0's average_precision: 0.631453\n",
      "[400]\tvalid_0's binary_logloss: 0.672148\tvalid_0's average_precision: 0.62801\n",
      "[500]\tvalid_0's binary_logloss: 0.670675\tvalid_0's average_precision: 0.628659\n",
      "[600]\tvalid_0's binary_logloss: 0.669039\tvalid_0's average_precision: 0.635658\n",
      "[700]\tvalid_0's binary_logloss: 0.668299\tvalid_0's average_precision: 0.63789\n",
      "[800]\tvalid_0's binary_logloss: 0.66831\tvalid_0's average_precision: 0.634634\n",
      "[900]\tvalid_0's binary_logloss: 0.668352\tvalid_0's average_precision: 0.634427\n",
      "[1000]\tvalid_0's binary_logloss: 0.667887\tvalid_0's average_precision: 0.635674\n",
      "[1100]\tvalid_0's binary_logloss: 0.66749\tvalid_0's average_precision: 0.635681\n",
      "[1200]\tvalid_0's binary_logloss: 0.667815\tvalid_0's average_precision: 0.633642\n",
      "[1300]\tvalid_0's binary_logloss: 0.667601\tvalid_0's average_precision: 0.634838\n",
      "[1400]\tvalid_0's binary_logloss: 0.667873\tvalid_0's average_precision: 0.634208\n",
      "[1500]\tvalid_0's binary_logloss: 0.667441\tvalid_0's average_precision: 0.634887\n",
      "[1600]\tvalid_0's binary_logloss: 0.667099\tvalid_0's average_precision: 0.635713\n",
      "[1700]\tvalid_0's binary_logloss: 0.667193\tvalid_0's average_precision: 0.635657\n",
      "[1800]\tvalid_0's binary_logloss: 0.666597\tvalid_0's average_precision: 0.637201\n",
      "[1900]\tvalid_0's binary_logloss: 0.666742\tvalid_0's average_precision: 0.636534\n",
      "[2000]\tvalid_0's binary_logloss: 0.666624\tvalid_0's average_precision: 0.636711\n",
      "[2100]\tvalid_0's binary_logloss: 0.666553\tvalid_0's average_precision: 0.637186\n",
      "Logloss: 0.6665526465368187, Confident objects precision: 0.699438202247191, % of confident objects: 0.17248062015503876\n",
      "Fold #2\n",
      "[100]\tvalid_0's binary_logloss: 0.679151\tvalid_0's average_precision: 0.632432\n",
      "[200]\tvalid_0's binary_logloss: 0.676165\tvalid_0's average_precision: 0.637479\n",
      "[300]\tvalid_0's binary_logloss: 0.672857\tvalid_0's average_precision: 0.645376\n",
      "[400]\tvalid_0's binary_logloss: 0.670716\tvalid_0's average_precision: 0.654681\n",
      "[500]\tvalid_0's binary_logloss: 0.6687\tvalid_0's average_precision: 0.657326\n",
      "[600]\tvalid_0's binary_logloss: 0.667924\tvalid_0's average_precision: 0.660963\n",
      "[700]\tvalid_0's binary_logloss: 0.666389\tvalid_0's average_precision: 0.662892\n",
      "[800]\tvalid_0's binary_logloss: 0.667068\tvalid_0's average_precision: 0.661164\n",
      "[900]\tvalid_0's binary_logloss: 0.666201\tvalid_0's average_precision: 0.662372\n",
      "[1000]\tvalid_0's binary_logloss: 0.665957\tvalid_0's average_precision: 0.661862\n",
      "[1100]\tvalid_0's binary_logloss: 0.665264\tvalid_0's average_precision: 0.665912\n",
      "[1200]\tvalid_0's binary_logloss: 0.665534\tvalid_0's average_precision: 0.665261\n",
      "[1300]\tvalid_0's binary_logloss: 0.665721\tvalid_0's average_precision: 0.664535\n",
      "[1400]\tvalid_0's binary_logloss: 0.664789\tvalid_0's average_precision: 0.665964\n",
      "[1500]\tvalid_0's binary_logloss: 0.663862\tvalid_0's average_precision: 0.667685\n",
      "[1600]\tvalid_0's binary_logloss: 0.664199\tvalid_0's average_precision: 0.666917\n",
      "[1700]\tvalid_0's binary_logloss: 0.663249\tvalid_0's average_precision: 0.668297\n",
      "[1800]\tvalid_0's binary_logloss: 0.662089\tvalid_0's average_precision: 0.669992\n",
      "[1900]\tvalid_0's binary_logloss: 0.661887\tvalid_0's average_precision: 0.673374\n",
      "[2000]\tvalid_0's binary_logloss: 0.660977\tvalid_0's average_precision: 0.674466\n",
      "[2100]\tvalid_0's binary_logloss: 0.659715\tvalid_0's average_precision: 0.675875\n",
      "Logloss: 0.6597149900885613, Confident objects precision: 0.7386018237082067, % of confident objects: 0.18267629094947252\n",
      "Fold #3\n",
      "[100]\tvalid_0's binary_logloss: 0.680321\tvalid_0's average_precision: 0.622659\n",
      "[200]\tvalid_0's binary_logloss: 0.675483\tvalid_0's average_precision: 0.632174\n",
      "[300]\tvalid_0's binary_logloss: 0.671977\tvalid_0's average_precision: 0.636299\n",
      "[400]\tvalid_0's binary_logloss: 0.670302\tvalid_0's average_precision: 0.641147\n",
      "[500]\tvalid_0's binary_logloss: 0.667666\tvalid_0's average_precision: 0.644683\n",
      "[600]\tvalid_0's binary_logloss: 0.667394\tvalid_0's average_precision: 0.643959\n",
      "[700]\tvalid_0's binary_logloss: 0.666411\tvalid_0's average_precision: 0.645318\n",
      "[800]\tvalid_0's binary_logloss: 0.665772\tvalid_0's average_precision: 0.64651\n",
      "[900]\tvalid_0's binary_logloss: 0.664733\tvalid_0's average_precision: 0.647504\n",
      "[1000]\tvalid_0's binary_logloss: 0.664333\tvalid_0's average_precision: 0.647496\n",
      "[1100]\tvalid_0's binary_logloss: 0.66326\tvalid_0's average_precision: 0.648858\n",
      "[1200]\tvalid_0's binary_logloss: 0.663182\tvalid_0's average_precision: 0.648195\n",
      "[1300]\tvalid_0's binary_logloss: 0.661781\tvalid_0's average_precision: 0.65106\n",
      "[1400]\tvalid_0's binary_logloss: 0.661782\tvalid_0's average_precision: 0.650751\n",
      "[1500]\tvalid_0's binary_logloss: 0.661222\tvalid_0's average_precision: 0.652142\n",
      "[1600]\tvalid_0's binary_logloss: 0.660837\tvalid_0's average_precision: 0.652341\n",
      "[1700]\tvalid_0's binary_logloss: 0.660702\tvalid_0's average_precision: 0.651186\n",
      "[1800]\tvalid_0's binary_logloss: 0.659853\tvalid_0's average_precision: 0.653923\n",
      "[1900]\tvalid_0's binary_logloss: 0.660207\tvalid_0's average_precision: 0.653613\n",
      "[2000]\tvalid_0's binary_logloss: 0.660245\tvalid_0's average_precision: 0.652166\n",
      "[2100]\tvalid_0's binary_logloss: 0.660087\tvalid_0's average_precision: 0.653274\n",
      "Logloss: 0.6600869602153484, Confident objects precision: 0.7023498694516971, % of confident objects: 0.18719452590420332\n",
      "Fold #4\n",
      "[100]\tvalid_0's binary_logloss: 0.680055\tvalid_0's average_precision: 0.603126\n",
      "[200]\tvalid_0's binary_logloss: 0.676235\tvalid_0's average_precision: 0.605912\n",
      "[300]\tvalid_0's binary_logloss: 0.672924\tvalid_0's average_precision: 0.61132\n",
      "[400]\tvalid_0's binary_logloss: 0.670883\tvalid_0's average_precision: 0.61266\n",
      "[500]\tvalid_0's binary_logloss: 0.669123\tvalid_0's average_precision: 0.615549\n",
      "[600]\tvalid_0's binary_logloss: 0.667851\tvalid_0's average_precision: 0.620949\n",
      "[700]\tvalid_0's binary_logloss: 0.666982\tvalid_0's average_precision: 0.622293\n",
      "[800]\tvalid_0's binary_logloss: 0.666813\tvalid_0's average_precision: 0.624173\n",
      "[900]\tvalid_0's binary_logloss: 0.665202\tvalid_0's average_precision: 0.628007\n",
      "[1000]\tvalid_0's binary_logloss: 0.664088\tvalid_0's average_precision: 0.627909\n",
      "[1100]\tvalid_0's binary_logloss: 0.663613\tvalid_0's average_precision: 0.629976\n",
      "[1200]\tvalid_0's binary_logloss: 0.663652\tvalid_0's average_precision: 0.629318\n",
      "[1300]\tvalid_0's binary_logloss: 0.663556\tvalid_0's average_precision: 0.628491\n",
      "[1400]\tvalid_0's binary_logloss: 0.663485\tvalid_0's average_precision: 0.626962\n",
      "[1500]\tvalid_0's binary_logloss: 0.66329\tvalid_0's average_precision: 0.627312\n",
      "[1600]\tvalid_0's binary_logloss: 0.66336\tvalid_0's average_precision: 0.626265\n",
      "[1700]\tvalid_0's binary_logloss: 0.663511\tvalid_0's average_precision: 0.626282\n",
      "[1800]\tvalid_0's binary_logloss: 0.663546\tvalid_0's average_precision: 0.62658\n",
      "[1900]\tvalid_0's binary_logloss: 0.663909\tvalid_0's average_precision: 0.626703\n",
      "[2000]\tvalid_0's binary_logloss: 0.663505\tvalid_0's average_precision: 0.626638\n",
      "[2100]\tvalid_0's binary_logloss: 0.663953\tvalid_0's average_precision: 0.624453\n",
      "Logloss: 0.6639528814414838, Confident objects precision: 0.6652977412731006, % of confident objects: 0.20901287553648068\n",
      "Fold #5\n",
      "[100]\tvalid_0's binary_logloss: 0.67678\tvalid_0's average_precision: 0.642586\n",
      "[200]\tvalid_0's binary_logloss: 0.672086\tvalid_0's average_precision: 0.652422\n",
      "[300]\tvalid_0's binary_logloss: 0.667998\tvalid_0's average_precision: 0.654858\n",
      "[400]\tvalid_0's binary_logloss: 0.665166\tvalid_0's average_precision: 0.658704\n",
      "[500]\tvalid_0's binary_logloss: 0.662132\tvalid_0's average_precision: 0.663273\n",
      "[600]\tvalid_0's binary_logloss: 0.661059\tvalid_0's average_precision: 0.66589\n",
      "[700]\tvalid_0's binary_logloss: 0.659531\tvalid_0's average_precision: 0.667079\n",
      "[800]\tvalid_0's binary_logloss: 0.659627\tvalid_0's average_precision: 0.665649\n",
      "[900]\tvalid_0's binary_logloss: 0.657515\tvalid_0's average_precision: 0.668251\n",
      "[1000]\tvalid_0's binary_logloss: 0.656995\tvalid_0's average_precision: 0.668078\n",
      "[1100]\tvalid_0's binary_logloss: 0.656004\tvalid_0's average_precision: 0.666256\n",
      "[1200]\tvalid_0's binary_logloss: 0.654414\tvalid_0's average_precision: 0.670508\n",
      "[1300]\tvalid_0's binary_logloss: 0.653921\tvalid_0's average_precision: 0.668503\n",
      "[1400]\tvalid_0's binary_logloss: 0.653138\tvalid_0's average_precision: 0.667139\n",
      "[1500]\tvalid_0's binary_logloss: 0.652723\tvalid_0's average_precision: 0.667667\n",
      "[1600]\tvalid_0's binary_logloss: 0.651798\tvalid_0's average_precision: 0.66832\n",
      "[1700]\tvalid_0's binary_logloss: 0.651097\tvalid_0's average_precision: 0.669563\n",
      "[1800]\tvalid_0's binary_logloss: 0.649944\tvalid_0's average_precision: 0.671601\n",
      "[1900]\tvalid_0's binary_logloss: 0.649289\tvalid_0's average_precision: 0.672635\n",
      "[2000]\tvalid_0's binary_logloss: 0.649372\tvalid_0's average_precision: 0.671997\n",
      "[2100]\tvalid_0's binary_logloss: 0.647984\tvalid_0's average_precision: 0.674678\n",
      "Logloss: 0.6479844452265489, Confident objects precision: 0.7526652452025586, % of confident objects: 0.17539267015706805\n",
      "Test fold\n",
      "[100]\ttraining's binary_logloss: 0.656818\ttraining's average_precision: 0.772591\n",
      "[200]\ttraining's binary_logloss: 0.639933\ttraining's average_precision: 0.798827\n",
      "[300]\ttraining's binary_logloss: 0.621642\ttraining's average_precision: 0.831965\n",
      "[400]\ttraining's binary_logloss: 0.605438\ttraining's average_precision: 0.857541\n",
      "[500]\ttraining's binary_logloss: 0.588446\ttraining's average_precision: 0.883007\n",
      "[600]\ttraining's binary_logloss: 0.580776\ttraining's average_precision: 0.899906\n",
      "[700]\ttraining's binary_logloss: 0.567758\ttraining's average_precision: 0.916712\n",
      "[800]\ttraining's binary_logloss: 0.559376\ttraining's average_precision: 0.927772\n",
      "[900]\ttraining's binary_logloss: 0.546876\ttraining's average_precision: 0.939203\n",
      "[1000]\ttraining's binary_logloss: 0.537933\ttraining's average_precision: 0.94674\n",
      "[1100]\ttraining's binary_logloss: 0.527512\ttraining's average_precision: 0.954413\n",
      "[1200]\ttraining's binary_logloss: 0.520715\ttraining's average_precision: 0.960698\n",
      "[1300]\ttraining's binary_logloss: 0.511591\ttraining's average_precision: 0.967401\n",
      "[1400]\ttraining's binary_logloss: 0.499377\ttraining's average_precision: 0.973066\n",
      "[1500]\ttraining's binary_logloss: 0.493834\ttraining's average_precision: 0.976757\n",
      "[1600]\ttraining's binary_logloss: 0.484016\ttraining's average_precision: 0.98167\n",
      "[1700]\ttraining's binary_logloss: 0.473253\ttraining's average_precision: 0.98509\n",
      "[1800]\ttraining's binary_logloss: 0.464216\ttraining's average_precision: 0.987795\n",
      "[1900]\ttraining's binary_logloss: 0.455098\ttraining's average_precision: 0.989919\n",
      "[2000]\ttraining's binary_logloss: 0.447794\ttraining's average_precision: 0.991715\n",
      "[2100]\ttraining's binary_logloss: 0.437158\ttraining's average_precision: 0.993697\n",
      "Total Logloss: 0.6591085570770997, Total confident objects precision: 0.7104743083003953, Total % of confident objects: 0.18543289051763628\n",
      "Total test Logloss: 0.6607095613168987, Total test confident objects precision: 0.7315789473684211, Total % of test confident objects: 0.16843971631205673\n"
     ]
    }
   ],
   "source": [
    "def model_train(train_df, test_df, features, task_type, how, n_folds, low_bound, high_bound): \n",
    "    oof = np.zeros([train_df['target'].shape[0], 1])\n",
    "    oof_test = np.zeros([test_df['target'].shape[0], 1])\n",
    "\n",
    "    X, X_test, groups = train_df[features], test_df[features], train_df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(train_df['pattern'], drop_first=True)], axis=1)\n",
    "    X_test = pd.concat([X_test, pd.get_dummies(test_df['pattern'], drop_first=True)], axis=1)\n",
    "    \n",
    "    if task_type == 'cls':\n",
    "        y = train_df['target']\n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "    else:\n",
    "        y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "        kf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    oe_enc = OrdinalEncoder()\n",
    "    groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "    print(f\"Training with {len(features)} features\")\n",
    "    \n",
    "    if how == 'lreg':\n",
    "        scaler = StandardScaler()\n",
    "        X[X.columns] = scaler.fit_transform(X)\n",
    "    \n",
    "    for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "        print(f'Fold #{fold + 1}')\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        models = list()\n",
    "        if how == 'lgbm':\n",
    "            if task_type == 'cls':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                          eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                          eval_metric='mse', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "        elif how == 'lreg':\n",
    "            if task_type == 'cls':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "            else:\n",
    "                model = LinearRegression(positive=True)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            prec_score, prec_obj_pct = confident_score(y_val, val_preds[:,1], low_bound, high_bound)\n",
    "            print(f'Logloss: {val_score}, Confident objects precision: {prec_score}, % of confident objects: {prec_obj_pct}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "        else:\n",
    "            val_preds = model.predict(X_val)\n",
    "            val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            print('RMSE: {val_score}')\n",
    "            oof[val_idx, 0] = val_preds\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "    # fit model on full dataset and predict on test\n",
    "    print(\"Test fold\")\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X, y, eval_set=[(X, y)], \n",
    "              eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "    oof_test[:,0] = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    return oof, oof_test, models\n",
    "\n",
    "def confident_score(y, oof, low_bound, high_bound):\n",
    "    ''' Consider only high confident objects for accuracy and precision scores calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    pred_conf_acc = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "    pred_conf_prec = pred_conf[(oof > high_bound)]\n",
    "\n",
    "    y_conf_acc = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "    y_conf_prec = y.values.reshape(-1,1)[(oof > high_bound)]\n",
    "\n",
    "    return precision_score(y_conf_prec, pred_conf_prec), y_conf_prec.shape[0]/y.shape[0]\n",
    "\n",
    "# task_type = 'reg'\n",
    "task_type = 'cls'\n",
    "\n",
    "# best params for classification\n",
    "if task_type == 'cls':\n",
    "    params = {\n",
    "        'boosting_type': 'dart',\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.02,\n",
    "        #   'early_stopping_round': 50,\n",
    "        'max_depth': 10,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'subsample': 0.85,\n",
    "        'subsample_freq': 1,\n",
    "        'num_leaves': 25,\n",
    "        'verbosity': -1,\n",
    "        'max_bin': 255,\n",
    "        'reg_alpha': 1e-6,\n",
    "        'reg_lambda': 1e-8,\n",
    "        'objective': 'binary',\n",
    "        # 'is_unbalance': True,\n",
    "        # 'class_weight': 'balanced',\n",
    "        'metric': 'average_precision'\n",
    "        }\n",
    "else:\n",
    "    # best params for regression\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.021,\n",
    "            'early_stopping_round': 100,\n",
    "            'max_depth': 7,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.8,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 27,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 511,\n",
    "            'reg_alpha': 1e-4,\n",
    "            'reg_lambda': 1e-4,\n",
    "            'objective': 'regression'\n",
    "            }\n",
    "\n",
    "if CFG.train_LGBM:\n",
    "    low_bound, high_bound = 0.38, 0.62\n",
    "    features = res['Feature'].head(150)\n",
    "    oof, oof_test, models = model_train(train_df, test_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound)\n",
    "\n",
    "    if task_type == 'cls':\n",
    "        y = train_df['target']\n",
    "        oof_val_score = log_loss(y, oof)\n",
    "        oof_conf_prec_score, oof_conf_obj_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "        print(f'Total Logloss: {oof_val_score}, Total confident objects precision: {oof_conf_prec_score}, Total % of confident objects: {oof_conf_obj_pct}')\n",
    "        y_test = test_df['target']\n",
    "        test_val_score = log_loss(y_test, oof_test)\n",
    "        test_conf_prec_score, test_conf_obj_pct = confident_score(y_test, oof_test, low_bound, high_bound)\n",
    "        print(f'Total test Logloss: {test_val_score}, Total test confident objects precision: {test_conf_prec_score}, Total % of test confident objects: {test_conf_obj_pct}')\n",
    "    else:\n",
    "        y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "        display(mean_squared_error(y, oof, squared=False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Logloss: 0.6596041772081067, Total confident objects precision: 0.7153925619834711, Total % of confident objects: 0.17737059092991297\n",
    "\n",
    "Total test Logloss: 0.6608237932967793, Total test confident objects precision: 0.7421052631578947, Total % of test confident objects: 0.16843971631205673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#         'boosting_type': 'dart',\n",
    "#         'n_estimators': 1900,\n",
    "#         'learning_rate': 0.02,\n",
    "#         #   'early_stopping_round': 50,\n",
    "#         'max_depth': 10,\n",
    "#         'colsample_bytree': 0.7,\n",
    "#         'subsample': 0.85,\n",
    "#         'subsample_freq': 1,\n",
    "#         'num_leaves': 25,\n",
    "#         'verbosity': -1,\n",
    "#         'max_bin': 255,\n",
    "#         'reg_alpha': 1e-6,\n",
    "#         'reg_lambda': 1e-8,\n",
    "#         'objective': 'binary',\n",
    "#         # 'is_unbalance': True,\n",
    "#         # 'class_weight': 'balanced',\n",
    "#         'metric': 'average_precision'\n",
    "#         }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Logloss: 0.6582101740689003, Total confident objects precision: 0.7227138643067846, Total % of confident objects: 0.17532971295577968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_features = list()\n",
    "# low_bound, high_bound = 0.4, 0.6\n",
    "# features = res.groupby('Feature')['rank'].sum().sort_values().head(150).index.to_list()\n",
    "# oof, models = model_train(train_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound)\n",
    "# baseline_prec, baseline_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "# print(f'Total confident objects precision: {baseline_prec}')\n",
    "\n",
    "# for feat in tqdm(features[30:]):\n",
    "#     tmp_features = [f for f in features if f != feat]\n",
    "#     oof, models = model_train(train_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound)\n",
    "#     prec_score, pct = confident_score(y, oof, low_bound, high_bound)\n",
    "#     print(f'Feature: {feat}, Total precision: {prec_score}, Baseline precision: {baseline_prec}, Total pct: {pct}, Baseline precision: {baseline_pct}')\n",
    "#     if prec_score > baseline_prec and pct > baseline_pct:\n",
    "        \n",
    "#         bad_features.append(feat)\n",
    "#         print(bad_features)\n",
    "\n",
    "# bad_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150: Total Logloss: 0.6079201456679827, Total confident objects precision: 0.7093922651933702, Total % of confident objects: 0.07814523788964683"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
