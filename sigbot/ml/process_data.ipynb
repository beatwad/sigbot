{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from os import environ\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from indicators import indicators\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from config.config import ConfigFactory\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import log_loss, mean_squared_error, precision_score\n",
    "\n",
    "import torch\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    collect_data = False # create new dataset or load previous\n",
    "    select_features = False\n",
    "    train_NN = False\n",
    "    train_LGBM = True\n",
    "    n_repeats = 1\n",
    "    n_folds = 5\n",
    "    cls_target_ratio = 1.021"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_angle_prev_72</th>\n",
       "      <th>macd_prev_72</th>\n",
       "      <th>macdsignal_prev_72</th>\n",
       "      <th>macdhist_prev_72</th>\n",
       "      <th>macd_dir_prev_72</th>\n",
       "      <th>macdsignal_dir_prev_72</th>\n",
       "      <th>atr_prev_72</th>\n",
       "      <th>close_smooth_prev_72</th>\n",
       "      <th>target</th>\n",
       "      <th>ttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-10 21:00:00</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>227818.00</td>\n",
       "      <td>58.380200</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>34.523810</td>\n",
       "      <td>0.205808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.856226</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.318808</td>\n",
       "      <td>0.227337</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-15 15:00:00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>250135.00</td>\n",
       "      <td>58.463239</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>53.174603</td>\n",
       "      <td>0.357436</td>\n",
       "      <td>...</td>\n",
       "      <td>5.250146</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.229362</td>\n",
       "      <td>0.110533</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.999742</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-21 19:00:00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>49801.00</td>\n",
       "      <td>45.480088</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>61.904762</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>5.078633</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-1.898082</td>\n",
       "      <td>-0.192558</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>16.2600</td>\n",
       "      <td>16.2800</td>\n",
       "      <td>16.2200</td>\n",
       "      <td>16.2600</td>\n",
       "      <td>8091.48</td>\n",
       "      <td>34.890095</td>\n",
       "      <td>26.002932</td>\n",
       "      <td>23.959198</td>\n",
       "      <td>0.064161</td>\n",
       "      <td>...</td>\n",
       "      <td>10.131495</td>\n",
       "      <td>0.101978</td>\n",
       "      <td>0.165148</td>\n",
       "      <td>-0.063170</td>\n",
       "      <td>-0.103585</td>\n",
       "      <td>-0.067033</td>\n",
       "      <td>0.161075</td>\n",
       "      <td>16.473750</td>\n",
       "      <td>1</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>6.8900</td>\n",
       "      <td>6.9100</td>\n",
       "      <td>6.8900</td>\n",
       "      <td>6.9000</td>\n",
       "      <td>3986.13</td>\n",
       "      <td>30.533005</td>\n",
       "      <td>14.404762</td>\n",
       "      <td>18.392857</td>\n",
       "      <td>-0.204445</td>\n",
       "      <td>...</td>\n",
       "      <td>11.812509</td>\n",
       "      <td>0.028818</td>\n",
       "      <td>-0.003283</td>\n",
       "      <td>0.032101</td>\n",
       "      <td>-0.946146</td>\n",
       "      <td>-0.453481</td>\n",
       "      <td>0.088674</td>\n",
       "      <td>6.850417</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time     open     high      low    close     volume  \\\n",
       "0 2022-09-10 21:00:00   0.9999   0.9999   0.9998   0.9998  227818.00   \n",
       "1 2022-09-15 15:00:00   1.0000   1.0000   0.9999   1.0000  250135.00   \n",
       "2 2022-09-21 19:00:00   1.0000   1.0000   0.9999   0.9999   49801.00   \n",
       "3 2022-12-25 15:00:00  16.2600  16.2800  16.2200  16.2600    8091.48   \n",
       "4 2022-12-25 15:00:00   6.8900   6.9100   6.8900   6.9000    3986.13   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  58.380200    35.714286    34.523810         0.205808  ...   \n",
       "1  58.463239    64.285714    53.174603         0.357436  ...   \n",
       "2  45.480088    57.142857    61.904762        -0.066667  ...   \n",
       "3  34.890095    26.002932    23.959198         0.064161  ...   \n",
       "4  30.533005    14.404762    18.392857        -0.204445  ...   \n",
       "\n",
       "   linear_reg_angle_prev_72  macd_prev_72  macdsignal_prev_72  \\\n",
       "0                 -0.856226     -0.000020           -0.000010   \n",
       "1                  5.250146      0.000016            0.000011   \n",
       "2                  5.078633      0.000006           -0.000004   \n",
       "3                 10.131495      0.101978            0.165148   \n",
       "4                 11.812509      0.028818           -0.003283   \n",
       "\n",
       "   macdhist_prev_72  macd_dir_prev_72  macdsignal_dir_prev_72  atr_prev_72  \\\n",
       "0         -0.000009          0.318808                0.227337     0.000114   \n",
       "1          0.000006          0.229362                0.110533     0.000113   \n",
       "2          0.000010         -1.898082               -0.192558     0.000113   \n",
       "3         -0.063170         -0.103585               -0.067033     0.161075   \n",
       "4          0.032101         -0.946146               -0.453481     0.088674   \n",
       "\n",
       "   close_smooth_prev_72  target  ttype  \n",
       "0              1.000004       0    buy  \n",
       "1              0.999742       0    buy  \n",
       "2              0.999988       0    buy  \n",
       "3             16.473750       1   sell  \n",
       "4              6.850417       0   sell  \n",
       "\n",
       "[5 rows x 385 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(12043, 385)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"1h_4h\"\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "def get_file(ticker):\n",
    "    ''' Find files buy ticker names, file names can be in different formats '''\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def add_indicators(df, ttype, configs):\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, '', '', 0)\n",
    "    # add RSI\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, '', '', 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df = trend.get_indicator(df, '', '', 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df = macd.get_indicator(df, '', '', 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, '', '', 0)\n",
    "    # add SMA\n",
    "    # sma = indicators.SMA(ttype, configs)\n",
    "    # df = sma.get_indicator(df, '', '', 0)\n",
    "    return df\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step):\n",
    "    ''' Create train dataset from signal statistics and ticker candle data'''\n",
    "    train_df = pd.DataFrame()\n",
    "    tickers = df['ticker'].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df['ticker'] == ticker]\n",
    "        times = signal_df['time']\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        tmp_df_1h = add_indicators(tmp_df_1h, ttype, configs)\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc('pattern')]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h['time'] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + step, step):\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h['time'] == time_prev, :].reset_index(drop=True)\n",
    "                    row_tmp.columns = [c + f'_prev_{i}' for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row['ticker'] = ticker\n",
    "                row['pattern'] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "\n",
    "            row['target'] = 0\n",
    "            \n",
    "            if row['pattern'].values == 'STOCH_RSI':\n",
    "                if ttype == 'buy':\n",
    "                    row['ttype'] = 'sell'\n",
    "                else:\n",
    "                    row['ttype'] = 'buy'\n",
    "            else:\n",
    "                row['ttype'] = ttype\n",
    "            \n",
    "            # Ff ttype = buy and during the selected period high price was higher than close_price * target_ratio\n",
    "            # and earlier low price wasn't lower than close_price / target_ratio, than target is True, else target is False.\n",
    "            # Similarly for ttype = sell \n",
    "            close_price = tmp_df_1h.loc[tmp_df_1h['time'] == t, 'close'].values\n",
    "            \n",
    "            for i in range(1, target_offset + 1):\n",
    "                time_next = t + timedelta(hours=i)\n",
    "                target_buy = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'high'].reset_index(drop=True)\n",
    "                target_sell = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'low'].reset_index(drop=True)\n",
    "\n",
    "                try:\n",
    "                    target_buy = target_buy > close_price * CFG.cls_target_ratio\n",
    "                    target_sell = target_sell < close_price / CFG.cls_target_ratio\n",
    "                except ValueError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    if (row['ttype'].values == 'buy' and target_sell[0]) or (row['ttype'].values == 'sell' and target_buy[0]):\n",
    "                        break\n",
    "                    elif (row['ttype'].values == 'buy' and target_buy[0]) or (row['ttype'].values == 'sell' and target_sell[0]):\n",
    "                        row['target'] = 1\n",
    "                        break\n",
    "                except KeyError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "            \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "\n",
    "            # add data to the dataset\n",
    "            if train_df.shape[0] == 0:\n",
    "                train_df = row\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, row])\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "# for how long time (in hours) we want to predict\n",
    "target_offset = 24\n",
    "# first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "first = 4\n",
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 72\n",
    "# step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "step = 4\n",
    "\n",
    "if CFG.collect_data is True:\n",
    "    # Buy\n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "    # dataset for model train\n",
    "    train_buy = create_train_df(df, 'buy', configs, target_offset, first, last, step)\n",
    "    train_buy = train_buy.dropna()\n",
    "\n",
    "    # Sell\n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "    # dataset for model train\n",
    "    train_sell = create_train_df(df, 'sell', configs, target_offset, first, last, step)\n",
    "    train_sell = train_sell.dropna()\n",
    "\n",
    "    df = pd.concat([train_buy, train_sell]).sort_values('time').reset_index(drop=True)\n",
    "    df.to_pickle(f'signal_stat/train_df_{last}.pkl')\n",
    "else:\n",
    "    df = pd.read_pickle(f'signal_stat/train_df_{last}.pkl')\n",
    "\n",
    "display(df.head())\n",
    "display(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stablecoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_angle_prev_72</th>\n",
       "      <th>macd_prev_72</th>\n",
       "      <th>macdsignal_prev_72</th>\n",
       "      <th>macdhist_prev_72</th>\n",
       "      <th>macd_dir_prev_72</th>\n",
       "      <th>macdsignal_dir_prev_72</th>\n",
       "      <th>atr_prev_72</th>\n",
       "      <th>close_smooth_prev_72</th>\n",
       "      <th>target</th>\n",
       "      <th>ttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>16.26000</td>\n",
       "      <td>16.28000</td>\n",
       "      <td>16.2200</td>\n",
       "      <td>16.26000</td>\n",
       "      <td>8091.48</td>\n",
       "      <td>34.890095</td>\n",
       "      <td>26.002932</td>\n",
       "      <td>23.959198</td>\n",
       "      <td>0.064161</td>\n",
       "      <td>...</td>\n",
       "      <td>10.131495</td>\n",
       "      <td>0.101978</td>\n",
       "      <td>0.165148</td>\n",
       "      <td>-0.063170</td>\n",
       "      <td>-0.103585</td>\n",
       "      <td>-0.067033</td>\n",
       "      <td>0.161075</td>\n",
       "      <td>16.473750</td>\n",
       "      <td>1</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>6.89000</td>\n",
       "      <td>6.91000</td>\n",
       "      <td>6.8900</td>\n",
       "      <td>6.90000</td>\n",
       "      <td>3986.13</td>\n",
       "      <td>30.533005</td>\n",
       "      <td>14.404762</td>\n",
       "      <td>18.392857</td>\n",
       "      <td>-0.204445</td>\n",
       "      <td>...</td>\n",
       "      <td>11.812509</td>\n",
       "      <td>0.028818</td>\n",
       "      <td>-0.003283</td>\n",
       "      <td>0.032101</td>\n",
       "      <td>-0.946146</td>\n",
       "      <td>-0.453481</td>\n",
       "      <td>0.088674</td>\n",
       "      <td>6.850417</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>0.34710</td>\n",
       "      <td>0.34780</td>\n",
       "      <td>0.3462</td>\n",
       "      <td>0.34780</td>\n",
       "      <td>3774232.00</td>\n",
       "      <td>35.117650</td>\n",
       "      <td>17.786212</td>\n",
       "      <td>17.135809</td>\n",
       "      <td>-0.046770</td>\n",
       "      <td>...</td>\n",
       "      <td>4.279320</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.227125</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.344346</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>0.39700</td>\n",
       "      <td>0.39800</td>\n",
       "      <td>0.3930</td>\n",
       "      <td>0.39600</td>\n",
       "      <td>304916.80</td>\n",
       "      <td>36.897427</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>43.650794</td>\n",
       "      <td>-0.069109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303562</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>-0.292488</td>\n",
       "      <td>-0.109834</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.398667</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>0.01755</td>\n",
       "      <td>0.01755</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.01754</td>\n",
       "      <td>4566524.60</td>\n",
       "      <td>21.094090</td>\n",
       "      <td>8.823657</td>\n",
       "      <td>9.251647</td>\n",
       "      <td>-0.033617</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053866</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372109</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.017925</td>\n",
       "      <td>1</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time      open      high      low     close      volume  \\\n",
       "0 2022-12-25 15:00:00  16.26000  16.28000  16.2200  16.26000     8091.48   \n",
       "1 2022-12-25 15:00:00   6.89000   6.91000   6.8900   6.90000     3986.13   \n",
       "2 2022-12-25 15:00:00   0.34710   0.34780   0.3462   0.34780  3774232.00   \n",
       "3 2022-12-25 15:00:00   0.39700   0.39800   0.3930   0.39600   304916.80   \n",
       "4 2022-12-25 15:00:00   0.01755   0.01755   0.0175   0.01754  4566524.60   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  34.890095    26.002932    23.959198         0.064161  ...   \n",
       "1  30.533005    14.404762    18.392857        -0.204445  ...   \n",
       "2  35.117650    17.786212    17.135809        -0.046770  ...   \n",
       "3  36.897427    42.857143    43.650794        -0.069109  ...   \n",
       "4  21.094090     8.823657     9.251647        -0.033617  ...   \n",
       "\n",
       "   linear_reg_angle_prev_72  macd_prev_72  macdsignal_prev_72  \\\n",
       "0                 10.131495      0.101978            0.165148   \n",
       "1                 11.812509      0.028818           -0.003283   \n",
       "2                  4.279320      0.000238           -0.000280   \n",
       "3                  0.303562     -0.000151           -0.000647   \n",
       "4                  2.053866      0.000039            0.000020   \n",
       "\n",
       "   macdhist_prev_72  macd_dir_prev_72  macdsignal_dir_prev_72  atr_prev_72  \\\n",
       "0         -0.063170         -0.103585               -0.067033     0.161075   \n",
       "1          0.032101         -0.946146               -0.453481     0.088674   \n",
       "2          0.000518          0.000000               -0.227125     0.002465   \n",
       "3          0.000496         -0.292488               -0.109834     0.003108   \n",
       "4          0.000019          0.000000                0.372109     0.000129   \n",
       "\n",
       "   close_smooth_prev_72  target  ttype  \n",
       "0             16.473750       1   sell  \n",
       "1              6.850417       0   sell  \n",
       "2              0.344346       0   sell  \n",
       "3              0.398667       0   sell  \n",
       "4              0.017925       1   sell  \n",
       "\n",
       "[5 rows x 385 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11988, 385)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df[df['ticker'] != 'TUSDUSDT'].reset_index(drop=True)\n",
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check target correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1000\n",
    "\n",
    "# x = train_df.loc[(train_df.target == 1) & (train_df.ttype == 'buy'), ['ticker', 'ttype', 'pattern', 'time', 'close', 'target']]\n",
    "# y = x.iloc[i]\n",
    "# low_price, high_price = y['close'] / CFG.cls_target_ratio, y['close'] * CFG.cls_target_ratio,\n",
    "# print(y['ticker'], y['time'], y['ttype'])\n",
    "\n",
    "# tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{y[\"ticker\"]}_1h.pkl')\n",
    "# # tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{y[\"ticker\"][:-4]}-{y[\"ticker\"][-4:]}_1h.pkl')\n",
    "# # tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{y[\"ticker\"][:-4]}-{y[\"ticker\"][-4:]}-SWAP_4h.pkl')\n",
    "\n",
    "# tmp_df_1h['low_price'] = low_price\n",
    "# tmp_df_1h['high_price'] = high_price\n",
    "# idx = tmp_df_1h[tmp_df_1h['time'] == y['time']].index[0]\n",
    "\n",
    "# tmp_df_1h = tmp_df_1h.iloc[idx:idx+24][['time', 'close', 'high', 'high_price', 'low', 'low_price']]\n",
    "\n",
    "# if y['ttype'] == 'buy':\n",
    "#     tmp_df_1h['signal'] = tmp_df_1h['high'] > tmp_df_1h['high_price']\n",
    "#     tmp_df_1h['anti_signal'] = tmp_df_1h['low'] < tmp_df_1h['low_price']\n",
    "# else:\n",
    "#     tmp_df_1h['signal'] = tmp_df_1h['low'] < tmp_df_1h['low_price']\n",
    "#     tmp_df_1h['anti_signal'] = tmp_df_1h['high'] > tmp_df_1h['high_price']\n",
    "\n",
    "# tmp_df_1h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split by ticker group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.1, n_splits=2, random_state = 7)\n",
    "\n",
    "split = splitter.split(df, groups=df['ticker'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_df = df.iloc[train_inds]\n",
    "test_df = df.iloc[test_inds]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_size=0.2\n",
    "\n",
    "x_data = train_df.drop(['target', 'time', 'ticker', 'pattern', 'ttype'], axis=1)\n",
    "y_data = train_df['target']\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n",
    "input_channels = x_data.shape[1]\n",
    "output_size = len(set(y_data))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train[x_train.columns] = scaler.fit_transform(x_train)\n",
    "x_valid[x_valid.columns] = scaler.transform(x_valid)\n",
    "\n",
    "x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "x_valid = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_train), type(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(SigModel, self, ).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(input_channels, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "        self.layers.add_module('sigmoid', torch.nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train).squeeze()\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train)\n",
    "    train_acc = (train_preds.round() == y_train).float().mean()\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad() # ~ model.zero_grad()\n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid).squeeze()\n",
    "        val_loss = criterion(val_preds, y_valid)\n",
    "        val_acc = (val_preds.round() == y_valid).float().mean()\n",
    "    \n",
    "    # update weights according to gradient value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# Initialize model\n",
    "if CFG.train_NN:\n",
    "    model = SigModel(input_channels).to(device)\n",
    "\n",
    "    # Number of epochs\n",
    "    epochs = 100000\n",
    "\n",
    "    # Send data to the device\n",
    "    x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "    y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "    train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    # Empty loss lists to track values\n",
    "    epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    learning_rate = 1e-6\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2000, threshold=1e-2)\n",
    "\n",
    "    # Loop through the data\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc, val_loss, val_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "        # Print progress a total of 20 times\n",
    "        if epoch % int(epochs / 20) == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.5f}\\\n",
    "                Validation Loss: {val_loss:.5f}, Val Acc: {val_acc:.5f}\\\n",
    "                    LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]}')\n",
    "\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(train_loss.cpu().detach().numpy())\n",
    "            valid_loss_values.append(val_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NN train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CFG.train_NN:\n",
    "    plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "    plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss Curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern', 'ttype']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target'] >= df['close']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.01,\n",
    "        #   'early_stopping_round': 100,\n",
    "          'max_depth': 7,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree': 0.75,\n",
    "          'num_leaves': 32,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "if CFG.select_features:\n",
    "    perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(df, permut=True, boruta=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.select_features:\n",
    "    perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "    boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "    feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "    fi = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "    fi = fi.groupby('Feature')['rank'].sum().sort_values().reset_index()\n",
    "    fi.to_csv('feature_importance.csv')\n",
    "else:\n",
    "    fi = pd.read_csv('feature_importance.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test split based on ticker groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 150 features\n",
      "Fold #1\n",
      "[100]\tvalid_0's binary_logloss: 0.678262\tvalid_0's average_precision: 0.638358\n",
      "[200]\tvalid_0's binary_logloss: 0.673603\tvalid_0's average_precision: 0.640675\n",
      "[300]\tvalid_0's binary_logloss: 0.669645\tvalid_0's average_precision: 0.644619\n",
      "[400]\tvalid_0's binary_logloss: 0.66644\tvalid_0's average_precision: 0.645051\n",
      "[500]\tvalid_0's binary_logloss: 0.664827\tvalid_0's average_precision: 0.644699\n",
      "[600]\tvalid_0's binary_logloss: 0.664263\tvalid_0's average_precision: 0.647787\n",
      "[700]\tvalid_0's binary_logloss: 0.664232\tvalid_0's average_precision: 0.647425\n",
      "[800]\tvalid_0's binary_logloss: 0.66444\tvalid_0's average_precision: 0.645029\n",
      "[900]\tvalid_0's binary_logloss: 0.663914\tvalid_0's average_precision: 0.646479\n",
      "[1000]\tvalid_0's binary_logloss: 0.662805\tvalid_0's average_precision: 0.647901\n",
      "[1100]\tvalid_0's binary_logloss: 0.662065\tvalid_0's average_precision: 0.649652\n",
      "[1200]\tvalid_0's binary_logloss: 0.661035\tvalid_0's average_precision: 0.651003\n",
      "[1300]\tvalid_0's binary_logloss: 0.659736\tvalid_0's average_precision: 0.651368\n",
      "[1400]\tvalid_0's binary_logloss: 0.658554\tvalid_0's average_precision: 0.652924\n",
      "[1500]\tvalid_0's binary_logloss: 0.65844\tvalid_0's average_precision: 0.652364\n",
      "[1600]\tvalid_0's binary_logloss: 0.657348\tvalid_0's average_precision: 0.655421\n",
      "[1700]\tvalid_0's binary_logloss: 0.65745\tvalid_0's average_precision: 0.654152\n",
      "[1800]\tvalid_0's binary_logloss: 0.657188\tvalid_0's average_precision: 0.656404\n",
      "[1900]\tvalid_0's binary_logloss: 0.657533\tvalid_0's average_precision: 0.656297\n",
      "[2000]\tvalid_0's binary_logloss: 0.658239\tvalid_0's average_precision: 0.653917\n",
      "Logloss: 0.6582387333408225, Confident objects precision: 0.7212121212121212, % of confident objects: 0.1819184123484013\n",
      "Fold #2\n",
      "[100]\tvalid_0's binary_logloss: 0.677223\tvalid_0's average_precision: 0.63612\n",
      "[200]\tvalid_0's binary_logloss: 0.671608\tvalid_0's average_precision: 0.642989\n",
      "[300]\tvalid_0's binary_logloss: 0.667331\tvalid_0's average_precision: 0.646482\n",
      "[400]\tvalid_0's binary_logloss: 0.66459\tvalid_0's average_precision: 0.647383\n",
      "[500]\tvalid_0's binary_logloss: 0.661939\tvalid_0's average_precision: 0.650848\n",
      "[600]\tvalid_0's binary_logloss: 0.661553\tvalid_0's average_precision: 0.651765\n",
      "[700]\tvalid_0's binary_logloss: 0.660205\tvalid_0's average_precision: 0.65372\n",
      "[800]\tvalid_0's binary_logloss: 0.659029\tvalid_0's average_precision: 0.654862\n",
      "[900]\tvalid_0's binary_logloss: 0.656861\tvalid_0's average_precision: 0.658861\n",
      "[1000]\tvalid_0's binary_logloss: 0.655829\tvalid_0's average_precision: 0.658923\n",
      "[1100]\tvalid_0's binary_logloss: 0.655372\tvalid_0's average_precision: 0.658827\n",
      "[1200]\tvalid_0's binary_logloss: 0.654651\tvalid_0's average_precision: 0.660023\n",
      "[1300]\tvalid_0's binary_logloss: 0.654385\tvalid_0's average_precision: 0.66065\n",
      "[1400]\tvalid_0's binary_logloss: 0.654435\tvalid_0's average_precision: 0.658409\n",
      "[1500]\tvalid_0's binary_logloss: 0.653676\tvalid_0's average_precision: 0.65905\n",
      "[1600]\tvalid_0's binary_logloss: 0.653273\tvalid_0's average_precision: 0.659198\n",
      "[1700]\tvalid_0's binary_logloss: 0.65259\tvalid_0's average_precision: 0.659763\n",
      "[1800]\tvalid_0's binary_logloss: 0.65134\tvalid_0's average_precision: 0.662787\n",
      "[1900]\tvalid_0's binary_logloss: 0.651044\tvalid_0's average_precision: 0.663174\n",
      "[2000]\tvalid_0's binary_logloss: 0.650581\tvalid_0's average_precision: 0.663299\n",
      "Logloss: 0.6505814714593866, Confident objects precision: 0.710239651416122, % of confident objects: 0.21132596685082872\n",
      "Fold #3\n",
      "[100]\tvalid_0's binary_logloss: 0.677954\tvalid_0's average_precision: 0.639962\n",
      "[200]\tvalid_0's binary_logloss: 0.673781\tvalid_0's average_precision: 0.647853\n",
      "[300]\tvalid_0's binary_logloss: 0.670019\tvalid_0's average_precision: 0.654245\n",
      "[400]\tvalid_0's binary_logloss: 0.667516\tvalid_0's average_precision: 0.657505\n",
      "[500]\tvalid_0's binary_logloss: 0.665575\tvalid_0's average_precision: 0.657409\n",
      "[600]\tvalid_0's binary_logloss: 0.664671\tvalid_0's average_precision: 0.660261\n",
      "[700]\tvalid_0's binary_logloss: 0.662386\tvalid_0's average_precision: 0.665321\n",
      "[800]\tvalid_0's binary_logloss: 0.662875\tvalid_0's average_precision: 0.663734\n",
      "[900]\tvalid_0's binary_logloss: 0.662065\tvalid_0's average_precision: 0.664203\n",
      "[1000]\tvalid_0's binary_logloss: 0.661613\tvalid_0's average_precision: 0.664333\n",
      "[1100]\tvalid_0's binary_logloss: 0.661404\tvalid_0's average_precision: 0.664748\n",
      "[1200]\tvalid_0's binary_logloss: 0.661573\tvalid_0's average_precision: 0.665399\n",
      "[1300]\tvalid_0's binary_logloss: 0.66285\tvalid_0's average_precision: 0.662628\n",
      "[1400]\tvalid_0's binary_logloss: 0.662141\tvalid_0's average_precision: 0.665012\n",
      "[1500]\tvalid_0's binary_logloss: 0.661877\tvalid_0's average_precision: 0.666993\n",
      "[1600]\tvalid_0's binary_logloss: 0.660805\tvalid_0's average_precision: 0.669168\n",
      "[1700]\tvalid_0's binary_logloss: 0.660683\tvalid_0's average_precision: 0.66911\n",
      "[1800]\tvalid_0's binary_logloss: 0.660855\tvalid_0's average_precision: 0.669643\n",
      "[1900]\tvalid_0's binary_logloss: 0.661\tvalid_0's average_precision: 0.668924\n",
      "[2000]\tvalid_0's binary_logloss: 0.660546\tvalid_0's average_precision: 0.670021\n",
      "Logloss: 0.6605462340952304, Confident objects precision: 0.7156626506024096, % of confident objects: 0.20708582834331338\n",
      "Fold #4\n",
      "[100]\tvalid_0's binary_logloss: 0.681447\tvalid_0's average_precision: 0.603249\n",
      "[200]\tvalid_0's binary_logloss: 0.678626\tvalid_0's average_precision: 0.609744\n",
      "[300]\tvalid_0's binary_logloss: 0.676449\tvalid_0's average_precision: 0.618276\n",
      "[400]\tvalid_0's binary_logloss: 0.675025\tvalid_0's average_precision: 0.620942\n",
      "[500]\tvalid_0's binary_logloss: 0.674705\tvalid_0's average_precision: 0.622425\n",
      "[600]\tvalid_0's binary_logloss: 0.673826\tvalid_0's average_precision: 0.627924\n",
      "[700]\tvalid_0's binary_logloss: 0.67405\tvalid_0's average_precision: 0.624805\n",
      "[800]\tvalid_0's binary_logloss: 0.673376\tvalid_0's average_precision: 0.626822\n",
      "[900]\tvalid_0's binary_logloss: 0.673254\tvalid_0's average_precision: 0.627066\n",
      "[1000]\tvalid_0's binary_logloss: 0.672331\tvalid_0's average_precision: 0.628133\n",
      "[1100]\tvalid_0's binary_logloss: 0.672414\tvalid_0's average_precision: 0.627506\n",
      "[1200]\tvalid_0's binary_logloss: 0.672259\tvalid_0's average_precision: 0.628518\n",
      "[1300]\tvalid_0's binary_logloss: 0.672723\tvalid_0's average_precision: 0.627614\n",
      "[1400]\tvalid_0's binary_logloss: 0.673029\tvalid_0's average_precision: 0.627937\n",
      "[1500]\tvalid_0's binary_logloss: 0.67269\tvalid_0's average_precision: 0.62907\n",
      "[1600]\tvalid_0's binary_logloss: 0.672918\tvalid_0's average_precision: 0.62959\n",
      "[1700]\tvalid_0's binary_logloss: 0.673057\tvalid_0's average_precision: 0.629781\n",
      "[1800]\tvalid_0's binary_logloss: 0.672605\tvalid_0's average_precision: 0.631283\n",
      "[1900]\tvalid_0's binary_logloss: 0.672697\tvalid_0's average_precision: 0.630641\n",
      "[2000]\tvalid_0's binary_logloss: 0.673701\tvalid_0's average_precision: 0.628386\n",
      "Logloss: 0.673700884934535, Confident objects precision: 0.6812933025404158, % of confident objects: 0.17544570502431117\n",
      "Fold #5\n",
      "[100]\tvalid_0's binary_logloss: 0.679281\tvalid_0's average_precision: 0.632393\n",
      "[200]\tvalid_0's binary_logloss: 0.674864\tvalid_0's average_precision: 0.636853\n",
      "[300]\tvalid_0's binary_logloss: 0.67065\tvalid_0's average_precision: 0.645823\n",
      "[400]\tvalid_0's binary_logloss: 0.669052\tvalid_0's average_precision: 0.645497\n",
      "[500]\tvalid_0's binary_logloss: 0.668151\tvalid_0's average_precision: 0.643043\n",
      "[600]\tvalid_0's binary_logloss: 0.667672\tvalid_0's average_precision: 0.644117\n",
      "[700]\tvalid_0's binary_logloss: 0.66628\tvalid_0's average_precision: 0.646073\n",
      "[800]\tvalid_0's binary_logloss: 0.665572\tvalid_0's average_precision: 0.645255\n",
      "[900]\tvalid_0's binary_logloss: 0.665449\tvalid_0's average_precision: 0.643743\n",
      "[1000]\tvalid_0's binary_logloss: 0.665483\tvalid_0's average_precision: 0.640871\n",
      "[1100]\tvalid_0's binary_logloss: 0.665304\tvalid_0's average_precision: 0.641071\n",
      "[1200]\tvalid_0's binary_logloss: 0.665551\tvalid_0's average_precision: 0.641414\n",
      "[1300]\tvalid_0's binary_logloss: 0.664985\tvalid_0's average_precision: 0.641803\n",
      "[1400]\tvalid_0's binary_logloss: 0.664723\tvalid_0's average_precision: 0.642388\n",
      "[1500]\tvalid_0's binary_logloss: 0.664418\tvalid_0's average_precision: 0.641143\n",
      "[1600]\tvalid_0's binary_logloss: 0.664458\tvalid_0's average_precision: 0.639566\n",
      "[1700]\tvalid_0's binary_logloss: 0.66348\tvalid_0's average_precision: 0.643046\n",
      "[1800]\tvalid_0's binary_logloss: 0.663359\tvalid_0's average_precision: 0.642458\n",
      "[1900]\tvalid_0's binary_logloss: 0.663038\tvalid_0's average_precision: 0.642495\n",
      "[2000]\tvalid_0's binary_logloss: 0.663392\tvalid_0's average_precision: 0.641503\n",
      "Logloss: 0.6633920515175953, Confident objects precision: 0.7016706443914081, % of confident objects: 0.1713000817661488\n",
      "Train on full data\n",
      "[100]\ttraining's binary_logloss: 0.6584\ttraining's average_precision: 0.764301\n",
      "[200]\ttraining's binary_logloss: 0.642405\ttraining's average_precision: 0.790899\n",
      "[300]\ttraining's binary_logloss: 0.625744\ttraining's average_precision: 0.818097\n",
      "[400]\ttraining's binary_logloss: 0.610418\ttraining's average_precision: 0.847126\n",
      "[500]\ttraining's binary_logloss: 0.594501\ttraining's average_precision: 0.870055\n",
      "[600]\ttraining's binary_logloss: 0.587507\ttraining's average_precision: 0.886393\n",
      "[700]\ttraining's binary_logloss: 0.575013\ttraining's average_precision: 0.901879\n",
      "[800]\ttraining's binary_logloss: 0.566488\ttraining's average_precision: 0.916211\n",
      "[900]\ttraining's binary_logloss: 0.554856\ttraining's average_precision: 0.929061\n",
      "[1000]\ttraining's binary_logloss: 0.546102\ttraining's average_precision: 0.939854\n",
      "[1100]\ttraining's binary_logloss: 0.536289\ttraining's average_precision: 0.949669\n",
      "[1200]\ttraining's binary_logloss: 0.530571\ttraining's average_precision: 0.955181\n",
      "[1300]\ttraining's binary_logloss: 0.522246\ttraining's average_precision: 0.961605\n",
      "[1400]\ttraining's binary_logloss: 0.511338\ttraining's average_precision: 0.968106\n",
      "[1500]\ttraining's binary_logloss: 0.505929\ttraining's average_precision: 0.972559\n",
      "[1600]\ttraining's binary_logloss: 0.495997\ttraining's average_precision: 0.976945\n",
      "[1700]\ttraining's binary_logloss: 0.486484\ttraining's average_precision: 0.980654\n",
      "[1800]\ttraining's binary_logloss: 0.478221\ttraining's average_precision: 0.98389\n",
      "[1900]\ttraining's binary_logloss: 0.469716\ttraining's average_precision: 0.98631\n",
      "[2000]\ttraining's binary_logloss: 0.462968\ttraining's average_precision: 0.988476\n",
      "Total Logloss: 0.6617932330747756, Total confident objects precision: 0.7052529182879378, Total % of confident objects: 0.18855465884079237\n"
     ]
    }
   ],
   "source": [
    "def model_train(df, train_df, test_df, features, task_type, how, n_folds, low_bound, high_bound, train_test): \n",
    "    oof = np.zeros([train_df['target'].shape[0], 1])\n",
    "    oof_test = np.zeros([test_df['target'].shape[0], 1])\n",
    "\n",
    "    X, groups = train_df[features], train_df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(train_df['pattern'], drop_first=True)], axis=1)\n",
    "    \n",
    "    if task_type == 'cls':\n",
    "        y = train_df['target']\n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "    else:\n",
    "        y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "        kf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    oe_enc = OrdinalEncoder()\n",
    "    groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "    print(f\"Training with {len(features)} features\")\n",
    "    \n",
    "    if how == 'lreg':\n",
    "        scaler = StandardScaler()\n",
    "        X[X.columns] = scaler.fit_transform(X)\n",
    "    \n",
    "    for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "        print(f'Fold #{fold + 1}')\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        models = list()\n",
    "        if how == 'lgbm':\n",
    "            if task_type == 'cls':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                          eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                          eval_metric='mse', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "        elif how == 'lreg':\n",
    "            if task_type == 'cls':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "            else:\n",
    "                model = LinearRegression(positive=True)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            prec_score, prec_obj_pct = confident_score(y_val, val_preds[:,1], low_bound, high_bound)\n",
    "            print(f'Logloss: {val_score}, Confident objects precision: {prec_score}, % of confident objects: {prec_obj_pct}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "        else:\n",
    "            val_preds = model.predict(X_val)\n",
    "            val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            print('RMSE: {val_score}')\n",
    "            oof[val_idx, 0] = val_preds\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    if train_test == 'train':\n",
    "        print('Train on full data')\n",
    "        X, y = df[features], df['target']\n",
    "        X = pd.concat([X, pd.get_dummies(df['pattern'], drop_first=True)], axis=1)\n",
    "        model.fit(X, y, eval_set=[(X, y)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "    else:\n",
    "        # fit model on full dataset and predict on test\n",
    "        print(\"Test fold\")\n",
    "        X, y = test_df[features], test_df['target']\n",
    "        X = pd.concat([X, pd.get_dummies(test_df['pattern'], drop_first=True)], axis=1)\n",
    "        model.fit(X, y, eval_set=[(X, y)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        oof_test[:,0] = model.predict_proba(X)[:,1]\n",
    "    \n",
    "    return oof, oof_test, model\n",
    "\n",
    "def prepare_features(fi):\n",
    "    ''' Get features, sort them by their time appearance and return for using in train and inference'''\n",
    "    fi = fi['Feature'].head(150)\n",
    "    feature_dict = defaultdict(list)\n",
    "    features = list()\n",
    "\n",
    "    for f in fi:\n",
    "        period = f.split('_')\n",
    "        if period[-1].isdigit():\n",
    "            feature_dict[int(period[-1])].append('_'.join(period[:-2]))\n",
    "        else:\n",
    "            feature_dict[0].append(f)\n",
    "\n",
    "    feature_dict = dict(sorted(feature_dict.items()))\n",
    "    \n",
    "    for item in feature_dict.items():\n",
    "        if item[0] > 0:\n",
    "            features.extend([i + f'_prev_{item[0]}' for i in item[1]])\n",
    "        else:\n",
    "            features.extend([i for i in item[1]])\n",
    "\n",
    "    feature_dict['features'] = features + ['Pattern_Trend', 'STOCH_RSI']\n",
    "\n",
    "    return features, feature_dict\n",
    "\n",
    "def confident_score(y, oof, low_bound, high_bound):\n",
    "    ''' Consider only high confident objects for accuracy and precision scores calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    # pred_conf_acc = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "    pred_conf_prec = pred_conf[(oof > high_bound)]\n",
    "    # y_conf_acc = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "    y_conf_prec = y.values.reshape(-1,1)[(oof > high_bound)]\n",
    "\n",
    "    return precision_score(y_conf_prec, pred_conf_prec), y_conf_prec.shape[0]/y.shape[0]\n",
    "\n",
    "# task_type = 'reg'\n",
    "task_type = 'cls'\n",
    "\n",
    "# best params for classification\n",
    "if task_type == 'cls':\n",
    "    params = {\n",
    "        'boosting_type': 'dart',\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.02,\n",
    "        #   'early_stopping_round': 50,\n",
    "        'max_depth': 10,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'subsample': 0.85,\n",
    "        'subsample_freq': 1,\n",
    "        'num_leaves': 25,\n",
    "        'verbosity': -1,\n",
    "        'max_bin': 255,\n",
    "        'reg_alpha': 1e-6,\n",
    "        'reg_lambda': 1e-8,\n",
    "        'objective': 'binary',\n",
    "        # 'is_unbalance': True,\n",
    "        # 'class_weight': 'balanced',\n",
    "        'metric': 'average_precision'\n",
    "        }\n",
    "else:\n",
    "    # best params for regression\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.021,\n",
    "            'early_stopping_round': 100,\n",
    "            'max_depth': 7,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.8,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 27,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 511,\n",
    "            'reg_alpha': 1e-4,\n",
    "            'reg_lambda': 1e-4,\n",
    "            'objective': 'regression'\n",
    "            }\n",
    "\n",
    "if CFG.train_LGBM:\n",
    "    fi = pd.read_csv('feature_importance.csv')\n",
    "    train_test = 'train'\n",
    "    low_bound, high_bound = 0.38, 0.62\n",
    "    features, feature_dict = prepare_features(fi)\n",
    "    oof, oof_test, model = model_train(df, train_df, test_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound, train_test=train_test)\n",
    "\n",
    "    if task_type == 'cls':\n",
    "        y = train_df['target']\n",
    "        oof_val_score = log_loss(y, oof)\n",
    "        oof_conf_prec_score, oof_conf_obj_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "        print(f'Total Logloss: {oof_val_score}, Total confident objects precision: {oof_conf_prec_score}, Total % of confident objects: {oof_conf_obj_pct}')\n",
    "        if train_test == 'test':\n",
    "            y_test = test_df['target']\n",
    "            test_val_score = log_loss(y_test, oof_test)\n",
    "            test_conf_prec_score, test_conf_obj_pct = confident_score(y_test, oof_test, low_bound, high_bound)\n",
    "            print(f'Total test Logloss: {test_val_score}, Total test confident objects precision: {test_conf_prec_score}, Total % of test confident objects: {test_conf_obj_pct}')\n",
    "    else:\n",
    "        y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "        display(mean_squared_error(y, oof, squared=False))\n",
    "\n",
    "    # save feature dictionary for further inference\n",
    "    with open(f'features.json', 'w') as f:\n",
    "        json.dump(feature_dict, f)\n",
    "\n",
    "if train_test == 'train':\n",
    "    model.booster_.save_model('lgbm.pkl')\n",
    "    joblib.dump(model, 'lgbm.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Logloss: 0.6596041772081067, Total confident objects precision: 0.7153925619834711, Total % of confident objects: 0.17737059092991297\n",
    "\n",
    "Total test Logloss: 0.6608237932967793, Total test confident objects precision: 0.7421052631578947, Total % of test confident objects: 0.16843971631205673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_features = list()\n",
    "# low_bound, high_bound = 0.4, 0.6\n",
    "# features = fi.groupby('Feature')['rank'].sum().sort_values().head(150).index.to_list()\n",
    "# oof, models = model_train(train_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound)\n",
    "# baseline_prec, baseline_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "# print(f'Total confident objects precision: {baseline_prec}')\n",
    "\n",
    "# for feat in tqdm(features[30:]):\n",
    "#     tmp_features = [f for f in features if f != feat]\n",
    "#     oof, models = model_train(train_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound)\n",
    "#     prec_score, pct = confident_score(y, oof, low_bound, high_bound)\n",
    "#     print(f'Feature: {feat}, Total precision: {prec_score}, Baseline precision: {baseline_prec}, Total pct: {pct}, Baseline precision: {baseline_pct}')\n",
    "#     if prec_score > baseline_prec and pct > baseline_pct:\n",
    "        \n",
    "#         bad_features.append(feat)\n",
    "#         print(bad_features)\n",
    "\n",
    "# bad_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150: Total Logloss: 0.6079201456679827, Total confident objects precision: 0.7093922651933702, Total % of confident objects: 0.07814523788964683"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
