{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b2227e883e4c7fa5ddb988f4445b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51c6d22ca484f398fdd1247411c69d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_prev_48</th>\n",
       "      <th>linear_reg_angle_prev_48</th>\n",
       "      <th>macd_prev_48</th>\n",
       "      <th>macdsignal_prev_48</th>\n",
       "      <th>macdhist_prev_48</th>\n",
       "      <th>macd_dir_prev_48</th>\n",
       "      <th>macdsignal_dir_prev_48</th>\n",
       "      <th>atr_prev_48</th>\n",
       "      <th>close_smooth_prev_48</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-10 21:00:00</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>227818.000</td>\n",
       "      <td>58.380200</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>34.523810</td>\n",
       "      <td>0.205808</td>\n",
       "      <td>...</td>\n",
       "      <td>22.072420</td>\n",
       "      <td>3.511172</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>1.467330</td>\n",
       "      <td>0.043803</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-24 11:00:00</td>\n",
       "      <td>0.6560</td>\n",
       "      <td>0.6570</td>\n",
       "      <td>0.6550</td>\n",
       "      <td>0.6550</td>\n",
       "      <td>797.660</td>\n",
       "      <td>34.767502</td>\n",
       "      <td>21.821036</td>\n",
       "      <td>16.977731</td>\n",
       "      <td>0.254408</td>\n",
       "      <td>...</td>\n",
       "      <td>42.201288</td>\n",
       "      <td>-18.370972</td>\n",
       "      <td>-0.006460</td>\n",
       "      <td>-0.007129</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.020892</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>0.674292</td>\n",
       "      <td>0.6550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-24 12:00:00</td>\n",
       "      <td>68.1000</td>\n",
       "      <td>69.1000</td>\n",
       "      <td>68.0000</td>\n",
       "      <td>68.7000</td>\n",
       "      <td>1712.657</td>\n",
       "      <td>25.443832</td>\n",
       "      <td>13.872597</td>\n",
       "      <td>11.125943</td>\n",
       "      <td>0.187309</td>\n",
       "      <td>...</td>\n",
       "      <td>35.878666</td>\n",
       "      <td>10.537428</td>\n",
       "      <td>0.371309</td>\n",
       "      <td>0.686006</td>\n",
       "      <td>-0.314697</td>\n",
       "      <td>-0.112998</td>\n",
       "      <td>-0.099589</td>\n",
       "      <td>1.140238</td>\n",
       "      <td>73.870833</td>\n",
       "      <td>69.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-24 16:00:00</td>\n",
       "      <td>1.5450</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>1.5420</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>30778.600</td>\n",
       "      <td>38.085151</td>\n",
       "      <td>21.873965</td>\n",
       "      <td>19.685508</td>\n",
       "      <td>0.065530</td>\n",
       "      <td>...</td>\n",
       "      <td>47.570096</td>\n",
       "      <td>-18.038119</td>\n",
       "      <td>-0.009375</td>\n",
       "      <td>-0.010977</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.050115</td>\n",
       "      <td>0.015902</td>\n",
       "      <td>1.598875</td>\n",
       "      <td>1.5060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-24 18:00:00</td>\n",
       "      <td>0.1778</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.1776</td>\n",
       "      <td>123162.000</td>\n",
       "      <td>34.573072</td>\n",
       "      <td>19.576850</td>\n",
       "      <td>16.777965</td>\n",
       "      <td>0.140748</td>\n",
       "      <td>...</td>\n",
       "      <td>36.983668</td>\n",
       "      <td>-26.734391</td>\n",
       "      <td>-0.001127</td>\n",
       "      <td>-0.000703</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>0.296932</td>\n",
       "      <td>0.114185</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.186696</td>\n",
       "      <td>0.1694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time     open     high      low    close      volume  \\\n",
       "0 2022-09-10 21:00:00   0.9999   0.9999   0.9998   0.9998  227818.000   \n",
       "1 2022-12-24 11:00:00   0.6560   0.6570   0.6550   0.6550     797.660   \n",
       "2 2022-12-24 12:00:00  68.1000  69.1000  68.0000  68.7000    1712.657   \n",
       "3 2022-12-24 16:00:00   1.5450   1.5500   1.5420   1.5500   30778.600   \n",
       "4 2022-12-24 18:00:00   0.1778   0.1779   0.1774   0.1776  123162.000   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  58.380200    35.714286    34.523810         0.205808  ...   \n",
       "1  34.767502    21.821036    16.977731         0.254408  ...   \n",
       "2  25.443832    13.872597    11.125943         0.187309  ...   \n",
       "3  38.085151    21.873965    19.685508         0.065530  ...   \n",
       "4  34.573072    19.576850    16.777965         0.140748  ...   \n",
       "\n",
       "   linear_reg_prev_48  linear_reg_angle_prev_48  macd_prev_48  \\\n",
       "0           22.072420                  3.511172     -0.000010   \n",
       "1           42.201288                -18.370972     -0.006460   \n",
       "2           35.878666                 10.537428      0.371309   \n",
       "3           47.570096                -18.038119     -0.009375   \n",
       "4           36.983668                -26.734391     -0.001127   \n",
       "\n",
       "   macdsignal_prev_48  macdhist_prev_48  macd_dir_prev_48  \\\n",
       "0           -0.000006         -0.000004          1.467330   \n",
       "1           -0.007129          0.000669          0.000000   \n",
       "2            0.686006         -0.314697         -0.112998   \n",
       "3           -0.010977          0.001602          0.000000   \n",
       "4           -0.000703         -0.000423          0.296932   \n",
       "\n",
       "   macdsignal_dir_prev_48  atr_prev_48  close_smooth_prev_48   target  \n",
       "0                0.043803     0.000108              0.999946   0.9998  \n",
       "1               -0.020892     0.004426              0.674292   0.6550  \n",
       "2               -0.099589     1.140238             73.870833  69.8000  \n",
       "3               -0.050115     0.015902              1.598875   1.5060  \n",
       "4                0.114185     0.001354              0.186696   0.1694  \n",
       "\n",
       "[5 rows x 264 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(12242, 264)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from indicators import indicators\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from config.config import ConfigFactory\n",
    "\n",
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"1h_4h\"\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "def get_file(ticker):\n",
    "    ''' Find files buy ticker names, file names can be in different formats '''\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def add_indicators(df, ttype, configs):\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, '', '', 0)\n",
    "    # add RSI\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, '', '', 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df = trend.get_indicator(df, '', '', 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df = macd.get_indicator(df, '', '', 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, '', '', 0)\n",
    "    # add SMA\n",
    "    # sma = indicators.SMA(ttype, configs)\n",
    "    # df = sma.get_indicator(df, '', '', 0)\n",
    "    return df\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step):\n",
    "    ''' Create train dataset from signal statistics and ticker candle data'''\n",
    "    train_df = pd.DataFrame()\n",
    "    tickers = df['ticker'].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df['ticker'] == ticker]\n",
    "        times = signal_df['time']\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        tmp_df_1h = add_indicators(tmp_df_1h, ttype, configs)\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc('pattern')]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h['time'] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + step, step):\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h['time'] == time_prev, :].reset_index(drop=True)\n",
    "                    row_tmp.columns = [c + f'_prev_{i}' for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row['ticker'] = ticker\n",
    "                row['pattern'] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "            \n",
    "            # add target\n",
    "            time_next = t + timedelta(hours=target_offset)\n",
    "            if ttype == 'buy':\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'high'].reset_index(drop=True)\n",
    "            else:\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'low'].reset_index(drop=True)\n",
    "\n",
    "            target.name = 'target'\n",
    "            rows = pd.concat([row, target], axis=1)\n",
    "            \n",
    "            # add data to the dataset\n",
    "            if train_df.shape[0] == 0:\n",
    "                train_df = rows\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, rows])\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "# for how long time (in hours) we want to predict\n",
    "target_offset = 24\n",
    "# first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "first = 4\n",
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 48\n",
    "# step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "step = 4\n",
    "\n",
    "# Buy\n",
    "# dataset with the signal statistics\n",
    "df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "# dataset for model train\n",
    "train_buy = create_train_df(df, 'buy', configs, target_offset, first, last, step)\n",
    "train_buy = train_buy.dropna()\n",
    "\n",
    "# Sell\n",
    "# dataset with the signal statistics\n",
    "df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "# dataset for model train\n",
    "train_sell = create_train_df(df, 'sell', configs, target_offset, first, last, step)\n",
    "train_sell = train_sell.dropna()\n",
    "\n",
    "train_df = pd.concat([train_buy, train_sell]).sort_values('time').reset_index(drop=True)\n",
    "display(train_df.head())\n",
    "display(train_df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat \u001b[1m\u001b[34m#1\n",
      "[100]\ttraining's binary_logloss: 0.666445\tvalid_1's binary_logloss: 0.685064\n",
      "[200]\ttraining's binary_logloss: 0.653676\tvalid_1's binary_logloss: 0.682842\n",
      "[300]\ttraining's binary_logloss: 0.639283\tvalid_1's binary_logloss: 0.6797\n",
      "[400]\ttraining's binary_logloss: 0.626068\tvalid_1's binary_logloss: 0.677556\n",
      "[500]\ttraining's binary_logloss: 0.611998\tvalid_1's binary_logloss: 0.674979\n",
      "[600]\ttraining's binary_logloss: 0.605685\tvalid_1's binary_logloss: 0.673847\n",
      "[700]\ttraining's binary_logloss: 0.59495\tvalid_1's binary_logloss: 0.67257\n",
      "[800]\ttraining's binary_logloss: 0.587397\tvalid_1's binary_logloss: 0.67147\n",
      "[900]\ttraining's binary_logloss: 0.577149\tvalid_1's binary_logloss: 0.670331\n",
      "[1000]\ttraining's binary_logloss: 0.569968\tvalid_1's binary_logloss: 0.669587\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| loss: \u001b[1m\u001b[34m0.66959\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.666793\tvalid_1's binary_logloss: 0.682736\n",
      "[200]\ttraining's binary_logloss: 0.65397\tvalid_1's binary_logloss: 0.678806\n",
      "[300]\ttraining's binary_logloss: 0.639669\tvalid_1's binary_logloss: 0.674723\n",
      "[400]\ttraining's binary_logloss: 0.62666\tvalid_1's binary_logloss: 0.671453\n",
      "[500]\ttraining's binary_logloss: 0.612984\tvalid_1's binary_logloss: 0.668915\n",
      "[600]\ttraining's binary_logloss: 0.607452\tvalid_1's binary_logloss: 0.66793\n",
      "[700]\ttraining's binary_logloss: 0.597264\tvalid_1's binary_logloss: 0.666214\n",
      "[800]\ttraining's binary_logloss: 0.590612\tvalid_1's binary_logloss: 0.665309\n",
      "[900]\ttraining's binary_logloss: 0.581504\tvalid_1's binary_logloss: 0.66481\n",
      "[1000]\ttraining's binary_logloss: 0.574728\tvalid_1's binary_logloss: 0.664409\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| loss: \u001b[1m\u001b[34m0.66441\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.66746\tvalid_1's binary_logloss: 0.682265\n",
      "[200]\ttraining's binary_logloss: 0.655029\tvalid_1's binary_logloss: 0.677988\n",
      "[300]\ttraining's binary_logloss: 0.640831\tvalid_1's binary_logloss: 0.674263\n",
      "[400]\ttraining's binary_logloss: 0.62785\tvalid_1's binary_logloss: 0.670998\n",
      "[500]\ttraining's binary_logloss: 0.614009\tvalid_1's binary_logloss: 0.66681\n",
      "[600]\ttraining's binary_logloss: 0.608319\tvalid_1's binary_logloss: 0.665714\n",
      "[700]\ttraining's binary_logloss: 0.59802\tvalid_1's binary_logloss: 0.663525\n",
      "[800]\ttraining's binary_logloss: 0.590617\tvalid_1's binary_logloss: 0.66212\n",
      "[900]\ttraining's binary_logloss: 0.581368\tvalid_1's binary_logloss: 0.660651\n",
      "[1000]\ttraining's binary_logloss: 0.574616\tvalid_1's binary_logloss: 0.660102\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| loss: \u001b[1m\u001b[34m0.66010\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.667427\tvalid_1's binary_logloss: 0.682427\n",
      "[200]\ttraining's binary_logloss: 0.655495\tvalid_1's binary_logloss: 0.678842\n",
      "[300]\ttraining's binary_logloss: 0.641621\tvalid_1's binary_logloss: 0.675486\n",
      "[400]\ttraining's binary_logloss: 0.627673\tvalid_1's binary_logloss: 0.671859\n",
      "[500]\ttraining's binary_logloss: 0.613272\tvalid_1's binary_logloss: 0.669012\n",
      "[600]\ttraining's binary_logloss: 0.606952\tvalid_1's binary_logloss: 0.667978\n",
      "[700]\ttraining's binary_logloss: 0.596298\tvalid_1's binary_logloss: 0.666604\n",
      "[800]\ttraining's binary_logloss: 0.58854\tvalid_1's binary_logloss: 0.665903\n",
      "[900]\ttraining's binary_logloss: 0.577869\tvalid_1's binary_logloss: 0.664814\n",
      "[1000]\ttraining's binary_logloss: 0.570627\tvalid_1's binary_logloss: 0.663991\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| loss: \u001b[1m\u001b[34m0.66399\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.66728\tvalid_1's binary_logloss: 0.680892\n",
      "[200]\ttraining's binary_logloss: 0.654398\tvalid_1's binary_logloss: 0.675887\n",
      "[300]\ttraining's binary_logloss: 0.639729\tvalid_1's binary_logloss: 0.671025\n",
      "[400]\ttraining's binary_logloss: 0.626594\tvalid_1's binary_logloss: 0.66759\n",
      "[500]\ttraining's binary_logloss: 0.613475\tvalid_1's binary_logloss: 0.664394\n",
      "[600]\ttraining's binary_logloss: 0.607595\tvalid_1's binary_logloss: 0.663911\n",
      "[700]\ttraining's binary_logloss: 0.597489\tvalid_1's binary_logloss: 0.662263\n",
      "[800]\ttraining's binary_logloss: 0.589706\tvalid_1's binary_logloss: 0.660844\n",
      "[900]\ttraining's binary_logloss: 0.579497\tvalid_1's binary_logloss: 0.659459\n",
      "[1000]\ttraining's binary_logloss: 0.573034\tvalid_1's binary_logloss: 0.65867\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| loss: \u001b[1m\u001b[34m0.65867\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "\u001b[1m\u001b[31m Outer Holdout avg score: \u001b[0m log_loss: \u001b[1m\u001b[31m0.66317\u001b[0m\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "class CFG:\n",
    "    n_repeats = 1\n",
    "    n_folds = 5\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target'] >= df['close']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.01,\n",
    "          'early_stopping_round': 100,\n",
    "          'max_depth': 7,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree': 0.75,\n",
    "          'num_leaves': 32,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(train_df, permut=True, boruta=True)\n",
    "\n",
    "# perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "# boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "# feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "# res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "# res.groupby('Feature')['rank'].sum().sort_values().head(30).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature\n",
       "stoch_slowk_dir_prev_4        4.0\n",
       "stoch_slowd_dir_prev_4        9.5\n",
       "stoch_slowd_dir_prev_48      12.0\n",
       "macdhist_prev_40             13.0\n",
       "stoch_diff_prev_48           17.0\n",
       "stoch_slowk                  55.0\n",
       "rsi_prev_12                  87.5\n",
       "linear_reg_angle            145.0\n",
       "linear_reg_angle_prev_28    148.0\n",
       "stoch_slowd_prev_40         159.0\n",
       "stoch_slowk_prev_16         171.0\n",
       "rsi_prev_44                 173.0\n",
       "linear_reg_angle_prev_16    174.0\n",
       "linear_reg_prev_8           177.0\n",
       "stoch_slowd                 182.0\n",
       "rsi                         183.0\n",
       "linear_reg_angle_prev_24    185.0\n",
       "stoch_diff_prev_32          186.0\n",
       "stoch_diff                  186.0\n",
       "stoch_slowk_dir_prev_32     188.0\n",
       "stoch_slowk_dir_prev_48     189.0\n",
       "rsi_prev_48                 192.0\n",
       "rsi_prev_24                 196.0\n",
       "linear_reg_angle_prev_44    198.0\n",
       "rsi_prev_8                  203.0\n",
       "macdsignal_dir_prev_40      206.0\n",
       "stoch_slowk_prev_48         209.0\n",
       "stoch_slowd_prev_36         212.0\n",
       "linear_reg_angle_prev_20    213.0\n",
       "volume_prev_4               213.0\n",
       "Name: rank, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "res.groupby('Feature')['rank'].sum().sort_values().head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 30 features\n",
      "[100]\tvalid_0's binary_logloss: 0.680215\n",
      "[200]\tvalid_0's binary_logloss: 0.675149\n",
      "[300]\tvalid_0's binary_logloss: 0.671369\n",
      "[400]\tvalid_0's binary_logloss: 0.669378\n",
      "[500]\tvalid_0's binary_logloss: 0.667129\n",
      "[600]\tvalid_0's binary_logloss: 0.667367\n",
      "[700]\tvalid_0's binary_logloss: 0.66606\n",
      "[800]\tvalid_0's binary_logloss: 0.66536\n",
      "[900]\tvalid_0's binary_logloss: 0.664277\n",
      "Logloss: 0.6642772955253781, Confident objects accuracy: 0.746875\n",
      "[100]\tvalid_0's binary_logloss: 0.675612\n",
      "[200]\tvalid_0's binary_logloss: 0.670721\n",
      "[300]\tvalid_0's binary_logloss: 0.667151\n",
      "[400]\tvalid_0's binary_logloss: 0.664309\n",
      "[500]\tvalid_0's binary_logloss: 0.661912\n",
      "[600]\tvalid_0's binary_logloss: 0.661403\n",
      "[700]\tvalid_0's binary_logloss: 0.659943\n",
      "[800]\tvalid_0's binary_logloss: 0.659438\n",
      "[900]\tvalid_0's binary_logloss: 0.658975\n",
      "Logloss: 0.6589753823927194, Confident objects accuracy: 0.7472826086956522\n",
      "[100]\tvalid_0's binary_logloss: 0.676342\n",
      "[200]\tvalid_0's binary_logloss: 0.671086\n",
      "[300]\tvalid_0's binary_logloss: 0.666605\n",
      "[400]\tvalid_0's binary_logloss: 0.66437\n",
      "[500]\tvalid_0's binary_logloss: 0.66079\n",
      "[600]\tvalid_0's binary_logloss: 0.659961\n",
      "[700]\tvalid_0's binary_logloss: 0.657541\n",
      "[800]\tvalid_0's binary_logloss: 0.657322\n",
      "[900]\tvalid_0's binary_logloss: 0.656615\n",
      "Logloss: 0.6566154563890136, Confident objects accuracy: 0.7815126050420168\n",
      "[100]\tvalid_0's binary_logloss: 0.675993\n",
      "[200]\tvalid_0's binary_logloss: 0.671729\n",
      "[300]\tvalid_0's binary_logloss: 0.667826\n",
      "[400]\tvalid_0's binary_logloss: 0.665514\n",
      "[500]\tvalid_0's binary_logloss: 0.662946\n",
      "[600]\tvalid_0's binary_logloss: 0.661851\n",
      "[700]\tvalid_0's binary_logloss: 0.661014\n",
      "[800]\tvalid_0's binary_logloss: 0.660387\n",
      "[900]\tvalid_0's binary_logloss: 0.659098\n",
      "Logloss: 0.6590977197409298, Confident objects accuracy: 0.7580645161290323\n",
      "[100]\tvalid_0's binary_logloss: 0.674376\n",
      "[200]\tvalid_0's binary_logloss: 0.668795\n",
      "[300]\tvalid_0's binary_logloss: 0.66438\n",
      "[400]\tvalid_0's binary_logloss: 0.660906\n",
      "[500]\tvalid_0's binary_logloss: 0.658513\n",
      "[600]\tvalid_0's binary_logloss: 0.657878\n",
      "[700]\tvalid_0's binary_logloss: 0.65656\n",
      "[800]\tvalid_0's binary_logloss: 0.655138\n",
      "[900]\tvalid_0's binary_logloss: 0.654171\n",
      "Logloss: 0.6541714406026568, Confident objects accuracy: 0.7831325301204819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6584281345050492"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7641921397379913"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "\n",
    "def model_train(df, task_type, how, n_folds): \n",
    "    oof = np.zeros([df['target'].shape[0], 1])\n",
    "    # features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    features = res.groupby('Feature')['rank'].sum().sort_values().head(30).index.to_list()\n",
    "\n",
    "    X, groups = df[features], df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(df['pattern'], drop_first=True)], axis=1)\n",
    "    \n",
    "    if task_type == 'cls':\n",
    "        y = df['target'] >= df['close']\n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "    else:\n",
    "        y = (df['target'] - df['close']) / df['close']\n",
    "        kf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    oe_enc = OrdinalEncoder()\n",
    "    groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "    print(f\"Training with {len(features)} features\")\n",
    "    \n",
    "    if how == 'lreg':\n",
    "        scaler = StandardScaler()\n",
    "        X[X.columns] = scaler.fit_transform(X)\n",
    "    \n",
    "    for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        models = list()\n",
    "        if how == 'lgbm':\n",
    "            if task_type == 'cls':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='mse', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "        elif how == 'lreg':\n",
    "            if task_type == 'cls':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "            else:\n",
    "                model = LinearRegression(positive=True)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            acc_score = confident_accuracy_score(y_val, val_preds[:,1])\n",
    "            print(f'Logloss: {val_score}, Confident objects accuracy: {acc_score}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "        else:\n",
    "            val_preds = model.predict(X_val)\n",
    "            val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            print('RMSE: {val_score}')\n",
    "            oof[val_idx, 0] = val_preds\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "    return oof, models\n",
    "\n",
    "def confident_accuracy_score(y, oof, low_bound=0.35, high_bound=0.65):\n",
    "    ''' Consider only high confident objects for accuracy score calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    pred_conf = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    y_conf = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    return accuracy_score(y_conf, pred_conf)   \n",
    "\n",
    "# task_type = 'reg'\n",
    "task_type = 'cls'\n",
    "\n",
    "# best params for classification\n",
    "if task_type == 'cls':\n",
    "    params = {\n",
    "            'boosting_type': 'dart',\n",
    "            'n_estimators': 900,\n",
    "            'learning_rate': 0.02,\n",
    "            #   'early_stopping_round': 50,\n",
    "            'max_depth': 10,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.9,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 26,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 255,\n",
    "            'reg_alpha': 1e-5,\n",
    "            'reg_lambda': 1e-7,\n",
    "            'objective': 'binary'\n",
    "            }\n",
    "else:\n",
    "    # best params for regression\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.021,\n",
    "            'early_stopping_round': 100,\n",
    "            'max_depth': 7,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.8,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 27,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 511,\n",
    "            'reg_alpha': 1e-4,\n",
    "            'reg_lambda': 1e-4,\n",
    "            'objective': 'regression'\n",
    "            }\n",
    "\n",
    "oof, models = model_train(train_df, task_type=task_type, how='lgbm', n_folds=5) # 0.061096263508601985 / logloss: 0.6226973017816237 acc: 0.6424792139077853\n",
    "# oof, models = model_train(train_df, task_type=task_type, how='lreg', n_folds=5) # 0.06958035063954768 / logloss: 0.6985539186132326 acc: 0.587892049598833\n",
    "\n",
    "if task_type == 'cls':\n",
    "    y = train_df['target'] >= train_df['close']\n",
    "    low_bound, high_bound = 0.35, 0.65\n",
    "    display(log_loss(y, oof))\n",
    "    display(confident_accuracy_score(y, oof, low_bound, high_bound))\n",
    "else:\n",
    "    y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "    display(mean_squared_error(y, oof, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6584281345050492\n",
    "\n",
    "# 0.7641921397379913"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
