{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e55f84fc8074d6d9174d371a262e9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c0a365677e423197aa3f24247aeec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_prev_48</th>\n",
       "      <th>linear_reg_angle_prev_48</th>\n",
       "      <th>macd_prev_48</th>\n",
       "      <th>macdsignal_prev_48</th>\n",
       "      <th>macdhist_prev_48</th>\n",
       "      <th>macd_dir_prev_48</th>\n",
       "      <th>macdsignal_dir_prev_48</th>\n",
       "      <th>atr_prev_48</th>\n",
       "      <th>close_smooth_prev_48</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-25 11:00:00</td>\n",
       "      <td>46.2600</td>\n",
       "      <td>46.3600</td>\n",
       "      <td>46.2200</td>\n",
       "      <td>46.3200</td>\n",
       "      <td>118.178</td>\n",
       "      <td>46.491941</td>\n",
       "      <td>25.582139</td>\n",
       "      <td>30.565279</td>\n",
       "      <td>-0.153935</td>\n",
       "      <td>...</td>\n",
       "      <td>15.836865</td>\n",
       "      <td>-4.003925</td>\n",
       "      <td>-0.110585</td>\n",
       "      <td>-0.156218</td>\n",
       "      <td>0.045632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.078445</td>\n",
       "      <td>0.460133</td>\n",
       "      <td>46.190833</td>\n",
       "      <td>45.8300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-25 19:00:00</td>\n",
       "      <td>1.2920</td>\n",
       "      <td>1.2930</td>\n",
       "      <td>1.2800</td>\n",
       "      <td>1.2920</td>\n",
       "      <td>24194.500</td>\n",
       "      <td>27.640603</td>\n",
       "      <td>16.651064</td>\n",
       "      <td>16.746801</td>\n",
       "      <td>-0.150625</td>\n",
       "      <td>...</td>\n",
       "      <td>10.488927</td>\n",
       "      <td>4.181646</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.152035</td>\n",
       "      <td>-0.013956</td>\n",
       "      <td>0.010223</td>\n",
       "      <td>1.352333</td>\n",
       "      <td>1.3220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-26 11:00:00</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.9164</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>0.9116</td>\n",
       "      <td>74806.210</td>\n",
       "      <td>46.098886</td>\n",
       "      <td>73.599792</td>\n",
       "      <td>71.800194</td>\n",
       "      <td>0.083753</td>\n",
       "      <td>...</td>\n",
       "      <td>23.329862</td>\n",
       "      <td>5.584309</td>\n",
       "      <td>-0.003143</td>\n",
       "      <td>-0.000664</td>\n",
       "      <td>-0.002479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.144364</td>\n",
       "      <td>0.030074</td>\n",
       "      <td>0.977667</td>\n",
       "      <td>0.9273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-26 11:00:00</td>\n",
       "      <td>31.6700</td>\n",
       "      <td>31.7000</td>\n",
       "      <td>31.5700</td>\n",
       "      <td>31.6000</td>\n",
       "      <td>978.546</td>\n",
       "      <td>37.067652</td>\n",
       "      <td>39.271895</td>\n",
       "      <td>51.004334</td>\n",
       "      <td>-0.195484</td>\n",
       "      <td>...</td>\n",
       "      <td>21.090391</td>\n",
       "      <td>-17.363179</td>\n",
       "      <td>-0.122110</td>\n",
       "      <td>-0.088182</td>\n",
       "      <td>-0.033928</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.098692</td>\n",
       "      <td>0.184513</td>\n",
       "      <td>33.060833</td>\n",
       "      <td>32.6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-26 19:00:00</td>\n",
       "      <td>3.2970</td>\n",
       "      <td>3.3000</td>\n",
       "      <td>3.2950</td>\n",
       "      <td>3.2980</td>\n",
       "      <td>11074.080</td>\n",
       "      <td>27.710780</td>\n",
       "      <td>13.148723</td>\n",
       "      <td>11.582050</td>\n",
       "      <td>0.100909</td>\n",
       "      <td>...</td>\n",
       "      <td>16.509265</td>\n",
       "      <td>-5.070883</td>\n",
       "      <td>-0.003804</td>\n",
       "      <td>-0.002525</td>\n",
       "      <td>-0.001279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273099</td>\n",
       "      <td>0.013469</td>\n",
       "      <td>3.373042</td>\n",
       "      <td>3.3050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time     open     high      low    close     volume  \\\n",
       "0 2022-12-25 11:00:00  46.2600  46.3600  46.2200  46.3200    118.178   \n",
       "1 2022-12-25 19:00:00   1.2920   1.2930   1.2800   1.2920  24194.500   \n",
       "2 2022-12-26 11:00:00   0.9162   0.9164   0.9041   0.9116  74806.210   \n",
       "3 2022-12-26 11:00:00  31.6700  31.7000  31.5700  31.6000    978.546   \n",
       "4 2022-12-26 19:00:00   3.2970   3.3000   3.2950   3.2980  11074.080   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  46.491941    25.582139    30.565279        -0.153935  ...   \n",
       "1  27.640603    16.651064    16.746801        -0.150625  ...   \n",
       "2  46.098886    73.599792    71.800194         0.083753  ...   \n",
       "3  37.067652    39.271895    51.004334        -0.195484  ...   \n",
       "4  27.710780    13.148723    11.582050         0.100909  ...   \n",
       "\n",
       "   linear_reg_prev_48  linear_reg_angle_prev_48  macd_prev_48  \\\n",
       "0           15.836865                 -4.003925     -0.110585   \n",
       "1           10.488927                  4.181646      0.000600   \n",
       "2           23.329862                  5.584309     -0.003143   \n",
       "3           21.090391                -17.363179     -0.122110   \n",
       "4           16.509265                 -5.070883     -0.003804   \n",
       "\n",
       "   macdsignal_prev_48  macdhist_prev_48  macd_dir_prev_48  \\\n",
       "0           -0.156218          0.045632          0.000000   \n",
       "1            0.000702         -0.000102         -0.152035   \n",
       "2           -0.000664         -0.002479          0.000000   \n",
       "3           -0.088182         -0.033928          0.102736   \n",
       "4           -0.002525         -0.001279          0.000000   \n",
       "\n",
       "   macdsignal_dir_prev_48  atr_prev_48  close_smooth_prev_48   target  \n",
       "0               -0.078445     0.460133             46.190833  45.8300  \n",
       "1               -0.013956     0.010223              1.352333   1.3220  \n",
       "2                4.144364     0.030074              0.977667   0.9273  \n",
       "3                0.098692     0.184513             33.060833  32.6200  \n",
       "4                0.273099     0.013469              3.373042   3.3050  \n",
       "\n",
       "[5 rows x 264 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5292, 264)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from indicators import indicators\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from config.config import ConfigFactory\n",
    "\n",
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"1h_4h\"\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "def get_file(ticker):\n",
    "    ''' Find files buy ticker names, file names can be in different formats '''\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def add_indicators(df, ttype, configs):\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, '', '', 0)\n",
    "    # add RSI\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, '', '', 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df = trend.get_indicator(df, '', '', 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df = macd.get_indicator(df, '', '', 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, '', '', 0)\n",
    "    # add SMA\n",
    "    # sma = indicators.SMA(ttype, configs)\n",
    "    # df = sma.get_indicator(df, '', '', 0)\n",
    "    return df\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step):\n",
    "    ''' Create train dataset from signal statistics and ticker candle data'''\n",
    "    train_df = pd.DataFrame()\n",
    "    tickers = df['ticker'].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df['ticker'] == ticker]\n",
    "        times = signal_df['time']\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        tmp_df_1h = add_indicators(tmp_df_1h, ttype, configs)\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc('pattern')]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h['time'] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + step, step):\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h['time'] == time_prev, :].reset_index(drop=True)\n",
    "                    row_tmp.columns = [c + f'_prev_{i}' for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row['ticker'] = ticker\n",
    "                row['pattern'] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "            \n",
    "            # add target\n",
    "            time_next = t + timedelta(hours=target_offset)\n",
    "            if ttype == 'buy':\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'high'].reset_index(drop=True)\n",
    "            else:\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'low'].reset_index(drop=True)\n",
    "\n",
    "            target.name = 'target'\n",
    "            rows = pd.concat([row, target], axis=1)\n",
    "            \n",
    "            # add data to the dataset\n",
    "            if train_df.shape[0] == 0:\n",
    "                train_df = rows\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, rows])\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "# for how long time (in hours) we want to predict\n",
    "target_offset = 24\n",
    "# first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "first = 4\n",
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 48\n",
    "# step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "step = 4\n",
    "\n",
    "# Buy\n",
    "# dataset with the signal statistics\n",
    "df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "# dataset for model train\n",
    "train_buy = create_train_df(df, 'buy', configs, target_offset, first, last, step)\n",
    "train_buy = train_buy.dropna()\n",
    "\n",
    "# Sell\n",
    "# dataset with the signal statistics\n",
    "df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "# dataset for model train\n",
    "train_sell = create_train_df(df, 'sell', configs, target_offset, first, last, step)\n",
    "train_sell = train_sell.dropna()\n",
    "\n",
    "train_df = pd.concat([train_buy, train_sell]).sort_values('time').reset_index(drop=True)\n",
    "display(train_df.head())\n",
    "display(train_df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat \u001b[1m\u001b[34m#1\n",
      "[100]\ttraining's binary_logloss: 0.629555\tvalid_1's binary_logloss: 0.668531\n",
      "[200]\ttraining's binary_logloss: 0.600252\tvalid_1's binary_logloss: 0.658932\n",
      "[300]\ttraining's binary_logloss: 0.570159\tvalid_1's binary_logloss: 0.651191\n",
      "[400]\ttraining's binary_logloss: 0.543204\tvalid_1's binary_logloss: 0.646409\n",
      "[500]\ttraining's binary_logloss: 0.516703\tvalid_1's binary_logloss: 0.643065\n",
      "[600]\ttraining's binary_logloss: 0.504283\tvalid_1's binary_logloss: 0.640452\n",
      "[700]\ttraining's binary_logloss: 0.485808\tvalid_1's binary_logloss: 0.637168\n",
      "[800]\ttraining's binary_logloss: 0.471286\tvalid_1's binary_logloss: 0.633747\n",
      "[900]\ttraining's binary_logloss: 0.453675\tvalid_1's binary_logloss: 0.630768\n",
      "[1000]\ttraining's binary_logloss: 0.442356\tvalid_1's binary_logloss: 0.630699\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| loss: \u001b[1m\u001b[34m0.63070\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.624893\tvalid_1's binary_logloss: 0.671183\n",
      "[200]\ttraining's binary_logloss: 0.594855\tvalid_1's binary_logloss: 0.664903\n",
      "[300]\ttraining's binary_logloss: 0.562845\tvalid_1's binary_logloss: 0.658176\n",
      "[400]\ttraining's binary_logloss: 0.534711\tvalid_1's binary_logloss: 0.652996\n",
      "[500]\ttraining's binary_logloss: 0.507875\tvalid_1's binary_logloss: 0.649433\n",
      "[600]\ttraining's binary_logloss: 0.496489\tvalid_1's binary_logloss: 0.648329\n",
      "[700]\ttraining's binary_logloss: 0.475365\tvalid_1's binary_logloss: 0.647442\n",
      "[800]\ttraining's binary_logloss: 0.460557\tvalid_1's binary_logloss: 0.646685\n",
      "[900]\ttraining's binary_logloss: 0.440626\tvalid_1's binary_logloss: 0.647294\n",
      "[1000]\ttraining's binary_logloss: 0.429941\tvalid_1's binary_logloss: 0.646344\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| loss: \u001b[1m\u001b[34m0.64634\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.62326\tvalid_1's binary_logloss: 0.668062\n",
      "[200]\ttraining's binary_logloss: 0.591302\tvalid_1's binary_logloss: 0.661159\n",
      "[300]\ttraining's binary_logloss: 0.55831\tvalid_1's binary_logloss: 0.655399\n",
      "[400]\ttraining's binary_logloss: 0.528765\tvalid_1's binary_logloss: 0.649532\n",
      "[500]\ttraining's binary_logloss: 0.499347\tvalid_1's binary_logloss: 0.644459\n",
      "[600]\ttraining's binary_logloss: 0.487133\tvalid_1's binary_logloss: 0.642338\n",
      "[700]\ttraining's binary_logloss: 0.465885\tvalid_1's binary_logloss: 0.640094\n",
      "[800]\ttraining's binary_logloss: 0.451498\tvalid_1's binary_logloss: 0.638624\n",
      "[900]\ttraining's binary_logloss: 0.432196\tvalid_1's binary_logloss: 0.637352\n",
      "[1000]\ttraining's binary_logloss: 0.41875\tvalid_1's binary_logloss: 0.637483\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| loss: \u001b[1m\u001b[34m0.63748\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.628784\tvalid_1's binary_logloss: 0.664346\n",
      "[200]\ttraining's binary_logloss: 0.598452\tvalid_1's binary_logloss: 0.655666\n",
      "[300]\ttraining's binary_logloss: 0.567717\tvalid_1's binary_logloss: 0.647131\n",
      "[400]\ttraining's binary_logloss: 0.539443\tvalid_1's binary_logloss: 0.641345\n",
      "[500]\ttraining's binary_logloss: 0.512173\tvalid_1's binary_logloss: 0.636891\n",
      "[600]\ttraining's binary_logloss: 0.501138\tvalid_1's binary_logloss: 0.635003\n",
      "[700]\ttraining's binary_logloss: 0.48128\tvalid_1's binary_logloss: 0.633128\n",
      "[800]\ttraining's binary_logloss: 0.468149\tvalid_1's binary_logloss: 0.632252\n",
      "[900]\ttraining's binary_logloss: 0.449633\tvalid_1's binary_logloss: 0.632003\n",
      "[1000]\ttraining's binary_logloss: 0.436766\tvalid_1's binary_logloss: 0.632537\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| loss: \u001b[1m\u001b[34m0.63254\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.629404\tvalid_1's binary_logloss: 0.663734\n",
      "[200]\ttraining's binary_logloss: 0.59853\tvalid_1's binary_logloss: 0.652461\n",
      "[300]\ttraining's binary_logloss: 0.567593\tvalid_1's binary_logloss: 0.642878\n",
      "[400]\ttraining's binary_logloss: 0.539453\tvalid_1's binary_logloss: 0.635283\n",
      "[500]\ttraining's binary_logloss: 0.511272\tvalid_1's binary_logloss: 0.628636\n",
      "[600]\ttraining's binary_logloss: 0.499246\tvalid_1's binary_logloss: 0.627038\n",
      "[700]\ttraining's binary_logloss: 0.478563\tvalid_1's binary_logloss: 0.623615\n",
      "[800]\ttraining's binary_logloss: 0.464594\tvalid_1's binary_logloss: 0.621482\n",
      "[900]\ttraining's binary_logloss: 0.445588\tvalid_1's binary_logloss: 0.618426\n",
      "[1000]\ttraining's binary_logloss: 0.43287\tvalid_1's binary_logloss: 0.617537\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| loss: \u001b[1m\u001b[34m0.61754\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "\u001b[1m\u001b[31m Outer Holdout avg score: \u001b[0m log_loss: \u001b[1m\u001b[31m0.63288\u001b[0m\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "class CFG:\n",
    "    n_repeats = 1\n",
    "    n_folds = 5\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target'] >= df['close']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.01,\n",
    "          'early_stopping_round': 100,\n",
    "          'max_depth': 7,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree': 0.75,\n",
    "          'num_leaves': 32,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(train_df, permut=True, boruta=True)\n",
    "\n",
    "# perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "# boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "# feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "# res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "# res.groupby('Feature')['rank'].sum().sort_values().head(30).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature\n",
       "stoch_diff                   3.5\n",
       "stoch_slowk_dir              9.0\n",
       "stoch_slowd_prev_8          11.0\n",
       "rsi                         14.0\n",
       "macdsignal_dir              14.0\n",
       "macd_prev_36                27.5\n",
       "linear_reg_angle_prev_8     36.5\n",
       "rsi_prev_4                  37.5\n",
       "macdsignal_prev_36          57.5\n",
       "stoch_slowk_prev_8          64.5\n",
       "stoch_slowd_dir_prev_48     76.5\n",
       "stoch_slowk                 85.5\n",
       "rsi_prev_36                157.5\n",
       "stoch_slowd_dir_prev_4     170.0\n",
       "stoch_slowd_prev_4         176.0\n",
       "stoch_slowk_dir_prev_24    176.0\n",
       "stoch_slowd_prev_12        178.0\n",
       "stoch_diff_prev_4          193.0\n",
       "stoch_slowd_dir_prev_32    197.0\n",
       "stoch_slowk_prev_40        198.0\n",
       "stoch_slowd_dir_prev_40    199.0\n",
       "stoch_diff_prev_8          212.0\n",
       "rsi_prev_8                 212.0\n",
       "linear_reg_prev_44         214.0\n",
       "stoch_diff_prev_36         216.0\n",
       "stoch_slowd                216.0\n",
       "stoch_slowd_dir_prev_8     217.0\n",
       "rsi_prev_24                227.0\n",
       "rsi_prev_44                228.0\n",
       "stoch_slowk_prev_16        231.0\n",
       "Name: rank, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "res.groupby('Feature')['rank'].sum().sort_values().head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 30 features\n",
      "[100]\tvalid_0's binary_logloss: 0.655505\n",
      "[200]\tvalid_0's binary_logloss: 0.645369\n",
      "[300]\tvalid_0's binary_logloss: 0.635218\n",
      "[400]\tvalid_0's binary_logloss: 0.628613\n",
      "[500]\tvalid_0's binary_logloss: 0.624361\n",
      "[600]\tvalid_0's binary_logloss: 0.621881\n",
      "[700]\tvalid_0's binary_logloss: 0.621013\n",
      "[800]\tvalid_0's binary_logloss: 0.620899\n",
      "Logloss: 0.6208992424519378, Confident objects accuracy: 0.7933333333333333\n",
      "[100]\tvalid_0's binary_logloss: 0.654601\n",
      "[200]\tvalid_0's binary_logloss: 0.644081\n",
      "[300]\tvalid_0's binary_logloss: 0.637742\n",
      "[400]\tvalid_0's binary_logloss: 0.635968\n",
      "[500]\tvalid_0's binary_logloss: 0.634911\n",
      "[600]\tvalid_0's binary_logloss: 0.634532\n",
      "[700]\tvalid_0's binary_logloss: 0.63317\n",
      "[800]\tvalid_0's binary_logloss: 0.632782\n",
      "Logloss: 0.6327821318886288, Confident objects accuracy: 0.7518987341772152\n",
      "[100]\tvalid_0's binary_logloss: 0.65499\n",
      "[200]\tvalid_0's binary_logloss: 0.646602\n",
      "[300]\tvalid_0's binary_logloss: 0.640441\n",
      "[400]\tvalid_0's binary_logloss: 0.635496\n",
      "[500]\tvalid_0's binary_logloss: 0.631588\n",
      "[600]\tvalid_0's binary_logloss: 0.628715\n",
      "[700]\tvalid_0's binary_logloss: 0.626617\n",
      "[800]\tvalid_0's binary_logloss: 0.624961\n",
      "Logloss: 0.6249614057215055, Confident objects accuracy: 0.750465549348231\n",
      "[100]\tvalid_0's binary_logloss: 0.652842\n",
      "[200]\tvalid_0's binary_logloss: 0.64605\n",
      "[300]\tvalid_0's binary_logloss: 0.638699\n",
      "[400]\tvalid_0's binary_logloss: 0.631823\n",
      "[500]\tvalid_0's binary_logloss: 0.629154\n",
      "[600]\tvalid_0's binary_logloss: 0.628308\n",
      "[700]\tvalid_0's binary_logloss: 0.626814\n",
      "[800]\tvalid_0's binary_logloss: 0.627901\n",
      "Logloss: 0.627900731694222, Confident objects accuracy: 0.7514450867052023\n",
      "[100]\tvalid_0's binary_logloss: 0.646047\n",
      "[200]\tvalid_0's binary_logloss: 0.633628\n",
      "[300]\tvalid_0's binary_logloss: 0.623739\n",
      "[400]\tvalid_0's binary_logloss: 0.617338\n",
      "[500]\tvalid_0's binary_logloss: 0.614082\n",
      "[600]\tvalid_0's binary_logloss: 0.613172\n",
      "[700]\tvalid_0's binary_logloss: 0.608333\n",
      "[800]\tvalid_0's binary_logloss: 0.608174\n",
      "Logloss: 0.6081739066391587, Confident objects accuracy: 0.8027210884353742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6226973017816237"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7686973749380882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "\n",
    "def model_train(df, task_type, how, n_folds): \n",
    "    oof = np.zeros([df['target'].shape[0], 1])\n",
    "    # features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    features = res.groupby('Feature')['rank'].sum().sort_values().head(30).index.to_list()\n",
    "\n",
    "    X, groups = df[features], df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(df['pattern'], drop_first=True)], axis=1)\n",
    "    \n",
    "    if task_type == 'cls':\n",
    "        y = df['target'] >= df['close']\n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "    else:\n",
    "        y = (df['target'] - df['close']) / df['close']\n",
    "        kf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    oe_enc = OrdinalEncoder()\n",
    "    groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "    print(f\"Training with {len(features)} features\")\n",
    "    \n",
    "    if how == 'lreg':\n",
    "        scaler = StandardScaler()\n",
    "        X[X.columns] = scaler.fit_transform(X)\n",
    "    \n",
    "    for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        models = list()\n",
    "        if how == 'lgbm':\n",
    "            if task_type == 'cls':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='mse', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "        elif how == 'lreg':\n",
    "            if task_type == 'cls':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "            else:\n",
    "                model = LinearRegression(positive=True)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            acc_score = confident_accuracy_score(y_val, val_preds[:,1])\n",
    "            print(f'Logloss: {val_score}, Confident objects accuracy: {acc_score}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "        else:\n",
    "            val_preds = model.predict(X_val)\n",
    "            val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            print('RMSE: {val_score}')\n",
    "            oof[val_idx, 0] = val_preds\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "    return oof, models\n",
    "\n",
    "def confident_accuracy_score(y, oof, low_bound=0.35, high_bound=0.65):\n",
    "    ''' Consider only high confident objects for accuracy score calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    pred_conf = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    y_conf = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    return accuracy_score(y_conf, pred_conf)   \n",
    "\n",
    "# task_type = 'reg'\n",
    "task_type = 'cls'\n",
    "\n",
    "# best params for classification\n",
    "if task_type == 'cls':\n",
    "    params = {\n",
    "            'boosting_type': 'dart',\n",
    "            'n_estimators': 800,\n",
    "            'learning_rate': 0.02,\n",
    "            #   'early_stopping_round': 50,\n",
    "            'max_depth': 10,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'subsample': 0.9,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 26,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 255,\n",
    "            'reg_alpha': 1e-5,\n",
    "            'reg_lambda': 1e-7,\n",
    "            'objective': 'binary'\n",
    "            }\n",
    "else:\n",
    "    # best params for regression\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.021,\n",
    "            'early_stopping_round': 100,\n",
    "            'max_depth': 7,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.8,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 27,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 511,\n",
    "            'reg_alpha': 1e-4,\n",
    "            'reg_lambda': 1e-4,\n",
    "            'objective': 'regression'\n",
    "            }\n",
    "\n",
    "oof, models = model_train(train_df, task_type=task_type, how='lgbm', n_folds=5) # 0.061096263508601985 / logloss: 0.6226973017816237 acc: 0.6424792139077853\n",
    "# oof, models = model_train(train_df, task_type=task_type, how='lreg', n_folds=5) # 0.06958035063954768 / logloss: 0.6985539186132326 acc: 0.587892049598833\n",
    "\n",
    "if task_type == 'cls':\n",
    "    y = train_df['target'] >= train_df['close']\n",
    "    low_bound, high_bound = 0.35, 0.65\n",
    "    display(log_loss(y, oof))\n",
    "    display(confident_accuracy_score(y, oof, low_bound, high_bound))\n",
    "else:\n",
    "    y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "    display(mean_squared_error(y, oof, squared=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
