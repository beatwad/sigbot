{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from indicators import indicators\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from config.config import ConfigFactory\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score, precision_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    collect_data = False # create new dataset or load previous\n",
    "    select_features = False\n",
    "    train_NN = False\n",
    "    train_LGBM = True\n",
    "    n_repeats = 1\n",
    "    n_folds = 5\n",
    "    cls_target_ratio = 1.021"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_angle_prev_72</th>\n",
       "      <th>macd_prev_72</th>\n",
       "      <th>macdsignal_prev_72</th>\n",
       "      <th>macdhist_prev_72</th>\n",
       "      <th>macd_dir_prev_72</th>\n",
       "      <th>macdsignal_dir_prev_72</th>\n",
       "      <th>atr_prev_72</th>\n",
       "      <th>close_smooth_prev_72</th>\n",
       "      <th>target</th>\n",
       "      <th>ttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-10 21:00:00</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>227818.00</td>\n",
       "      <td>58.380200</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>34.523810</td>\n",
       "      <td>0.205808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.856226</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.318808</td>\n",
       "      <td>0.227337</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-15 15:00:00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>250135.00</td>\n",
       "      <td>58.463239</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>53.174603</td>\n",
       "      <td>0.357436</td>\n",
       "      <td>...</td>\n",
       "      <td>5.250146</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.229362</td>\n",
       "      <td>0.110533</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.999742</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-21 19:00:00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>49801.00</td>\n",
       "      <td>45.480088</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>61.904762</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>5.078633</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-1.898082</td>\n",
       "      <td>-0.192558</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>0.3970</td>\n",
       "      <td>0.3980</td>\n",
       "      <td>0.3930</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>304916.80</td>\n",
       "      <td>36.897427</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>43.650794</td>\n",
       "      <td>-0.069109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303562</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>-0.292488</td>\n",
       "      <td>-0.109834</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.398667</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>6.8900</td>\n",
       "      <td>6.9100</td>\n",
       "      <td>6.8900</td>\n",
       "      <td>6.9000</td>\n",
       "      <td>3986.13</td>\n",
       "      <td>30.533005</td>\n",
       "      <td>14.404762</td>\n",
       "      <td>18.392857</td>\n",
       "      <td>-0.204445</td>\n",
       "      <td>...</td>\n",
       "      <td>11.812509</td>\n",
       "      <td>0.028818</td>\n",
       "      <td>-0.003283</td>\n",
       "      <td>0.032101</td>\n",
       "      <td>-0.946146</td>\n",
       "      <td>-0.453481</td>\n",
       "      <td>0.088674</td>\n",
       "      <td>6.850417</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time    open    high     low   close     volume        rsi  \\\n",
       "0 2022-09-10 21:00:00  0.9999  0.9999  0.9998  0.9998  227818.00  58.380200   \n",
       "1 2022-09-15 15:00:00  1.0000  1.0000  0.9999  1.0000  250135.00  58.463239   \n",
       "2 2022-09-21 19:00:00  1.0000  1.0000  0.9999  0.9999   49801.00  45.480088   \n",
       "3 2022-12-25 15:00:00  0.3970  0.3980  0.3930  0.3960  304916.80  36.897427   \n",
       "4 2022-12-25 15:00:00  6.8900  6.9100  6.8900  6.9000    3986.13  30.533005   \n",
       "\n",
       "   stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  linear_reg_angle_prev_72  \\\n",
       "0    35.714286    34.523810         0.205808  ...                 -0.856226   \n",
       "1    64.285714    53.174603         0.357436  ...                  5.250146   \n",
       "2    57.142857    61.904762        -0.066667  ...                  5.078633   \n",
       "3    42.857143    43.650794        -0.069109  ...                  0.303562   \n",
       "4    14.404762    18.392857        -0.204445  ...                 11.812509   \n",
       "\n",
       "   macd_prev_72  macdsignal_prev_72  macdhist_prev_72  macd_dir_prev_72  \\\n",
       "0     -0.000020           -0.000010         -0.000009          0.318808   \n",
       "1      0.000016            0.000011          0.000006          0.229362   \n",
       "2      0.000006           -0.000004          0.000010         -1.898082   \n",
       "3     -0.000151           -0.000647          0.000496         -0.292488   \n",
       "4      0.028818           -0.003283          0.032101         -0.946146   \n",
       "\n",
       "   macdsignal_dir_prev_72  atr_prev_72  close_smooth_prev_72  target  ttype  \n",
       "0                0.227337     0.000114              1.000004       0    buy  \n",
       "1                0.110533     0.000113              0.999742       0    buy  \n",
       "2               -0.192558     0.000113              0.999988       0    buy  \n",
       "3               -0.109834     0.003108              0.398667       0   sell  \n",
       "4               -0.453481     0.088674              6.850417       0   sell  \n",
       "\n",
       "[5 rows x 385 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11601, 385)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"1h_4h\"\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "def get_file(ticker):\n",
    "    ''' Find files buy ticker names, file names can be in different formats '''\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def add_indicators(df, ttype, configs):\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, '', '', 0)\n",
    "    # add RSI\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, '', '', 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df = trend.get_indicator(df, '', '', 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df = macd.get_indicator(df, '', '', 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, '', '', 0)\n",
    "    # add SMA\n",
    "    # sma = indicators.SMA(ttype, configs)\n",
    "    # df = sma.get_indicator(df, '', '', 0)\n",
    "    return df\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step):\n",
    "    ''' Create train dataset from signal statistics and ticker candle data'''\n",
    "    train_df = pd.DataFrame()\n",
    "    tickers = df['ticker'].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df['ticker'] == ticker]\n",
    "        times = signal_df['time']\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        tmp_df_1h = add_indicators(tmp_df_1h, ttype, configs)\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc('pattern')]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h['time'] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + step, step):\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h['time'] == time_prev, :].reset_index(drop=True)\n",
    "                    row_tmp.columns = [c + f'_prev_{i}' for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row['ticker'] = ticker\n",
    "                row['pattern'] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "\n",
    "            row['target'] = 0\n",
    "            \n",
    "            if row['pattern'].values == 'STOCH_RSI':\n",
    "                if ttype == 'buy':\n",
    "                    row['ttype'] = 'sell'\n",
    "                else:\n",
    "                    row['ttype'] = 'buy'\n",
    "            else:\n",
    "                row['ttype'] = ttype\n",
    "            \n",
    "            # Ff ttype = buy and during the selected period high price was higher than close_price * target_ratio\n",
    "            # and earlier low price wasn't lower than close_price / target_ratio, than target is True, else target is False.\n",
    "            # Similarly for ttype = sell \n",
    "            close_price = tmp_df_1h.loc[tmp_df_1h['time'] == t, 'close'].values\n",
    "            \n",
    "            for i in range(1, target_offset + 1):\n",
    "                time_next = t + timedelta(hours=i)\n",
    "                target_buy = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'high'].reset_index(drop=True)\n",
    "                target_sell = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'low'].reset_index(drop=True)\n",
    "\n",
    "                try:\n",
    "                    target_buy = target_buy > close_price * CFG.cls_target_ratio\n",
    "                    target_sell = target_sell < close_price / CFG.cls_target_ratio\n",
    "                except ValueError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    if (row['ttype'].values == 'buy' and target_sell[0]) or (row['ttype'].values == 'sell' and target_buy[0]):\n",
    "                        break\n",
    "                    elif (row['ttype'].values == 'buy' and target_buy[0]) or (row['ttype'].values == 'sell' and target_sell[0]):\n",
    "                        row['target'] = 1\n",
    "                        break\n",
    "                except KeyError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "            \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "\n",
    "            # add data to the dataset\n",
    "            if train_df.shape[0] == 0:\n",
    "                train_df = row\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, row])\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "# for how long time (in hours) we want to predict\n",
    "target_offset = 24\n",
    "# first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "first = 4\n",
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 72\n",
    "# step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "step = 4\n",
    "\n",
    "if CFG.collect_data is True:\n",
    "    # Buy\n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "    # dataset for model train\n",
    "    train_buy = create_train_df(df, 'buy', configs, target_offset, first, last, step)\n",
    "    train_buy = train_buy.dropna()\n",
    "\n",
    "    # Sell\n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "    # dataset for model train\n",
    "    train_sell = create_train_df(df, 'sell', configs, target_offset, first, last, step)\n",
    "    train_sell = train_sell.dropna()\n",
    "\n",
    "    df = pd.concat([train_buy, train_sell]).sort_values('time').reset_index(drop=True)\n",
    "    df.to_pickle(f'signal_stat/train_df_{last}.pkl')\n",
    "else:\n",
    "    df = pd.read_pickle(f'signal_stat/train_df_{last}.pkl')\n",
    "\n",
    "display(df.head())\n",
    "display(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_angle_prev_72</th>\n",
       "      <th>macd_prev_72</th>\n",
       "      <th>macdsignal_prev_72</th>\n",
       "      <th>macdhist_prev_72</th>\n",
       "      <th>macd_dir_prev_72</th>\n",
       "      <th>macdsignal_dir_prev_72</th>\n",
       "      <th>atr_prev_72</th>\n",
       "      <th>close_smooth_prev_72</th>\n",
       "      <th>target</th>\n",
       "      <th>ttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>0.39700</td>\n",
       "      <td>0.39800</td>\n",
       "      <td>0.3930</td>\n",
       "      <td>0.39600</td>\n",
       "      <td>3.049168e+05</td>\n",
       "      <td>36.897427</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>43.650794</td>\n",
       "      <td>-0.069109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303562</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>-0.292488</td>\n",
       "      <td>-0.109834</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.398667</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>6.89000</td>\n",
       "      <td>6.91000</td>\n",
       "      <td>6.8900</td>\n",
       "      <td>6.90000</td>\n",
       "      <td>3.986130e+03</td>\n",
       "      <td>30.533005</td>\n",
       "      <td>14.404762</td>\n",
       "      <td>18.392857</td>\n",
       "      <td>-0.204445</td>\n",
       "      <td>...</td>\n",
       "      <td>11.812509</td>\n",
       "      <td>0.028818</td>\n",
       "      <td>-0.003283</td>\n",
       "      <td>0.032101</td>\n",
       "      <td>-0.946146</td>\n",
       "      <td>-0.453481</td>\n",
       "      <td>0.088674</td>\n",
       "      <td>6.850417</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>542.00000</td>\n",
       "      <td>543.00000</td>\n",
       "      <td>541.0000</td>\n",
       "      <td>543.00000</td>\n",
       "      <td>2.084170e+01</td>\n",
       "      <td>28.290712</td>\n",
       "      <td>14.496496</td>\n",
       "      <td>13.507009</td>\n",
       "      <td>0.033015</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.474321</td>\n",
       "      <td>-0.676518</td>\n",
       "      <td>-1.010276</td>\n",
       "      <td>0.333758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.056147</td>\n",
       "      <td>3.200445</td>\n",
       "      <td>549.875000</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>0.01755</td>\n",
       "      <td>0.01755</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.01754</td>\n",
       "      <td>4.566525e+06</td>\n",
       "      <td>21.094090</td>\n",
       "      <td>8.823657</td>\n",
       "      <td>9.251647</td>\n",
       "      <td>-0.033617</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053866</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372109</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.017925</td>\n",
       "      <td>1</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-25 15:00:00</td>\n",
       "      <td>0.34710</td>\n",
       "      <td>0.34780</td>\n",
       "      <td>0.3462</td>\n",
       "      <td>0.34780</td>\n",
       "      <td>3.774232e+06</td>\n",
       "      <td>35.117650</td>\n",
       "      <td>17.786212</td>\n",
       "      <td>17.135809</td>\n",
       "      <td>-0.046770</td>\n",
       "      <td>...</td>\n",
       "      <td>4.279320</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.227125</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.344346</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time       open       high       low      close  \\\n",
       "0 2022-12-25 15:00:00    0.39700    0.39800    0.3930    0.39600   \n",
       "1 2022-12-25 15:00:00    6.89000    6.91000    6.8900    6.90000   \n",
       "2 2022-12-25 15:00:00  542.00000  543.00000  541.0000  543.00000   \n",
       "3 2022-12-25 15:00:00    0.01755    0.01755    0.0175    0.01754   \n",
       "4 2022-12-25 15:00:00    0.34710    0.34780    0.3462    0.34780   \n",
       "\n",
       "         volume        rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  3.049168e+05  36.897427    42.857143    43.650794        -0.069109  ...   \n",
       "1  3.986130e+03  30.533005    14.404762    18.392857        -0.204445  ...   \n",
       "2  2.084170e+01  28.290712    14.496496    13.507009         0.033015  ...   \n",
       "3  4.566525e+06  21.094090     8.823657     9.251647        -0.033617  ...   \n",
       "4  3.774232e+06  35.117650    17.786212    17.135809        -0.046770  ...   \n",
       "\n",
       "   linear_reg_angle_prev_72  macd_prev_72  macdsignal_prev_72  \\\n",
       "0                  0.303562     -0.000151           -0.000647   \n",
       "1                 11.812509      0.028818           -0.003283   \n",
       "2                 -1.474321     -0.676518           -1.010276   \n",
       "3                  2.053866      0.000039            0.000020   \n",
       "4                  4.279320      0.000238           -0.000280   \n",
       "\n",
       "   macdhist_prev_72  macd_dir_prev_72  macdsignal_dir_prev_72  atr_prev_72  \\\n",
       "0          0.000496         -0.292488               -0.109834     0.003108   \n",
       "1          0.032101         -0.946146               -0.453481     0.088674   \n",
       "2          0.333758          0.000000               -0.056147     3.200445   \n",
       "3          0.000019          0.000000                0.372109     0.000129   \n",
       "4          0.000518          0.000000               -0.227125     0.002465   \n",
       "\n",
       "   close_smooth_prev_72  target  ttype  \n",
       "0              0.398667       0   sell  \n",
       "1              6.850417       0   sell  \n",
       "2            549.875000       0   sell  \n",
       "3              0.017925       1   sell  \n",
       "4              0.344346       0   sell  \n",
       "\n",
       "[5 rows x 385 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11548, 385)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df[df['ticker'] != 'TUSDUSDT'].reset_index(drop=True)\n",
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check target correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1000\n",
    "\n",
    "# x = train_df.loc[(train_df.target == 1) & (train_df.ttype == 'buy'), ['ticker', 'ttype', 'pattern', 'time', 'close', 'target']]\n",
    "# y = x.iloc[i]\n",
    "# low_price, high_price = y['close'] / CFG.cls_target_ratio, y['close'] * CFG.cls_target_ratio,\n",
    "# print(y['ticker'], y['time'], y['ttype'])\n",
    "\n",
    "# tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{y[\"ticker\"]}_1h.pkl')\n",
    "# # tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{y[\"ticker\"][:-4]}-{y[\"ticker\"][-4:]}_1h.pkl')\n",
    "# # tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{y[\"ticker\"][:-4]}-{y[\"ticker\"][-4:]}-SWAP_4h.pkl')\n",
    "\n",
    "# tmp_df_1h['low_price'] = low_price\n",
    "# tmp_df_1h['high_price'] = high_price\n",
    "# idx = tmp_df_1h[tmp_df_1h['time'] == y['time']].index[0]\n",
    "\n",
    "# tmp_df_1h = tmp_df_1h.iloc[idx:idx+24][['time', 'close', 'high', 'high_price', 'low', 'low_price']]\n",
    "\n",
    "# if y['ttype'] == 'buy':\n",
    "#     tmp_df_1h['signal'] = tmp_df_1h['high'] > tmp_df_1h['high_price']\n",
    "#     tmp_df_1h['anti_signal'] = tmp_df_1h['low'] < tmp_df_1h['low_price']\n",
    "# else:\n",
    "#     tmp_df_1h['signal'] = tmp_df_1h['low'] < tmp_df_1h['low_price']\n",
    "#     tmp_df_1h['anti_signal'] = tmp_df_1h['high'] > tmp_df_1h['high_price']\n",
    "\n",
    "# tmp_df_1h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split by ticker group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.1, n_splits=2, random_state = 7)\n",
    "\n",
    "split = splitter.split(df, groups=df['ticker'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_df = df.iloc[train_inds]\n",
    "test_df = df.iloc[test_inds]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_size=0.2\n",
    "\n",
    "x_data = train_df.drop(['target', 'time', 'ticker', 'pattern', 'ttype'], axis=1)\n",
    "y_data = train_df['target'] >= train_df['close']\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train[x_train.columns] = scaler.fit_transform(x_train)\n",
    "x_valid[x_valid.columns] = scaler.transform(x_valid)\n",
    "\n",
    "x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "x_valid = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_train), type(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigModel, self).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(260, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "        self.layers.add_module('sigmoid', torch.nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train).squeeze()\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train)\n",
    "    train_acc = (train_preds.round() == y_train).float().mean()\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad() # ~ model.zero_grad()\n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid).squeeze()\n",
    "        val_loss = criterion(val_preds, y_valid)\n",
    "        val_acc = (val_preds.round() == y_valid).float().mean()\n",
    "    \n",
    "    # update weights according to gradient value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# Initialize model\n",
    "if CFG.train_NN:\n",
    "    model = SigModel().to(device)\n",
    "\n",
    "    # Number of epochs\n",
    "    epochs = 10000\n",
    "\n",
    "    # Send data to the device\n",
    "    x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "    y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "    train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    # Empty loss lists to track values\n",
    "    epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    learning_rate = 1e-6\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2000, threshold=1e-2)\n",
    "\n",
    "    # Loop through the data\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc, val_loss, val_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "        # Print progress a total of 20 times\n",
    "        if epoch % int(epochs / 20) == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.5f}\\\n",
    "                Validation Loss: {val_loss:.5f}, Val Acc: {val_acc:.5f}\\\n",
    "                    LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]}')\n",
    "\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(train_loss.cpu().detach().numpy())\n",
    "            valid_loss_values.append(val_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TabularDataset at 0x7fa118b59210>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 380, 3], expected input[1, 32, 380] to have 380 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#Y122sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#Y122sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#Y122sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#Y122sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#Y122sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb Cell 19\u001b[0m in \u001b[0;36mFullyConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#Y122sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#Y122sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#Y122sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#Y122sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(x, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# Global average pooling over the sequence length\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 380, 3], expected input[1, 32, 380] to have 380 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your fully convolutional network architecture\n",
    "class FullyConvNet(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super(FullyConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3)\n",
    "        self.fc = nn.Linear(32, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.mean(x, dim=2)  # Global average pooling over the sequence length\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define your custom dataset for loading tabular data\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Prepare your tabular data\n",
    "data = df.drop(['target', 'time', 'ticker', 'pattern', 'ttype'], axis=1) # Your tabular data, shape: (num_samples, num_features)\n",
    "labels = df['target'] >= df['close'] # Your corresponding labels, shape: (num_samples,)\n",
    "input_channels = data.shape[1]\n",
    "output_size = len(set(labels))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_dataset = TabularDataset(train_data.values, train_labels.values)\n",
    "val_dataset = TabularDataset(val_data.values, val_labels.values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Create an instance of your fully convolutional network\n",
    "net = FullyConvNet(input_channels, output_size)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validate the model\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print loss and accuracy for this epoch\n",
    "    print(f\"Epoch {epoch+1}: Training Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {100*correct/total:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NN train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CFG.train_NN:\n",
    "    plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "    plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss Curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern', 'ttype']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target'] >= df['close']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.01,\n",
    "        #   'early_stopping_round': 100,\n",
    "          'max_depth': 7,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree': 0.75,\n",
    "          'num_leaves': 32,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "if CFG.select_features:\n",
    "    perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(df, permut=True, boruta=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.select_features:\n",
    "    perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "    boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "    feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "    res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "    res = res.groupby('Feature')['rank'].sum().sort_values().reset_index()\n",
    "    res.to_csv('feature_importance.csv')\n",
    "else:\n",
    "    res = pd.read_csv('feature_importance.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test split based on ticker groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 150 features\n",
      "Fold #1\n",
      "[100]\tvalid_0's binary_logloss: 0.676567\tvalid_0's average_precision: 0.642874\n",
      "[200]\tvalid_0's binary_logloss: 0.671804\tvalid_0's average_precision: 0.641134\n",
      "[300]\tvalid_0's binary_logloss: 0.667879\tvalid_0's average_precision: 0.645061\n",
      "[400]\tvalid_0's binary_logloss: 0.664867\tvalid_0's average_precision: 0.649525\n",
      "[500]\tvalid_0's binary_logloss: 0.66341\tvalid_0's average_precision: 0.648247\n",
      "[600]\tvalid_0's binary_logloss: 0.662756\tvalid_0's average_precision: 0.64913\n",
      "[700]\tvalid_0's binary_logloss: 0.661929\tvalid_0's average_precision: 0.646101\n",
      "[800]\tvalid_0's binary_logloss: 0.661998\tvalid_0's average_precision: 0.643068\n",
      "[900]\tvalid_0's binary_logloss: 0.661158\tvalid_0's average_precision: 0.645273\n",
      "[1000]\tvalid_0's binary_logloss: 0.660517\tvalid_0's average_precision: 0.644882\n",
      "[1100]\tvalid_0's binary_logloss: 0.659923\tvalid_0's average_precision: 0.643521\n",
      "[1200]\tvalid_0's binary_logloss: 0.658871\tvalid_0's average_precision: 0.646337\n",
      "[1300]\tvalid_0's binary_logloss: 0.658552\tvalid_0's average_precision: 0.647894\n",
      "[1400]\tvalid_0's binary_logloss: 0.658648\tvalid_0's average_precision: 0.64588\n",
      "[1500]\tvalid_0's binary_logloss: 0.658836\tvalid_0's average_precision: 0.645907\n",
      "[1600]\tvalid_0's binary_logloss: 0.658125\tvalid_0's average_precision: 0.646252\n",
      "[1700]\tvalid_0's binary_logloss: 0.657863\tvalid_0's average_precision: 0.646114\n",
      "[1800]\tvalid_0's binary_logloss: 0.657551\tvalid_0's average_precision: 0.647361\n",
      "[1900]\tvalid_0's binary_logloss: 0.657403\tvalid_0's average_precision: 0.648267\n",
      "[2000]\tvalid_0's binary_logloss: 0.657817\tvalid_0's average_precision: 0.646857\n",
      "Logloss: 0.657817430906193, Confident objects precision: 0.6964705882352941, % of confident objects: 0.185589519650655\n",
      "Fold #2\n",
      "[100]\tvalid_0's binary_logloss: 0.676451\tvalid_0's average_precision: 0.635845\n",
      "[200]\tvalid_0's binary_logloss: 0.671441\tvalid_0's average_precision: 0.643611\n",
      "[300]\tvalid_0's binary_logloss: 0.667287\tvalid_0's average_precision: 0.647354\n",
      "[400]\tvalid_0's binary_logloss: 0.664611\tvalid_0's average_precision: 0.650118\n",
      "[500]\tvalid_0's binary_logloss: 0.662859\tvalid_0's average_precision: 0.650483\n",
      "[600]\tvalid_0's binary_logloss: 0.662316\tvalid_0's average_precision: 0.655002\n",
      "[700]\tvalid_0's binary_logloss: 0.661327\tvalid_0's average_precision: 0.654879\n",
      "[800]\tvalid_0's binary_logloss: 0.661733\tvalid_0's average_precision: 0.650375\n",
      "[900]\tvalid_0's binary_logloss: 0.660573\tvalid_0's average_precision: 0.653776\n",
      "[1000]\tvalid_0's binary_logloss: 0.660374\tvalid_0's average_precision: 0.653667\n",
      "[1100]\tvalid_0's binary_logloss: 0.660389\tvalid_0's average_precision: 0.654759\n",
      "[1200]\tvalid_0's binary_logloss: 0.659905\tvalid_0's average_precision: 0.655496\n",
      "[1300]\tvalid_0's binary_logloss: 0.658952\tvalid_0's average_precision: 0.656448\n",
      "[1400]\tvalid_0's binary_logloss: 0.658361\tvalid_0's average_precision: 0.658114\n",
      "[1500]\tvalid_0's binary_logloss: 0.657578\tvalid_0's average_precision: 0.66199\n",
      "[1600]\tvalid_0's binary_logloss: 0.656977\tvalid_0's average_precision: 0.660949\n",
      "[1700]\tvalid_0's binary_logloss: 0.657032\tvalid_0's average_precision: 0.659474\n",
      "[1800]\tvalid_0's binary_logloss: 0.656616\tvalid_0's average_precision: 0.660707\n",
      "[1900]\tvalid_0's binary_logloss: 0.655644\tvalid_0's average_precision: 0.663428\n",
      "[2000]\tvalid_0's binary_logloss: 0.65493\tvalid_0's average_precision: 0.664251\n",
      "Logloss: 0.6549304371233815, Confident objects precision: 0.7251908396946565, % of confident objects: 0.1936914736323312\n",
      "Fold #3\n",
      "[100]\tvalid_0's binary_logloss: 0.679299\tvalid_0's average_precision: 0.609088\n",
      "[200]\tvalid_0's binary_logloss: 0.675073\tvalid_0's average_precision: 0.616419\n",
      "[300]\tvalid_0's binary_logloss: 0.672807\tvalid_0's average_precision: 0.615718\n",
      "[400]\tvalid_0's binary_logloss: 0.6704\tvalid_0's average_precision: 0.621924\n",
      "[500]\tvalid_0's binary_logloss: 0.66933\tvalid_0's average_precision: 0.620907\n",
      "[600]\tvalid_0's binary_logloss: 0.668257\tvalid_0's average_precision: 0.623627\n",
      "[700]\tvalid_0's binary_logloss: 0.668186\tvalid_0's average_precision: 0.623234\n",
      "[800]\tvalid_0's binary_logloss: 0.668106\tvalid_0's average_precision: 0.62218\n",
      "[900]\tvalid_0's binary_logloss: 0.667069\tvalid_0's average_precision: 0.625853\n",
      "[1000]\tvalid_0's binary_logloss: 0.667202\tvalid_0's average_precision: 0.625076\n",
      "[1100]\tvalid_0's binary_logloss: 0.666973\tvalid_0's average_precision: 0.626415\n",
      "[1200]\tvalid_0's binary_logloss: 0.666541\tvalid_0's average_precision: 0.627084\n",
      "[1300]\tvalid_0's binary_logloss: 0.666293\tvalid_0's average_precision: 0.628166\n",
      "[1400]\tvalid_0's binary_logloss: 0.665423\tvalid_0's average_precision: 0.62945\n",
      "[1500]\tvalid_0's binary_logloss: 0.665245\tvalid_0's average_precision: 0.628739\n",
      "[1600]\tvalid_0's binary_logloss: 0.666884\tvalid_0's average_precision: 0.624095\n",
      "[1700]\tvalid_0's binary_logloss: 0.666285\tvalid_0's average_precision: 0.625808\n",
      "[1800]\tvalid_0's binary_logloss: 0.665913\tvalid_0's average_precision: 0.626517\n",
      "[1900]\tvalid_0's binary_logloss: 0.666188\tvalid_0's average_precision: 0.625126\n",
      "[2000]\tvalid_0's binary_logloss: 0.666349\tvalid_0's average_precision: 0.625396\n",
      "Logloss: 0.6663493190387334, Confident objects precision: 0.7055555555555556, % of confident objects: 0.19812878370941112\n",
      "Fold #4\n",
      "[100]\tvalid_0's binary_logloss: 0.677947\tvalid_0's average_precision: 0.628779\n",
      "[200]\tvalid_0's binary_logloss: 0.673513\tvalid_0's average_precision: 0.635828\n",
      "[300]\tvalid_0's binary_logloss: 0.669558\tvalid_0's average_precision: 0.644201\n",
      "[400]\tvalid_0's binary_logloss: 0.667211\tvalid_0's average_precision: 0.647902\n",
      "[500]\tvalid_0's binary_logloss: 0.666579\tvalid_0's average_precision: 0.646195\n",
      "[600]\tvalid_0's binary_logloss: 0.666755\tvalid_0's average_precision: 0.645874\n",
      "[700]\tvalid_0's binary_logloss: 0.665536\tvalid_0's average_precision: 0.645676\n",
      "[800]\tvalid_0's binary_logloss: 0.664792\tvalid_0's average_precision: 0.647712\n",
      "[900]\tvalid_0's binary_logloss: 0.663231\tvalid_0's average_precision: 0.652962\n",
      "[1000]\tvalid_0's binary_logloss: 0.663194\tvalid_0's average_precision: 0.652256\n",
      "[1100]\tvalid_0's binary_logloss: 0.661883\tvalid_0's average_precision: 0.654676\n",
      "[1200]\tvalid_0's binary_logloss: 0.661955\tvalid_0's average_precision: 0.655568\n",
      "[1300]\tvalid_0's binary_logloss: 0.661977\tvalid_0's average_precision: 0.655642\n",
      "[1400]\tvalid_0's binary_logloss: 0.661363\tvalid_0's average_precision: 0.655622\n",
      "[1500]\tvalid_0's binary_logloss: 0.661339\tvalid_0's average_precision: 0.655664\n",
      "[1600]\tvalid_0's binary_logloss: 0.661017\tvalid_0's average_precision: 0.656261\n",
      "[1700]\tvalid_0's binary_logloss: 0.661053\tvalid_0's average_precision: 0.657432\n",
      "[1800]\tvalid_0's binary_logloss: 0.661337\tvalid_0's average_precision: 0.65643\n",
      "[1900]\tvalid_0's binary_logloss: 0.660492\tvalid_0's average_precision: 0.658772\n",
      "[2000]\tvalid_0's binary_logloss: 0.660321\tvalid_0's average_precision: 0.659922\n",
      "Logloss: 0.6603206251916977, Confident objects precision: 0.7057220708446866, % of confident objects: 0.2067605633802817\n",
      "Fold #5\n",
      "[100]\tvalid_0's binary_logloss: 0.677732\tvalid_0's average_precision: 0.642738\n",
      "[200]\tvalid_0's binary_logloss: 0.673675\tvalid_0's average_precision: 0.64352\n",
      "[300]\tvalid_0's binary_logloss: 0.669677\tvalid_0's average_precision: 0.652174\n",
      "[400]\tvalid_0's binary_logloss: 0.667319\tvalid_0's average_precision: 0.655837\n",
      "[500]\tvalid_0's binary_logloss: 0.666314\tvalid_0's average_precision: 0.653885\n",
      "[600]\tvalid_0's binary_logloss: 0.666098\tvalid_0's average_precision: 0.653147\n",
      "[700]\tvalid_0's binary_logloss: 0.665023\tvalid_0's average_precision: 0.655711\n",
      "[800]\tvalid_0's binary_logloss: 0.665439\tvalid_0's average_precision: 0.65188\n",
      "[900]\tvalid_0's binary_logloss: 0.665238\tvalid_0's average_precision: 0.652463\n",
      "[1000]\tvalid_0's binary_logloss: 0.664874\tvalid_0's average_precision: 0.651898\n",
      "[1100]\tvalid_0's binary_logloss: 0.664797\tvalid_0's average_precision: 0.651187\n",
      "[1200]\tvalid_0's binary_logloss: 0.664935\tvalid_0's average_precision: 0.649145\n",
      "[1300]\tvalid_0's binary_logloss: 0.664148\tvalid_0's average_precision: 0.648874\n",
      "[1400]\tvalid_0's binary_logloss: 0.663589\tvalid_0's average_precision: 0.650182\n",
      "[1500]\tvalid_0's binary_logloss: 0.663697\tvalid_0's average_precision: 0.648191\n",
      "[1600]\tvalid_0's binary_logloss: 0.663668\tvalid_0's average_precision: 0.646427\n",
      "[1700]\tvalid_0's binary_logloss: 0.662277\tvalid_0's average_precision: 0.650081\n",
      "[1800]\tvalid_0's binary_logloss: 0.662854\tvalid_0's average_precision: 0.647168\n",
      "[1900]\tvalid_0's binary_logloss: 0.661843\tvalid_0's average_precision: 0.6486\n",
      "[2000]\tvalid_0's binary_logloss: 0.662414\tvalid_0's average_precision: 0.647372\n",
      "Logloss: 0.6624140166840757, Confident objects precision: 0.701123595505618, % of confident objects: 0.18237704918032788\n",
      "Train on full data\n",
      "[100]\ttraining's binary_logloss: 0.657622\ttraining's average_precision: 0.772\n",
      "[200]\ttraining's binary_logloss: 0.641815\ttraining's average_precision: 0.797157\n",
      "[300]\ttraining's binary_logloss: 0.624472\ttraining's average_precision: 0.825954\n",
      "[400]\ttraining's binary_logloss: 0.608265\ttraining's average_precision: 0.851429\n",
      "[500]\ttraining's binary_logloss: 0.59217\ttraining's average_precision: 0.87617\n",
      "[600]\ttraining's binary_logloss: 0.585298\ttraining's average_precision: 0.890577\n",
      "[700]\ttraining's binary_logloss: 0.572411\ttraining's average_precision: 0.908833\n",
      "[800]\ttraining's binary_logloss: 0.563377\ttraining's average_precision: 0.92232\n",
      "[900]\ttraining's binary_logloss: 0.55143\ttraining's average_precision: 0.934585\n",
      "[1000]\ttraining's binary_logloss: 0.542668\ttraining's average_precision: 0.945098\n",
      "[1100]\ttraining's binary_logloss: 0.532493\ttraining's average_precision: 0.953854\n",
      "[1200]\ttraining's binary_logloss: 0.52618\ttraining's average_precision: 0.96001\n",
      "[1300]\ttraining's binary_logloss: 0.517874\ttraining's average_precision: 0.967284\n",
      "[1400]\ttraining's binary_logloss: 0.506102\ttraining's average_precision: 0.972599\n",
      "[1500]\ttraining's binary_logloss: 0.500414\ttraining's average_precision: 0.97586\n",
      "[1600]\ttraining's binary_logloss: 0.490902\ttraining's average_precision: 0.979899\n",
      "[1700]\ttraining's binary_logloss: 0.480828\ttraining's average_precision: 0.983722\n",
      "[1800]\ttraining's binary_logloss: 0.472239\ttraining's average_precision: 0.986425\n",
      "[1900]\ttraining's binary_logloss: 0.463401\ttraining's average_precision: 0.988718\n",
      "[2000]\ttraining's binary_logloss: 0.456028\ttraining's average_precision: 0.990712\n",
      "Total Logloss: 0.660261984041765, Total confident objects precision: 0.7065326633165829, Total % of confident objects: 0.19225195633272146\n"
     ]
    }
   ],
   "source": [
    "def model_train(df, train_df, test_df, features, task_type, how, n_folds, low_bound, high_bound, train_test): \n",
    "    oof = np.zeros([train_df['target'].shape[0], 1])\n",
    "    oof_test = np.zeros([test_df['target'].shape[0], 1])\n",
    "\n",
    "    X, groups = train_df[features], train_df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(train_df['pattern'], drop_first=True)], axis=1)\n",
    "    \n",
    "    if task_type == 'cls':\n",
    "        y = train_df['target']\n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "    else:\n",
    "        y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "        kf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    oe_enc = OrdinalEncoder()\n",
    "    groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "    print(f\"Training with {len(features)} features\")\n",
    "    \n",
    "    if how == 'lreg':\n",
    "        scaler = StandardScaler()\n",
    "        X[X.columns] = scaler.fit_transform(X)\n",
    "    \n",
    "    for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "        print(f'Fold #{fold + 1}')\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        models = list()\n",
    "        if how == 'lgbm':\n",
    "            if task_type == 'cls':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                          eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                          eval_metric='mse', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "        elif how == 'lreg':\n",
    "            if task_type == 'cls':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "            else:\n",
    "                model = LinearRegression(positive=True)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            prec_score, prec_obj_pct = confident_score(y_val, val_preds[:,1], low_bound, high_bound)\n",
    "            print(f'Logloss: {val_score}, Confident objects precision: {prec_score}, % of confident objects: {prec_obj_pct}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "        else:\n",
    "            val_preds = model.predict(X_val)\n",
    "            val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            print('RMSE: {val_score}')\n",
    "            oof[val_idx, 0] = val_preds\n",
    "    \n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    if train_test == 'train':\n",
    "        print('Train on full data')\n",
    "        X, y = df[features], df['target']\n",
    "        X = pd.concat([X, pd.get_dummies(df['pattern'], drop_first=True)], axis=1)\n",
    "        model.fit(X, y, eval_set=[(X, y)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "    else:\n",
    "        # fit model on full dataset and predict on test\n",
    "        print(\"Test fold\")\n",
    "        X, y = test_df[features], test_df['target']\n",
    "        X = pd.concat([X, pd.get_dummies(test_df['pattern'], drop_first=True)], axis=1)\n",
    "        model.fit(X, y, eval_set=[(X, y)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        oof_test[:,0] = model.predict_proba(X)[:,1]\n",
    "    \n",
    "    return oof, oof_test, model\n",
    "\n",
    "def confident_score(y, oof, low_bound, high_bound):\n",
    "    ''' Consider only high confident objects for accuracy and precision scores calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    pred_conf_acc = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "    pred_conf_prec = pred_conf[(oof > high_bound)]\n",
    "\n",
    "    y_conf_acc = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "    y_conf_prec = y.values.reshape(-1,1)[(oof > high_bound)]\n",
    "\n",
    "    return precision_score(y_conf_prec, pred_conf_prec), y_conf_prec.shape[0]/y.shape[0]\n",
    "\n",
    "# task_type = 'reg'\n",
    "task_type = 'cls'\n",
    "\n",
    "# best params for classification\n",
    "if task_type == 'cls':\n",
    "    params = {\n",
    "        'boosting_type': 'dart',\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.02,\n",
    "        #   'early_stopping_round': 50,\n",
    "        'max_depth': 10,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'subsample': 0.85,\n",
    "        'subsample_freq': 1,\n",
    "        'num_leaves': 25,\n",
    "        'verbosity': -1,\n",
    "        'max_bin': 255,\n",
    "        'reg_alpha': 1e-6,\n",
    "        'reg_lambda': 1e-8,\n",
    "        'objective': 'binary',\n",
    "        # 'is_unbalance': True,\n",
    "        # 'class_weight': 'balanced',\n",
    "        'metric': 'average_precision'\n",
    "        }\n",
    "else:\n",
    "    # best params for regression\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.021,\n",
    "            'early_stopping_round': 100,\n",
    "            'max_depth': 7,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.8,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 27,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 511,\n",
    "            'reg_alpha': 1e-4,\n",
    "            'reg_lambda': 1e-4,\n",
    "            'objective': 'regression'\n",
    "            }\n",
    "\n",
    "if CFG.train_LGBM:\n",
    "    train_test = 'train'\n",
    "    low_bound, high_bound = 0.38, 0.62\n",
    "    features = res['Feature'].head(150)\n",
    "    oof, oof_test, model = model_train(df, train_df, test_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound, train_test=train_test)\n",
    "\n",
    "    if task_type == 'cls':\n",
    "        y = train_df['target']\n",
    "        oof_val_score = log_loss(y, oof)\n",
    "        oof_conf_prec_score, oof_conf_obj_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "        print(f'Total Logloss: {oof_val_score}, Total confident objects precision: {oof_conf_prec_score}, Total % of confident objects: {oof_conf_obj_pct}')\n",
    "        if train_test == 'test':\n",
    "            y_test = test_df['target']\n",
    "            test_val_score = log_loss(y_test, oof_test)\n",
    "            test_conf_prec_score, test_conf_obj_pct = confident_score(y_test, oof_test, low_bound, high_bound)\n",
    "            print(f'Total test Logloss: {test_val_score}, Total test confident objects precision: {test_conf_prec_score}, Total % of test confident objects: {test_conf_obj_pct}')\n",
    "    else:\n",
    "        y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "        display(mean_squared_error(y, oof, squared=False))\n",
    "\n",
    "if train_test == 'train':\n",
    "    model.booster_.save_model('lgbm.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Logloss: 0.6596041772081067, Total confident objects precision: 0.7153925619834711, Total % of confident objects: 0.17737059092991297\n",
    "\n",
    "Total test Logloss: 0.6608237932967793, Total test confident objects precision: 0.7421052631578947, Total % of test confident objects: 0.16843971631205673"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Logloss: 0.6582101740689003, Total confident objects precision: 0.7227138643067846, Total % of confident objects: 0.17532971295577968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_features = list()\n",
    "# low_bound, high_bound = 0.4, 0.6\n",
    "# features = res.groupby('Feature')['rank'].sum().sort_values().head(150).index.to_list()\n",
    "# oof, models = model_train(train_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound)\n",
    "# baseline_prec, baseline_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "# print(f'Total confident objects precision: {baseline_prec}')\n",
    "\n",
    "# for feat in tqdm(features[30:]):\n",
    "#     tmp_features = [f for f in features if f != feat]\n",
    "#     oof, models = model_train(train_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound)\n",
    "#     prec_score, pct = confident_score(y, oof, low_bound, high_bound)\n",
    "#     print(f'Feature: {feat}, Total precision: {prec_score}, Baseline precision: {baseline_prec}, Total pct: {pct}, Baseline precision: {baseline_pct}')\n",
    "#     if prec_score > baseline_prec and pct > baseline_pct:\n",
    "        \n",
    "#         bad_features.append(feat)\n",
    "#         print(bad_features)\n",
    "\n",
    "# bad_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150: Total Logloss: 0.6079201456679827, Total confident objects precision: 0.7093922651933702, Total % of confident objects: 0.07814523788964683"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
