{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9773225a28482c9d825ecaa5ea34b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567ae790f4a74c7985d368b0fb9ff904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_prev_48</th>\n",
       "      <th>linear_reg_angle_prev_48</th>\n",
       "      <th>macd_prev_48</th>\n",
       "      <th>macdsignal_prev_48</th>\n",
       "      <th>macdhist_prev_48</th>\n",
       "      <th>macd_dir_prev_48</th>\n",
       "      <th>macdsignal_dir_prev_48</th>\n",
       "      <th>atr_prev_48</th>\n",
       "      <th>close_smooth_prev_48</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-10 21:00:00</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>227818.0</td>\n",
       "      <td>58.380200</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>34.523810</td>\n",
       "      <td>0.205808</td>\n",
       "      <td>...</td>\n",
       "      <td>22.072420</td>\n",
       "      <td>3.511172</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-6.099046e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>1.467330</td>\n",
       "      <td>0.043803</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-15 15:00:00</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>250135.0</td>\n",
       "      <td>58.463239</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>53.174603</td>\n",
       "      <td>0.357436</td>\n",
       "      <td>...</td>\n",
       "      <td>16.768418</td>\n",
       "      <td>-3.329827</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>9.525671e-07</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>14.888210</td>\n",
       "      <td>-0.535438</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>1.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-21 19:00:00</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>49801.0</td>\n",
       "      <td>45.480088</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>61.904762</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>38.183518</td>\n",
       "      <td>2.280714</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-1.015563e-05</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.330195</td>\n",
       "      <td>0.405625</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.000008</td>\n",
       "      <td>1.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-24 11:00:00</td>\n",
       "      <td>0.03940</td>\n",
       "      <td>0.03960</td>\n",
       "      <td>0.03940</td>\n",
       "      <td>0.03950</td>\n",
       "      <td>314037.8</td>\n",
       "      <td>38.183487</td>\n",
       "      <td>18.707483</td>\n",
       "      <td>21.445578</td>\n",
       "      <td>-0.137102</td>\n",
       "      <td>...</td>\n",
       "      <td>33.926121</td>\n",
       "      <td>14.685994</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>1.975215e-04</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.012304</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.039817</td>\n",
       "      <td>0.03890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-24 11:00:00</td>\n",
       "      <td>0.01981</td>\n",
       "      <td>0.01983</td>\n",
       "      <td>0.01978</td>\n",
       "      <td>0.01982</td>\n",
       "      <td>1202335.9</td>\n",
       "      <td>34.935800</td>\n",
       "      <td>13.366846</td>\n",
       "      <td>12.077312</td>\n",
       "      <td>0.081276</td>\n",
       "      <td>...</td>\n",
       "      <td>7.487028</td>\n",
       "      <td>0.255055</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>7.390256e-06</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.306244</td>\n",
       "      <td>-0.033131</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.020078</td>\n",
       "      <td>0.01974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time     open     high      low    close     volume  \\\n",
       "0 2022-09-10 21:00:00  0.99990  0.99990  0.99980  0.99980   227818.0   \n",
       "1 2022-09-15 15:00:00  1.00000  1.00000  0.99990  1.00000   250135.0   \n",
       "2 2022-09-21 19:00:00  1.00000  1.00000  0.99990  0.99990    49801.0   \n",
       "3 2022-12-24 11:00:00  0.03940  0.03960  0.03940  0.03950   314037.8   \n",
       "4 2022-12-24 11:00:00  0.01981  0.01983  0.01978  0.01982  1202335.9   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  58.380200    35.714286    34.523810         0.205808  ...   \n",
       "1  58.463239    64.285714    53.174603         0.357436  ...   \n",
       "2  45.480088    57.142857    61.904762        -0.066667  ...   \n",
       "3  38.183487    18.707483    21.445578        -0.137102  ...   \n",
       "4  34.935800    13.366846    12.077312         0.081276  ...   \n",
       "\n",
       "   linear_reg_prev_48  linear_reg_angle_prev_48  macd_prev_48  \\\n",
       "0           22.072420                  3.511172     -0.000010   \n",
       "1           16.768418                 -3.329827     -0.000010   \n",
       "2           38.183518                  2.280714     -0.000020   \n",
       "3           33.926121                 14.685994      0.000187   \n",
       "4            7.487028                  0.255055      0.000005   \n",
       "\n",
       "   macdsignal_prev_48  macdhist_prev_48  macd_dir_prev_48  \\\n",
       "0       -6.099046e-06         -0.000004          1.467330   \n",
       "1        9.525671e-07         -0.000011         14.888210   \n",
       "2       -1.015563e-05         -0.000009          0.330195   \n",
       "3        1.975215e-04         -0.000010          0.000000   \n",
       "4        7.390256e-06         -0.000002         -0.306244   \n",
       "\n",
       "   macdsignal_dir_prev_48  atr_prev_48  close_smooth_prev_48   target  \n",
       "0                0.043803     0.000108              0.999946  0.99980  \n",
       "1               -0.535438     0.000111              0.999804  1.00010  \n",
       "2                0.405625     0.000115              1.000008  1.00010  \n",
       "3               -0.012304     0.000469              0.039817  0.03890  \n",
       "4               -0.033131     0.000146              0.020078  0.01974  \n",
       "\n",
       "[5 rows x 264 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11584, 264)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from indicators import indicators\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from config.config import ConfigFactory\n",
    "\n",
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"1h_4h\"\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "def get_file(ticker):\n",
    "    ''' Find files buy ticker names, file names can be in different formats '''\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def add_indicators(df, ttype, configs):\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, '', '', 0)\n",
    "    # add RSI\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, '', '', 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df = trend.get_indicator(df, '', '', 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df = macd.get_indicator(df, '', '', 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, '', '', 0)\n",
    "    # add SMA\n",
    "    # sma = indicators.SMA(ttype, configs)\n",
    "    # df = sma.get_indicator(df, '', '', 0)\n",
    "    return df\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step):\n",
    "    ''' Create train dataset from signal statistics and ticker candle data'''\n",
    "    train_df = pd.DataFrame()\n",
    "    tickers = df['ticker'].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df['ticker'] == ticker]\n",
    "        times = signal_df['time']\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        tmp_df_1h = add_indicators(tmp_df_1h, ttype, configs)\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc('pattern')]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h['time'] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + step, step):\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h['time'] == time_prev, :].reset_index(drop=True)\n",
    "                    row_tmp.columns = [c + f'_prev_{i}' for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row['ticker'] = ticker\n",
    "                row['pattern'] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "            \n",
    "            # add target\n",
    "            time_next = t + timedelta(hours=target_offset)\n",
    "            if ttype == 'buy':\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'high'].reset_index(drop=True)\n",
    "            else:\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'low'].reset_index(drop=True)\n",
    "\n",
    "            target.name = 'target'\n",
    "            rows = pd.concat([row, target], axis=1)\n",
    "            \n",
    "            # add data to the dataset\n",
    "            if train_df.shape[0] == 0:\n",
    "                train_df = rows\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, rows])\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "# for how long time (in hours) we want to predict\n",
    "target_offset = 24\n",
    "# first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "first = 4\n",
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 48\n",
    "# step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "step = 4\n",
    "\n",
    "# Buy\n",
    "# dataset with the signal statistics\n",
    "df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "# dataset for model train\n",
    "train_buy = create_train_df(df, 'buy', configs, target_offset, first, last, step)\n",
    "train_buy = train_buy.dropna()\n",
    "\n",
    "# Sell\n",
    "# dataset with the signal statistics\n",
    "df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "# dataset for model train\n",
    "train_sell = create_train_df(df, 'sell', configs, target_offset, first, last, step)\n",
    "train_sell = train_sell.dropna()\n",
    "\n",
    "train_df = pd.concat([train_buy, train_sell]).sort_values('time').reset_index(drop=True)\n",
    "display(train_df.head())\n",
    "display(train_df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "test_size=0.2\n",
    "\n",
    "x_data = torch.tensor(train_df.drop(['target', 'time', 'ticker', 'pattern'], axis=1).values, dtype=torch.float32)\n",
    "y_data = torch.tensor(train_df['target'].values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_data), type(y_data))\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SigModel(\n",
       "  (layers): Sequential(\n",
       "    (lin1): Linear(in_features=263, out_features=64, bias=True)\n",
       "    (relu1): ReLU()\n",
       "    (do1): Dropout(p=0.25, inplace=False)\n",
       "    (lin2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (relu2): ReLU()\n",
       "    (do2): Dropout(p=0.25, inplace=False)\n",
       "    (lin3): Linear(in_features=128, out_features=96, bias=True)\n",
       "    (relu3): ReLU()\n",
       "    (do3): Dropout(p=0.25, inplace=False)\n",
       "    (lin4): Linear(in_features=96, out_features=32, bias=True)\n",
       "    (relu4): ReLU()\n",
       "    (do4): Dropout(p=0.25, inplace=False)\n",
       "    (lin5): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigModel, self).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(263, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "    \n",
    "model = SigModel().to(device)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad(set_to_none=True) # ~ model.zero_grad()\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train)\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train).item()\n",
    "    \n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights according to gradient value\n",
    "    optimizer.step()\n",
    "\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid)\n",
    "        val_loss = criterion(val_preds, y_valid).item()\n",
    "    \n",
    "    return train_loss, val_loss\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Send data to the device\n",
    "x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "# Empty loss lists to track values\n",
    "epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss\n",
    "learning_rate = 0.003\n",
    "optmizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loop through the data\n",
    "for epoch in range(epochs):\n",
    "    train_loss, val_loss = train_epoch(model, train_loader, criterion, optmizer)\n",
    "\n",
    "    # Print progress a total of 20 times\n",
    "    if epoch % int(epochs / 20) == 0:\n",
    "        print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f} | Validation Loss: {val_loss:.5f}')\n",
    "\n",
    "        epoch_count.append(epoch)\n",
    "        train_loss_values.append(train_loss.detach().numpy())\n",
    "        valid_loss_values.append(val_loss.detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "plt.title('Training & Validation Loss Curves')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat \u001b[1m\u001b[34m#1\n",
      "[100]\ttraining's binary_logloss: 0.662566\tvalid_1's binary_logloss: 0.681304\n",
      "[200]\ttraining's binary_logloss: 0.648888\tvalid_1's binary_logloss: 0.677378\n",
      "[300]\ttraining's binary_logloss: 0.63333\tvalid_1's binary_logloss: 0.674575\n",
      "[400]\ttraining's binary_logloss: 0.619671\tvalid_1's binary_logloss: 0.67248\n",
      "[500]\ttraining's binary_logloss: 0.605767\tvalid_1's binary_logloss: 0.669893\n",
      "[600]\ttraining's binary_logloss: 0.599702\tvalid_1's binary_logloss: 0.668797\n",
      "[700]\ttraining's binary_logloss: 0.58927\tvalid_1's binary_logloss: 0.667476\n",
      "[800]\ttraining's binary_logloss: 0.581746\tvalid_1's binary_logloss: 0.667003\n",
      "[900]\ttraining's binary_logloss: 0.571801\tvalid_1's binary_logloss: 0.666176\n",
      "[1000]\ttraining's binary_logloss: 0.564779\tvalid_1's binary_logloss: 0.665302\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| loss: \u001b[1m\u001b[34m0.66530\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.661905\tvalid_1's binary_logloss: 0.680816\n",
      "[200]\ttraining's binary_logloss: 0.648619\tvalid_1's binary_logloss: 0.676666\n",
      "[300]\ttraining's binary_logloss: 0.634247\tvalid_1's binary_logloss: 0.673354\n",
      "[400]\ttraining's binary_logloss: 0.620844\tvalid_1's binary_logloss: 0.671138\n",
      "[500]\ttraining's binary_logloss: 0.606689\tvalid_1's binary_logloss: 0.66922\n",
      "[600]\ttraining's binary_logloss: 0.601205\tvalid_1's binary_logloss: 0.668163\n",
      "[700]\ttraining's binary_logloss: 0.590585\tvalid_1's binary_logloss: 0.666954\n",
      "[800]\ttraining's binary_logloss: 0.583365\tvalid_1's binary_logloss: 0.66609\n",
      "[900]\ttraining's binary_logloss: 0.574091\tvalid_1's binary_logloss: 0.664889\n",
      "[1000]\ttraining's binary_logloss: 0.566941\tvalid_1's binary_logloss: 0.664301\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| loss: \u001b[1m\u001b[34m0.66430\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.662679\tvalid_1's binary_logloss: 0.679374\n",
      "[200]\ttraining's binary_logloss: 0.648977\tvalid_1's binary_logloss: 0.675196\n",
      "[300]\ttraining's binary_logloss: 0.634427\tvalid_1's binary_logloss: 0.672242\n",
      "[400]\ttraining's binary_logloss: 0.620792\tvalid_1's binary_logloss: 0.669511\n",
      "[500]\ttraining's binary_logloss: 0.605982\tvalid_1's binary_logloss: 0.666513\n",
      "[600]\ttraining's binary_logloss: 0.599872\tvalid_1's binary_logloss: 0.66543\n",
      "[700]\ttraining's binary_logloss: 0.589139\tvalid_1's binary_logloss: 0.663546\n",
      "[800]\ttraining's binary_logloss: 0.581326\tvalid_1's binary_logloss: 0.662714\n",
      "[900]\ttraining's binary_logloss: 0.571612\tvalid_1's binary_logloss: 0.661737\n",
      "[1000]\ttraining's binary_logloss: 0.564962\tvalid_1's binary_logloss: 0.661269\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| loss: \u001b[1m\u001b[34m0.66127\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.664292\tvalid_1's binary_logloss: 0.677141\n",
      "[200]\ttraining's binary_logloss: 0.651329\tvalid_1's binary_logloss: 0.671719\n",
      "[300]\ttraining's binary_logloss: 0.637184\tvalid_1's binary_logloss: 0.667086\n",
      "[400]\ttraining's binary_logloss: 0.623809\tvalid_1's binary_logloss: 0.662804\n",
      "[500]\ttraining's binary_logloss: 0.611256\tvalid_1's binary_logloss: 0.659847\n",
      "[600]\ttraining's binary_logloss: 0.605586\tvalid_1's binary_logloss: 0.659153\n",
      "[700]\ttraining's binary_logloss: 0.595573\tvalid_1's binary_logloss: 0.657143\n",
      "[800]\ttraining's binary_logloss: 0.588457\tvalid_1's binary_logloss: 0.656229\n",
      "[900]\ttraining's binary_logloss: 0.579475\tvalid_1's binary_logloss: 0.65426\n",
      "[1000]\ttraining's binary_logloss: 0.573027\tvalid_1's binary_logloss: 0.652838\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| loss: \u001b[1m\u001b[34m0.65284\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.662913\tvalid_1's binary_logloss: 0.679847\n",
      "[200]\ttraining's binary_logloss: 0.649177\tvalid_1's binary_logloss: 0.675996\n",
      "[300]\ttraining's binary_logloss: 0.63432\tvalid_1's binary_logloss: 0.672246\n",
      "[400]\ttraining's binary_logloss: 0.620555\tvalid_1's binary_logloss: 0.668721\n",
      "[500]\ttraining's binary_logloss: 0.607321\tvalid_1's binary_logloss: 0.666732\n",
      "[600]\ttraining's binary_logloss: 0.601573\tvalid_1's binary_logloss: 0.665689\n",
      "[700]\ttraining's binary_logloss: 0.591315\tvalid_1's binary_logloss: 0.664661\n",
      "[800]\ttraining's binary_logloss: 0.583991\tvalid_1's binary_logloss: 0.663614\n",
      "[900]\ttraining's binary_logloss: 0.574347\tvalid_1's binary_logloss: 0.662986\n",
      "[1000]\ttraining's binary_logloss: 0.567373\tvalid_1's binary_logloss: 0.662129\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| loss: \u001b[1m\u001b[34m0.66213\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n",
      "\u001b[1m\u001b[31m Outer Holdout avg score: \u001b[0m log_loss: \u001b[1m\u001b[31m0.66131\u001b[0m\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "class CFG:\n",
    "    n_repeats = 1\n",
    "    n_folds = 5\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target'] >= df['close']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.01,\n",
    "          'early_stopping_round': 100,\n",
    "          'max_depth': 7,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree': 0.75,\n",
    "          'num_leaves': 32,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(train_df, permut=True, boruta=True)\n",
    "\n",
    "# perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "# boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "# feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "# res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "# res.groupby('Feature')['rank'].sum().sort_values().head(30).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature\n",
       "rsi                           3.0\n",
       "rsi_prev_8                   11.0\n",
       "linear_reg_angle_prev_8      15.0\n",
       "macd                         15.5\n",
       "macdhist_prev_40             30.0\n",
       "stoch_slowk_dir              33.5\n",
       "linear_reg_angle            103.0\n",
       "linear_reg                  136.0\n",
       "linear_reg_angle_prev_16    144.0\n",
       "rsi_prev_4                  150.0\n",
       "stoch_diff                  155.0\n",
       "stoch_slowd_prev_40         162.0\n",
       "macdsignal_dir_prev_24      163.0\n",
       "macdhist                    164.0\n",
       "macdhist_prev_44            171.0\n",
       "stoch_slowd_dir_prev_48     178.0\n",
       "stoch_slowk_dir_prev_28     183.0\n",
       "macdsignal_dir              184.0\n",
       "stoch_diff_prev_28          184.0\n",
       "macdsignal_dir_prev_44      186.0\n",
       "linear_reg_prev_36          190.0\n",
       "stoch_diff_prev_44          192.0\n",
       "rsi_prev_12                 195.0\n",
       "rsi_prev_28                 199.0\n",
       "stoch_slowd_prev_8          199.0\n",
       "rsi_prev_40                 200.0\n",
       "linear_reg_angle_prev_12    205.0\n",
       "rsi_prev_16                 211.0\n",
       "stoch_slowk_prev_24         212.0\n",
       "linear_reg_angle_prev_28    212.0\n",
       "Name: rank, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "res.groupby('Feature')['rank'].sum().sort_values().head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6551301294298724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8026960784313726"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "\n",
    "def model_train(df, task_type, how, n_folds): \n",
    "    oof = np.zeros([df['target'].shape[0], 1])\n",
    "    # features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    features = res.groupby('Feature')['rank'].sum().sort_values().head(31).index.to_list()\n",
    "\n",
    "    X, groups = df[features], df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(df['pattern'], drop_first=True)], axis=1)\n",
    "    \n",
    "    if task_type == 'cls':\n",
    "        y = df['target'] >= df['close']\n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "    else:\n",
    "        y = (df['target'] - df['close']) / df['close']\n",
    "        kf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    oe_enc = OrdinalEncoder()\n",
    "    groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "    print(f\"Training with {len(features)} features\")\n",
    "    \n",
    "    if how == 'lreg':\n",
    "        scaler = StandardScaler()\n",
    "        X[X.columns] = scaler.fit_transform(X)\n",
    "    \n",
    "    for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        models = list()\n",
    "        if how == 'lgbm':\n",
    "            if task_type == 'cls':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='mse', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "        elif how == 'lreg':\n",
    "            if task_type == 'cls':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "            else:\n",
    "                model = LinearRegression(positive=True)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            acc_score = confident_accuracy_score(y_val, val_preds[:,1])\n",
    "            print(f'Logloss: {val_score}, Confident objects accuracy: {acc_score}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "        else:\n",
    "            val_preds = model.predict(X_val)\n",
    "            val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            print('RMSE: {val_score}')\n",
    "            oof[val_idx, 0] = val_preds\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "    return oof, models\n",
    "\n",
    "def confident_accuracy_score(y, oof, low_bound=0.35, high_bound=0.65):\n",
    "    ''' Consider only high confident objects for accuracy score calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    pred_conf = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    y_conf = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    return accuracy_score(y_conf, pred_conf)   \n",
    "\n",
    "# task_type = 'reg'\n",
    "task_type = 'cls'\n",
    "\n",
    "# best params for classification\n",
    "if task_type == 'cls':\n",
    "    params = {\n",
    "            'boosting_type': 'dart',\n",
    "            'n_estimators': 900,\n",
    "            'learning_rate': 0.02,\n",
    "            #   'early_stopping_round': 50,\n",
    "            'max_depth': 10,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.9,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 26,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 255,\n",
    "            'reg_alpha': 1e-5,\n",
    "            'reg_lambda': 1e-7,\n",
    "            'objective': 'binary'\n",
    "            }\n",
    "else:\n",
    "    # best params for regression\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.021,\n",
    "            'early_stopping_round': 100,\n",
    "            'max_depth': 7,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.8,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 27,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 511,\n",
    "            'reg_alpha': 1e-4,\n",
    "            'reg_lambda': 1e-4,\n",
    "            'objective': 'regression'\n",
    "            }\n",
    "\n",
    "# oof, models = model_train(train_df, task_type=task_type, how='lgbm', n_folds=5) # 0.061096263508601985 / logloss: 0.6226973017816237 acc: 0.6424792139077853\n",
    "# oof, models = model_train(train_df, task_type=task_type, how='lreg', n_folds=5) # 0.06958035063954768 / logloss: 0.6985539186132326 acc: 0.587892049598833\n",
    "\n",
    "if task_type == 'cls':\n",
    "    y = train_df['target'] >= train_df['close']\n",
    "    low_bound, high_bound = 0.3, 0.7\n",
    "    display(log_loss(y, oof))\n",
    "    display(confident_accuracy_score(y, oof, low_bound, high_bound))\n",
    "else:\n",
    "    y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "    display(mean_squared_error(y, oof, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6551301294298724\n",
    "\n",
    "# 0.8026960784313726"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
