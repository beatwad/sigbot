{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NumbaDeprecationWarning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m warnings\u001b[39m.\u001b[39msimplefilter(action\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m, category\u001b[39m=\u001b[39m[\u001b[39mFutureWarning\u001b[39;00m, NumbaDeprecationWarning])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NumbaDeprecationWarning' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from indicators import indicators\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from config.config import ConfigFactory\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score, precision_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    collect_data = True # create new dataset or load previous\n",
    "    select_features = False\n",
    "    train_NN = False\n",
    "    train_LGBM = True\n",
    "    n_repeats = 1\n",
    "    n_folds = 5\n",
    "    cls_target_ratio = 1.02"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbe0b728b2e4a649707fcef0d940961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd787314ff642d0a5be0458278beed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_prev_48</th>\n",
       "      <th>linear_reg_angle_prev_48</th>\n",
       "      <th>macd_prev_48</th>\n",
       "      <th>macdsignal_prev_48</th>\n",
       "      <th>macdhist_prev_48</th>\n",
       "      <th>macd_dir_prev_48</th>\n",
       "      <th>macdsignal_dir_prev_48</th>\n",
       "      <th>atr_prev_48</th>\n",
       "      <th>close_smooth_prev_48</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-10 21:00:00</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>227818.000</td>\n",
       "      <td>58.380200</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>34.523810</td>\n",
       "      <td>0.205808</td>\n",
       "      <td>...</td>\n",
       "      <td>22.072420</td>\n",
       "      <td>3.511172</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.488404</td>\n",
       "      <td>-0.017765</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-15 15:00:00</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>250135.000</td>\n",
       "      <td>58.463239</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>53.174603</td>\n",
       "      <td>0.357436</td>\n",
       "      <td>...</td>\n",
       "      <td>16.768418</td>\n",
       "      <td>-3.329827</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.302160</td>\n",
       "      <td>-0.266478</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-21 19:00:00</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>49801.000</td>\n",
       "      <td>45.480088</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>61.904762</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>38.183518</td>\n",
       "      <td>2.280714</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.359671</td>\n",
       "      <td>1.111904</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.000008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-24 12:00:00</td>\n",
       "      <td>68.10000</td>\n",
       "      <td>69.10000</td>\n",
       "      <td>68.0000</td>\n",
       "      <td>68.70000</td>\n",
       "      <td>1712.657</td>\n",
       "      <td>25.443832</td>\n",
       "      <td>13.872597</td>\n",
       "      <td>11.125943</td>\n",
       "      <td>0.187309</td>\n",
       "      <td>...</td>\n",
       "      <td>35.878666</td>\n",
       "      <td>10.537428</td>\n",
       "      <td>0.487438</td>\n",
       "      <td>0.885719</td>\n",
       "      <td>-0.398280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.079996</td>\n",
       "      <td>1.140238</td>\n",
       "      <td>73.870833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-24 20:00:00</td>\n",
       "      <td>0.01983</td>\n",
       "      <td>0.01986</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.01983</td>\n",
       "      <td>2034435.500</td>\n",
       "      <td>39.251422</td>\n",
       "      <td>48.677324</td>\n",
       "      <td>51.357005</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>...</td>\n",
       "      <td>9.486056</td>\n",
       "      <td>-10.186354</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.893395</td>\n",
       "      <td>0.167818</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.020048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time      open      high      low     close       volume  \\\n",
       "0 2022-09-10 21:00:00   0.99990   0.99990   0.9998   0.99980   227818.000   \n",
       "1 2022-09-15 15:00:00   1.00000   1.00000   0.9999   1.00000   250135.000   \n",
       "2 2022-09-21 19:00:00   1.00000   1.00000   0.9999   0.99990    49801.000   \n",
       "3 2022-12-24 12:00:00  68.10000  69.10000  68.0000  68.70000     1712.657   \n",
       "4 2022-12-24 20:00:00   0.01983   0.01986   0.0198   0.01983  2034435.500   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  58.380200    35.714286    34.523810         0.205808  ...   \n",
       "1  58.463239    64.285714    53.174603         0.357436  ...   \n",
       "2  45.480088    57.142857    61.904762        -0.066667  ...   \n",
       "3  25.443832    13.872597    11.125943         0.187309  ...   \n",
       "4  39.251422    48.677324    51.357005         0.000399  ...   \n",
       "\n",
       "   linear_reg_prev_48  linear_reg_angle_prev_48  macd_prev_48  \\\n",
       "0           22.072420                  3.511172     -0.000010   \n",
       "1           16.768418                 -3.329827     -0.000006   \n",
       "2           38.183518                  2.280714     -0.000016   \n",
       "3           35.878666                 10.537428      0.487438   \n",
       "4            9.486056                -10.186354     -0.000034   \n",
       "\n",
       "   macdsignal_prev_48  macdhist_prev_48  macd_dir_prev_48  \\\n",
       "0           -0.000008         -0.000002          0.488404   \n",
       "1            0.000005         -0.000011         -0.302160   \n",
       "2           -0.000006         -0.000010          0.359671   \n",
       "3            0.885719         -0.398280          0.000000   \n",
       "4           -0.000007         -0.000027          0.893395   \n",
       "\n",
       "   macdsignal_dir_prev_48  atr_prev_48  close_smooth_prev_48  target  \n",
       "0               -0.017765     0.000108              0.999946       0  \n",
       "1               -0.266478     0.000111              0.999804       0  \n",
       "2                1.111904     0.000115              1.000008       0  \n",
       "3               -0.079996     1.140238             73.870833       0  \n",
       "4                0.167818     0.000150              0.020048       1  \n",
       "\n",
       "[5 rows x 264 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11743, 264)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"1h_4h\"\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "def get_file(ticker):\n",
    "    ''' Find files buy ticker names, file names can be in different formats '''\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def add_indicators(df, ttype, configs):\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, '', '', 0)\n",
    "    # add RSI\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, '', '', 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df = trend.get_indicator(df, '', '', 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df = macd.get_indicator(df, '', '', 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, '', '', 0)\n",
    "    # add SMA\n",
    "    # sma = indicators.SMA(ttype, configs)\n",
    "    # df = sma.get_indicator(df, '', '', 0)\n",
    "    return df\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step):\n",
    "    ''' Create train dataset from signal statistics and ticker candle data'''\n",
    "    train_df = pd.DataFrame()\n",
    "    tickers = df['ticker'].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df['ticker'] == ticker]\n",
    "        times = signal_df['time']\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        tmp_df_1h = add_indicators(tmp_df_1h, ttype, configs)\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc('pattern')]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h['time'] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + step, step):\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h['time'] == time_prev, :].reset_index(drop=True)\n",
    "                    row_tmp.columns = [c + f'_prev_{i}' for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row['ticker'] = ticker\n",
    "                row['pattern'] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "\n",
    "            row['target'] = 0\n",
    "            \n",
    "            # Ff ttype = buy and during the selected period high price was higher than close_price * target_ratio\n",
    "            # and earlier low price wasn't lower than close_price / target_ratio, than target is True, else target is False.\n",
    "            # Similarly for ttype = sell \n",
    "            close_price = tmp_df_1h.loc[tmp_df_1h['time'] == t, 'close'].values\n",
    "            \n",
    "            for i in range(1, target_offset + 1):\n",
    "                time_next = t + timedelta(hours=i)\n",
    "                target_buy = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'high'].reset_index(drop=True)\n",
    "                target_sell = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'low'].reset_index(drop=True)\n",
    "\n",
    "                try:\n",
    "                    target_buy = target_buy > close_price * CFG.cls_target_ratio\n",
    "                    target_sell = target_sell < close_price / CFG.cls_target_ratio\n",
    "                except ValueError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    if (ttype == 'buy' and target_sell[0]) or (ttype == 'sell' and target_buy[0]):\n",
    "                        break\n",
    "                    elif (ttype == 'buy' and target_buy[0]) or (ttype == 'sell' and target_sell[0]):\n",
    "                        row['target'] = 1\n",
    "                        break\n",
    "                except KeyError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "            \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "\n",
    "            # add data to the dataset\n",
    "            if train_df.shape[0] == 0:\n",
    "                train_df = row\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, row])\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "if CFG.collect_data is True:\n",
    "    # for how long time (in hours) we want to predict\n",
    "    target_offset = 24\n",
    "    # first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "    first = 4\n",
    "    # last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "    last = 48\n",
    "    # step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "    step = 4\n",
    "\n",
    "    # Buy\n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "    # dataset for model train\n",
    "    train_buy = create_train_df(df, 'buy', configs, target_offset, first, last, step)\n",
    "    train_buy = train_buy.dropna()\n",
    "\n",
    "    # Sell\n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "    # dataset for model train\n",
    "    train_sell = create_train_df(df, 'sell', configs, target_offset, first, last, step)\n",
    "    train_sell = train_sell.dropna()\n",
    "\n",
    "    train_df = pd.concat([train_buy, train_sell]).sort_values('time').reset_index(drop=True)\n",
    "    train_df.to_pickle('signal_stat/train_df.pkl')\n",
    "else:\n",
    "    train_df = pd.read_pickle('signal_stat/train_df.pkl')\n",
    "\n",
    "display(train_df.head())\n",
    "display(train_df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_size=0.2\n",
    "\n",
    "x_data = train_df.drop(['target', 'time', 'ticker', 'pattern'], axis=1)\n",
    "y_data = train_df['target'] >= train_df['close']\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train[x_train.columns] = scaler.fit_transform(x_train)\n",
    "x_valid[x_valid.columns] = scaler.transform(x_valid)\n",
    "\n",
    "x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "x_valid = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_train), type(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigModel, self).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(260, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "        self.layers.add_module('sigmoid', torch.nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train).squeeze()\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train)\n",
    "    train_acc = (train_preds.round() == y_train).float().mean()\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad() # ~ model.zero_grad()\n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid).squeeze()\n",
    "        val_loss = criterion(val_preds, y_valid)\n",
    "        val_acc = (val_preds.round() == y_valid).float().mean()\n",
    "    \n",
    "    # update weights according to gradient value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# Initialize model\n",
    "if CFG.train_NN:\n",
    "    model = SigModel().to(device)\n",
    "\n",
    "    # Number of epochs\n",
    "    epochs = 10000\n",
    "\n",
    "    # Send data to the device\n",
    "    x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "    y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "    train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    # Empty loss lists to track values\n",
    "    epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    learning_rate = 1e-6\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2000, threshold=1e-2)\n",
    "\n",
    "    # Loop through the data\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc, val_loss, val_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "        # Print progress a total of 20 times\n",
    "        if epoch % int(epochs / 20) == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.5f}\\\n",
    "                Validation Loss: {val_loss:.5f}, Val Acc: {val_acc:.5f}\\\n",
    "                    LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]}')\n",
    "\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(train_loss.cpu().detach().numpy())\n",
    "            valid_loss_values.append(val_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NN train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CFG.train_NN:\n",
    "    plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "    plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss Curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target'] >= df['close']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.01,\n",
    "          'early_stopping_round': 100,\n",
    "          'max_depth': 7,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree': 0.75,\n",
    "          'num_leaves': 32,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "if CFG.select_features:\n",
    "    perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(train_df, permut=True, boruta=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.select_features:\n",
    "    perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "    boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "    feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "    res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "    res = res.groupby('Feature')['rank'].sum().sort_values()\n",
    "    res.to_csv('feature_importance.csv')\n",
    "else:\n",
    "    res = pd.read_csv('feature_importance.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 150 features\n",
      "Fold #1\n",
      "[100]\tvalid_0's binary_logloss: 0.674233\tvalid_0's average_precision: 0.647506\n",
      "[200]\tvalid_0's binary_logloss: 0.668453\tvalid_0's average_precision: 0.657099\n",
      "[300]\tvalid_0's binary_logloss: 0.665431\tvalid_0's average_precision: 0.659038\n",
      "[400]\tvalid_0's binary_logloss: 0.66259\tvalid_0's average_precision: 0.663303\n",
      "[500]\tvalid_0's binary_logloss: 0.661466\tvalid_0's average_precision: 0.66533\n",
      "[600]\tvalid_0's binary_logloss: 0.660869\tvalid_0's average_precision: 0.666021\n",
      "[700]\tvalid_0's binary_logloss: 0.660507\tvalid_0's average_precision: 0.665104\n",
      "[800]\tvalid_0's binary_logloss: 0.659873\tvalid_0's average_precision: 0.66733\n",
      "[900]\tvalid_0's binary_logloss: 0.659836\tvalid_0's average_precision: 0.666761\n",
      "[1000]\tvalid_0's binary_logloss: 0.660419\tvalid_0's average_precision: 0.663985\n",
      "[1100]\tvalid_0's binary_logloss: 0.659377\tvalid_0's average_precision: 0.66599\n",
      "[1200]\tvalid_0's binary_logloss: 0.658722\tvalid_0's average_precision: 0.667432\n",
      "[1300]\tvalid_0's binary_logloss: 0.658457\tvalid_0's average_precision: 0.668202\n",
      "[1400]\tvalid_0's binary_logloss: 0.658049\tvalid_0's average_precision: 0.668104\n",
      "[1500]\tvalid_0's binary_logloss: 0.658324\tvalid_0's average_precision: 0.66771\n",
      "[1600]\tvalid_0's binary_logloss: 0.658717\tvalid_0's average_precision: 0.666886\n",
      "[1700]\tvalid_0's binary_logloss: 0.658266\tvalid_0's average_precision: 0.66696\n",
      "[1800]\tvalid_0's binary_logloss: 0.657721\tvalid_0's average_precision: 0.667668\n",
      "Logloss: 0.6577209069115472, Confident objects precision: 0.7509225092250923, % of confident objects: 0.19124911785462245\n",
      "Fold #2\n",
      "[100]\tvalid_0's binary_logloss: 0.672993\tvalid_0's average_precision: 0.656506\n",
      "[200]\tvalid_0's binary_logloss: 0.668057\tvalid_0's average_precision: 0.66076\n",
      "[300]\tvalid_0's binary_logloss: 0.664943\tvalid_0's average_precision: 0.66104\n",
      "[400]\tvalid_0's binary_logloss: 0.663292\tvalid_0's average_precision: 0.658449\n",
      "[500]\tvalid_0's binary_logloss: 0.661557\tvalid_0's average_precision: 0.659154\n",
      "[600]\tvalid_0's binary_logloss: 0.66116\tvalid_0's average_precision: 0.659125\n",
      "[700]\tvalid_0's binary_logloss: 0.659561\tvalid_0's average_precision: 0.661195\n",
      "[800]\tvalid_0's binary_logloss: 0.658945\tvalid_0's average_precision: 0.662304\n",
      "[900]\tvalid_0's binary_logloss: 0.657967\tvalid_0's average_precision: 0.664499\n",
      "[1000]\tvalid_0's binary_logloss: 0.657028\tvalid_0's average_precision: 0.665625\n",
      "[1100]\tvalid_0's binary_logloss: 0.656997\tvalid_0's average_precision: 0.664976\n",
      "[1200]\tvalid_0's binary_logloss: 0.657392\tvalid_0's average_precision: 0.663939\n",
      "[1300]\tvalid_0's binary_logloss: 0.656421\tvalid_0's average_precision: 0.665631\n",
      "[1400]\tvalid_0's binary_logloss: 0.655727\tvalid_0's average_precision: 0.666436\n",
      "[1500]\tvalid_0's binary_logloss: 0.655747\tvalid_0's average_precision: 0.666214\n",
      "[1600]\tvalid_0's binary_logloss: 0.65507\tvalid_0's average_precision: 0.668237\n",
      "[1700]\tvalid_0's binary_logloss: 0.655585\tvalid_0's average_precision: 0.66522\n",
      "[1800]\tvalid_0's binary_logloss: 0.655756\tvalid_0's average_precision: 0.66364\n",
      "Logloss: 0.655755926374452, Confident objects precision: 0.7139303482587065, % of confident objects: 0.17654808959156784\n",
      "Fold #3\n",
      "[100]\tvalid_0's binary_logloss: 0.673612\tvalid_0's average_precision: 0.639211\n",
      "[200]\tvalid_0's binary_logloss: 0.669085\tvalid_0's average_precision: 0.645731\n",
      "[300]\tvalid_0's binary_logloss: 0.665733\tvalid_0's average_precision: 0.648131\n",
      "[400]\tvalid_0's binary_logloss: 0.663463\tvalid_0's average_precision: 0.650466\n",
      "[500]\tvalid_0's binary_logloss: 0.660788\tvalid_0's average_precision: 0.654375\n",
      "[600]\tvalid_0's binary_logloss: 0.660845\tvalid_0's average_precision: 0.652247\n",
      "[700]\tvalid_0's binary_logloss: 0.660577\tvalid_0's average_precision: 0.652332\n",
      "[800]\tvalid_0's binary_logloss: 0.65958\tvalid_0's average_precision: 0.656655\n",
      "[900]\tvalid_0's binary_logloss: 0.658961\tvalid_0's average_precision: 0.656511\n",
      "[1000]\tvalid_0's binary_logloss: 0.658615\tvalid_0's average_precision: 0.656765\n",
      "[1100]\tvalid_0's binary_logloss: 0.658673\tvalid_0's average_precision: 0.656258\n",
      "[1200]\tvalid_0's binary_logloss: 0.658562\tvalid_0's average_precision: 0.656685\n",
      "[1300]\tvalid_0's binary_logloss: 0.658088\tvalid_0's average_precision: 0.657836\n",
      "[1400]\tvalid_0's binary_logloss: 0.658614\tvalid_0's average_precision: 0.656942\n",
      "[1500]\tvalid_0's binary_logloss: 0.658748\tvalid_0's average_precision: 0.655909\n",
      "[1600]\tvalid_0's binary_logloss: 0.658458\tvalid_0's average_precision: 0.655453\n",
      "[1700]\tvalid_0's binary_logloss: 0.657659\tvalid_0's average_precision: 0.657233\n",
      "[1800]\tvalid_0's binary_logloss: 0.657075\tvalid_0's average_precision: 0.6576\n",
      "Logloss: 0.657074522869998, Confident objects precision: 0.7190569744597249, % of confident objects: 0.2108533554266777\n",
      "Fold #4\n",
      "[100]\tvalid_0's binary_logloss: 0.678515\tvalid_0's average_precision: 0.602783\n",
      "[200]\tvalid_0's binary_logloss: 0.676161\tvalid_0's average_precision: 0.603585\n",
      "[300]\tvalid_0's binary_logloss: 0.672982\tvalid_0's average_precision: 0.609325\n",
      "[400]\tvalid_0's binary_logloss: 0.672024\tvalid_0's average_precision: 0.610379\n",
      "[500]\tvalid_0's binary_logloss: 0.671383\tvalid_0's average_precision: 0.609592\n",
      "[600]\tvalid_0's binary_logloss: 0.671819\tvalid_0's average_precision: 0.608837\n",
      "[700]\tvalid_0's binary_logloss: 0.672246\tvalid_0's average_precision: 0.608776\n",
      "[800]\tvalid_0's binary_logloss: 0.672518\tvalid_0's average_precision: 0.607217\n",
      "[900]\tvalid_0's binary_logloss: 0.672233\tvalid_0's average_precision: 0.60804\n",
      "[1000]\tvalid_0's binary_logloss: 0.672335\tvalid_0's average_precision: 0.608453\n",
      "[1100]\tvalid_0's binary_logloss: 0.672889\tvalid_0's average_precision: 0.606946\n",
      "[1200]\tvalid_0's binary_logloss: 0.673528\tvalid_0's average_precision: 0.605788\n",
      "[1300]\tvalid_0's binary_logloss: 0.674339\tvalid_0's average_precision: 0.604155\n",
      "[1400]\tvalid_0's binary_logloss: 0.674621\tvalid_0's average_precision: 0.605299\n",
      "[1500]\tvalid_0's binary_logloss: 0.673752\tvalid_0's average_precision: 0.607532\n",
      "[1600]\tvalid_0's binary_logloss: 0.674575\tvalid_0's average_precision: 0.605745\n",
      "[1700]\tvalid_0's binary_logloss: 0.674759\tvalid_0's average_precision: 0.606475\n",
      "[1800]\tvalid_0's binary_logloss: 0.674337\tvalid_0's average_precision: 0.608281\n",
      "Logloss: 0.6743372154704095, Confident objects precision: 0.6606217616580311, % of confident objects: 0.18620356970574048\n",
      "Fold #5\n",
      "[100]\tvalid_0's binary_logloss: 0.676423\tvalid_0's average_precision: 0.627416\n",
      "[200]\tvalid_0's binary_logloss: 0.671788\tvalid_0's average_precision: 0.631091\n",
      "[300]\tvalid_0's binary_logloss: 0.668992\tvalid_0's average_precision: 0.637661\n",
      "[400]\tvalid_0's binary_logloss: 0.667739\tvalid_0's average_precision: 0.639568\n",
      "[500]\tvalid_0's binary_logloss: 0.666805\tvalid_0's average_precision: 0.640094\n",
      "[600]\tvalid_0's binary_logloss: 0.66671\tvalid_0's average_precision: 0.639609\n",
      "[700]\tvalid_0's binary_logloss: 0.666975\tvalid_0's average_precision: 0.639461\n",
      "[800]\tvalid_0's binary_logloss: 0.66673\tvalid_0's average_precision: 0.639838\n",
      "[900]\tvalid_0's binary_logloss: 0.665549\tvalid_0's average_precision: 0.640845\n",
      "[1000]\tvalid_0's binary_logloss: 0.664911\tvalid_0's average_precision: 0.641527\n",
      "[1100]\tvalid_0's binary_logloss: 0.664922\tvalid_0's average_precision: 0.641906\n",
      "[1200]\tvalid_0's binary_logloss: 0.665019\tvalid_0's average_precision: 0.64267\n",
      "[1300]\tvalid_0's binary_logloss: 0.664536\tvalid_0's average_precision: 0.644096\n",
      "[1400]\tvalid_0's binary_logloss: 0.665213\tvalid_0's average_precision: 0.642134\n",
      "[1500]\tvalid_0's binary_logloss: 0.664578\tvalid_0's average_precision: 0.643822\n",
      "[1600]\tvalid_0's binary_logloss: 0.665066\tvalid_0's average_precision: 0.643146\n",
      "[1700]\tvalid_0's binary_logloss: 0.664668\tvalid_0's average_precision: 0.645093\n",
      "[1800]\tvalid_0's binary_logloss: 0.664879\tvalid_0's average_precision: 0.645953\n",
      "Logloss: 0.6648787825168232, Confident objects precision: 0.7222222222222222, % of confident objects: 0.18461538461538463\n",
      "Total Logloss: 0.6614477756040937, Total confident objects precision: 0.716331096196868, Total % of confident objects: 0.19032615174997872\n"
     ]
    }
   ],
   "source": [
    "def model_train(df, features, task_type, how, n_folds, low_bound, high_bound): \n",
    "    oof = np.zeros([df['target'].shape[0], 1])\n",
    "\n",
    "    X, groups = df[features], df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(df['pattern'], drop_first=True)], axis=1)\n",
    "    \n",
    "    if task_type == 'cls':\n",
    "        y = df['target']\n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "    else:\n",
    "        y = (df['target'] - df['close']) / df['close']\n",
    "        kf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    oe_enc = OrdinalEncoder()\n",
    "    groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "    print(f\"Training with {len(features)} features\")\n",
    "    \n",
    "    if how == 'lreg':\n",
    "        scaler = StandardScaler()\n",
    "        X[X.columns] = scaler.fit_transform(X)\n",
    "    \n",
    "    for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "        print(f'Fold #{fold + 1}')\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        models = list()\n",
    "        if how == 'lgbm':\n",
    "            if task_type == 'cls':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='mse', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "        elif how == 'lreg':\n",
    "            if task_type == 'cls':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "            else:\n",
    "                model = LinearRegression(positive=True)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            prec_score, prec_obj_pct = confident_score(y_val, val_preds[:,1], low_bound, high_bound)\n",
    "            print(f'Logloss: {val_score}, Confident objects precision: {prec_score}, % of confident objects: {prec_obj_pct}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "        else:\n",
    "            val_preds = model.predict(X_val)\n",
    "            val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            print('RMSE: {val_score}')\n",
    "            oof[val_idx, 0] = val_preds\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "    return oof, models\n",
    "\n",
    "def confident_score(y, oof, low_bound, high_bound):\n",
    "    ''' Consider only high confident objects for accuracy and precision scores calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    pred_conf_acc = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "    pred_conf_prec = pred_conf[(oof > high_bound)]\n",
    "\n",
    "    y_conf_acc = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "    y_conf_prec = y.values.reshape(-1,1)[(oof > high_bound)]\n",
    "\n",
    "    return precision_score(y_conf_prec, pred_conf_prec), y_conf_prec.shape[0]/y.shape[0]\n",
    "\n",
    "# task_type = 'reg'\n",
    "task_type = 'cls'\n",
    "\n",
    "# best params for classification\n",
    "if task_type == 'cls':\n",
    "    params = {\n",
    "            'boosting_type': 'dart',\n",
    "            'n_estimators': 1800,\n",
    "            'learning_rate': 0.02,\n",
    "            #   'early_stopping_round': 50,\n",
    "            'max_depth': 10,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'subsample': 0.9,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 25,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 511,\n",
    "            'reg_alpha': 1e-5,\n",
    "            'reg_lambda': 1e-8,\n",
    "            'objective': 'binary',\n",
    "            # 'is_unbalance': True,\n",
    "            # 'class_weight': 'balanced',\n",
    "            'metric': 'average_precision'\n",
    "            }\n",
    "else:\n",
    "    # best params for regression\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.021,\n",
    "            'early_stopping_round': 100,\n",
    "            'max_depth': 7,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.8,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 27,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 511,\n",
    "            'reg_alpha': 1e-4,\n",
    "            'reg_lambda': 1e-4,\n",
    "            'objective': 'regression'\n",
    "            }\n",
    "\n",
    "if CFG.train_LGBM:\n",
    "    low_bound, high_bound = 0.38, 0.62\n",
    "    features = res.groupby('Feature')['rank'].sum().sort_values().head(150).index.to_list()\n",
    "    oof, models = model_train(train_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound)\n",
    "\n",
    "    if task_type == 'cls':\n",
    "        y = train_df['target']\n",
    "        oof_val_score = log_loss(y, oof)\n",
    "        oof_conf_prec_score, oof_conf_obj_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "        print(f'Total Logloss: {oof_val_score}, Total confident objects precision: {oof_conf_prec_score}, Total % of confident objects: {oof_conf_obj_pct}')\n",
    "    else:\n",
    "        y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "        display(mean_squared_error(y, oof, squared=False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Logloss: 0.6614477756040937, Total confident objects precision: 0.716331096196868, Total % of confident objects: 0.19032615174997872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_features = list()\n",
    "low_bound, high_bound = 0.4, 0.6\n",
    "features = res.groupby('Feature')['rank'].sum().sort_values().head(150).index.to_list()\n",
    "oof, models = model_train(train_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound)\n",
    "baseline_prec, baseline_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "print(f'Total confident objects precision: {baseline_prec}')\n",
    "\n",
    "for feat in tqdm(features[30:]):\n",
    "    tmp_features = [f for f in features if f != feat]\n",
    "    oof, models = model_train(train_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound)\n",
    "    prec_score, pct = confident_score(y, oof, low_bound, high_bound)\n",
    "    print(f'Feature: {feat}, Total precision: {prec_score}, Baseline precision: {baseline_prec}, Total pct: {pct}, Baseline precision: {baseline_pct}')\n",
    "    if prec_score > baseline_prec and pct > baseline_pct:\n",
    "        \n",
    "        bad_features.append(feat)\n",
    "        print(bad_features)\n",
    "\n",
    "bad_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volume_prev_8']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150: Total Logloss: 0.6079201456679827, Total confident objects precision: 0.7093922651933702, Total % of confident objects: 0.07814523788964683"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
