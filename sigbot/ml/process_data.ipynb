{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b4c1558c584c51ada22e3965b6bc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7543e44cdede462380e5903f7dbff431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_prev_48</th>\n",
       "      <th>linear_reg_angle_prev_48</th>\n",
       "      <th>macd_prev_48</th>\n",
       "      <th>macdsignal_prev_48</th>\n",
       "      <th>macdhist_prev_48</th>\n",
       "      <th>macd_dir_prev_48</th>\n",
       "      <th>macdsignal_dir_prev_48</th>\n",
       "      <th>atr_prev_48</th>\n",
       "      <th>close_smooth_prev_48</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-10 21:00:00</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>227818.000</td>\n",
       "      <td>58.380200</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>34.523810</td>\n",
       "      <td>0.205808</td>\n",
       "      <td>...</td>\n",
       "      <td>22.072420</td>\n",
       "      <td>3.511172</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>1.467330</td>\n",
       "      <td>0.043803</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-24 11:00:00</td>\n",
       "      <td>0.6560</td>\n",
       "      <td>0.6570</td>\n",
       "      <td>0.6550</td>\n",
       "      <td>0.6550</td>\n",
       "      <td>797.660</td>\n",
       "      <td>34.767502</td>\n",
       "      <td>21.821036</td>\n",
       "      <td>16.977731</td>\n",
       "      <td>0.254408</td>\n",
       "      <td>...</td>\n",
       "      <td>42.201288</td>\n",
       "      <td>-18.370972</td>\n",
       "      <td>-0.006460</td>\n",
       "      <td>-0.007129</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.020892</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>0.674292</td>\n",
       "      <td>0.6550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-24 12:00:00</td>\n",
       "      <td>68.1000</td>\n",
       "      <td>69.1000</td>\n",
       "      <td>68.0000</td>\n",
       "      <td>68.7000</td>\n",
       "      <td>1712.657</td>\n",
       "      <td>25.443832</td>\n",
       "      <td>13.872597</td>\n",
       "      <td>11.125943</td>\n",
       "      <td>0.187309</td>\n",
       "      <td>...</td>\n",
       "      <td>35.878666</td>\n",
       "      <td>10.537428</td>\n",
       "      <td>0.371309</td>\n",
       "      <td>0.686006</td>\n",
       "      <td>-0.314697</td>\n",
       "      <td>-0.112998</td>\n",
       "      <td>-0.099589</td>\n",
       "      <td>1.140238</td>\n",
       "      <td>73.870833</td>\n",
       "      <td>69.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-24 16:00:00</td>\n",
       "      <td>1.5450</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>1.5420</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>30778.600</td>\n",
       "      <td>38.085151</td>\n",
       "      <td>21.873965</td>\n",
       "      <td>19.685508</td>\n",
       "      <td>0.065530</td>\n",
       "      <td>...</td>\n",
       "      <td>47.570096</td>\n",
       "      <td>-18.038119</td>\n",
       "      <td>-0.009375</td>\n",
       "      <td>-0.010977</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.050115</td>\n",
       "      <td>0.015902</td>\n",
       "      <td>1.598875</td>\n",
       "      <td>1.5060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-24 18:00:00</td>\n",
       "      <td>0.1778</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.1776</td>\n",
       "      <td>123162.000</td>\n",
       "      <td>34.573072</td>\n",
       "      <td>19.576850</td>\n",
       "      <td>16.777965</td>\n",
       "      <td>0.140748</td>\n",
       "      <td>...</td>\n",
       "      <td>36.983668</td>\n",
       "      <td>-26.734391</td>\n",
       "      <td>-0.001127</td>\n",
       "      <td>-0.000703</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>0.296932</td>\n",
       "      <td>0.114185</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.186696</td>\n",
       "      <td>0.1694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time     open     high      low    close      volume  \\\n",
       "0 2022-09-10 21:00:00   0.9999   0.9999   0.9998   0.9998  227818.000   \n",
       "1 2022-12-24 11:00:00   0.6560   0.6570   0.6550   0.6550     797.660   \n",
       "2 2022-12-24 12:00:00  68.1000  69.1000  68.0000  68.7000    1712.657   \n",
       "3 2022-12-24 16:00:00   1.5450   1.5500   1.5420   1.5500   30778.600   \n",
       "4 2022-12-24 18:00:00   0.1778   0.1779   0.1774   0.1776  123162.000   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  58.380200    35.714286    34.523810         0.205808  ...   \n",
       "1  34.767502    21.821036    16.977731         0.254408  ...   \n",
       "2  25.443832    13.872597    11.125943         0.187309  ...   \n",
       "3  38.085151    21.873965    19.685508         0.065530  ...   \n",
       "4  34.573072    19.576850    16.777965         0.140748  ...   \n",
       "\n",
       "   linear_reg_prev_48  linear_reg_angle_prev_48  macd_prev_48  \\\n",
       "0           22.072420                  3.511172     -0.000010   \n",
       "1           42.201288                -18.370972     -0.006460   \n",
       "2           35.878666                 10.537428      0.371309   \n",
       "3           47.570096                -18.038119     -0.009375   \n",
       "4           36.983668                -26.734391     -0.001127   \n",
       "\n",
       "   macdsignal_prev_48  macdhist_prev_48  macd_dir_prev_48  \\\n",
       "0           -0.000006         -0.000004          1.467330   \n",
       "1           -0.007129          0.000669          0.000000   \n",
       "2            0.686006         -0.314697         -0.112998   \n",
       "3           -0.010977          0.001602          0.000000   \n",
       "4           -0.000703         -0.000423          0.296932   \n",
       "\n",
       "   macdsignal_dir_prev_48  atr_prev_48  close_smooth_prev_48   target  \n",
       "0                0.043803     0.000108              0.999946   0.9998  \n",
       "1               -0.020892     0.004426              0.674292   0.6550  \n",
       "2               -0.099589     1.140238             73.870833  69.8000  \n",
       "3               -0.050115     0.015902              1.598875   1.5060  \n",
       "4                0.114185     0.001354              0.186696   0.1694  \n",
       "\n",
       "[5 rows x 264 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(12295, 264)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from indicators import indicators\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from config.config import ConfigFactory\n",
    "\n",
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"1h_4h\"\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "def get_file(ticker):\n",
    "    ''' Find files buy ticker names, file names can be in different formats '''\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker[:-4]}-{ticker[-4:]}-SWAP_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def add_indicators(df, ttype, configs):\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, '', '', 0)\n",
    "    # add RSI\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, '', '', 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df = trend.get_indicator(df, '', '', 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df = macd.get_indicator(df, '', '', 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, '', '', 0)\n",
    "    # add SMA\n",
    "    # sma = indicators.SMA(ttype, configs)\n",
    "    # df = sma.get_indicator(df, '', '', 0)\n",
    "    return df\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step):\n",
    "    ''' Create train dataset from signal statistics and ticker candle data'''\n",
    "    train_df = pd.DataFrame()\n",
    "    tickers = df['ticker'].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df['ticker'] == ticker]\n",
    "        times = signal_df['time']\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        tmp_df_1h = add_indicators(tmp_df_1h, ttype, configs)\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc('pattern')]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h['time'] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + step, step):\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h['time'] == time_prev, :].reset_index(drop=True)\n",
    "                    row_tmp.columns = [c + f'_prev_{i}' for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row['ticker'] = ticker\n",
    "                row['pattern'] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "            \n",
    "            # add target\n",
    "            time_next = t + timedelta(hours=target_offset)\n",
    "            if ttype == 'buy':\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'high'].reset_index(drop=True)\n",
    "            else:\n",
    "                target = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'low'].reset_index(drop=True)\n",
    "\n",
    "            target.name = 'target'\n",
    "            rows = pd.concat([row, target], axis=1)\n",
    "            \n",
    "            # add data to the dataset\n",
    "            if train_df.shape[0] == 0:\n",
    "                train_df = rows\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, rows])\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "# for how long time (in hours) we want to predict\n",
    "target_offset = 24\n",
    "# first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "first = 4\n",
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 48\n",
    "# step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "step = 4\n",
    "\n",
    "# Buy\n",
    "# dataset with the signal statistics\n",
    "df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "# dataset for model train\n",
    "train_buy = create_train_df(df, 'buy', configs, target_offset, first, last, step)\n",
    "train_buy = train_buy.dropna()\n",
    "\n",
    "# Sell\n",
    "# dataset with the signal statistics\n",
    "df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "# dataset for model train\n",
    "train_sell = create_train_df(df, 'sell', configs, target_offset, first, last, step)\n",
    "train_sell = train_sell.dropna()\n",
    "\n",
    "train_df = pd.concat([train_buy, train_sell]).sort_values('time').reset_index(drop=True)\n",
    "display(train_df.head())\n",
    "display(train_df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/home/alex/.local/lib/python3.10/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat \u001b[1m\u001b[34m#1\n",
      "[100]\ttraining's binary_logloss: 0.667987\tvalid_1's binary_logloss: 0.681241\n",
      "[200]\ttraining's binary_logloss: 0.655747\tvalid_1's binary_logloss: 0.67726\n",
      "[300]\ttraining's binary_logloss: 0.641988\tvalid_1's binary_logloss: 0.673099\n",
      "[400]\ttraining's binary_logloss: 0.629333\tvalid_1's binary_logloss: 0.669702\n",
      "[500]\ttraining's binary_logloss: 0.615704\tvalid_1's binary_logloss: 0.666504\n",
      "[600]\ttraining's binary_logloss: 0.610062\tvalid_1's binary_logloss: 0.665565\n",
      "[700]\ttraining's binary_logloss: 0.600003\tvalid_1's binary_logloss: 0.663507\n",
      "[800]\ttraining's binary_logloss: 0.592845\tvalid_1's binary_logloss: 0.66196\n",
      "[900]\ttraining's binary_logloss: 0.583046\tvalid_1's binary_logloss: 0.659715\n",
      "[1000]\ttraining's binary_logloss: 0.576159\tvalid_1's binary_logloss: 0.658742\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| loss: \u001b[1m\u001b[34m0.65874\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "class CFG:\n",
    "    n_repeats = 1\n",
    "    n_folds = 5\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target'] >= df['close']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.01,\n",
    "          'early_stopping_round': 100,\n",
    "          'max_depth': 7,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree': 0.75,\n",
    "          'num_leaves': 32,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(train_df, permut=True, boruta=True)\n",
    "\n",
    "# perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "# boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "# feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "# res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "# res.groupby('Feature')['rank'].sum().sort_values().head(30).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perm_df_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m perm_df_[\u001b[39m'\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m perm_df_[\u001b[39m'\u001b[39m\u001b[39mimportance\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mrank(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m boruta_df_[\u001b[39m'\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m boruta_df_[\u001b[39m'\u001b[39m\u001b[39mimportance\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mrank()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alex/Trading/sigbot/sigbot/ml/process_data.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m feature_importances_[\u001b[39m'\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m feature_importances_[\u001b[39m'\u001b[39m\u001b[39mValue\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mrank(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'perm_df_' is not defined"
     ]
    }
   ],
   "source": [
    "perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "res = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "res.groupby('Feature')['rank'].sum().sort_values().head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 30 features\n",
      "[100]\tvalid_0's binary_logloss: 0.655505\n",
      "[200]\tvalid_0's binary_logloss: 0.645369\n",
      "[300]\tvalid_0's binary_logloss: 0.635218\n",
      "[400]\tvalid_0's binary_logloss: 0.628613\n",
      "[500]\tvalid_0's binary_logloss: 0.624361\n",
      "[600]\tvalid_0's binary_logloss: 0.621881\n",
      "[700]\tvalid_0's binary_logloss: 0.621013\n",
      "[800]\tvalid_0's binary_logloss: 0.620899\n",
      "Logloss: 0.6208992424519378, Confident objects accuracy: 0.7933333333333333\n",
      "[100]\tvalid_0's binary_logloss: 0.654601\n",
      "[200]\tvalid_0's binary_logloss: 0.644081\n",
      "[300]\tvalid_0's binary_logloss: 0.637742\n",
      "[400]\tvalid_0's binary_logloss: 0.635968\n",
      "[500]\tvalid_0's binary_logloss: 0.634911\n",
      "[600]\tvalid_0's binary_logloss: 0.634532\n",
      "[700]\tvalid_0's binary_logloss: 0.63317\n",
      "[800]\tvalid_0's binary_logloss: 0.632782\n",
      "Logloss: 0.6327821318886288, Confident objects accuracy: 0.7518987341772152\n",
      "[100]\tvalid_0's binary_logloss: 0.65499\n",
      "[200]\tvalid_0's binary_logloss: 0.646602\n",
      "[300]\tvalid_0's binary_logloss: 0.640441\n",
      "[400]\tvalid_0's binary_logloss: 0.635496\n",
      "[500]\tvalid_0's binary_logloss: 0.631588\n",
      "[600]\tvalid_0's binary_logloss: 0.628715\n",
      "[700]\tvalid_0's binary_logloss: 0.626617\n",
      "[800]\tvalid_0's binary_logloss: 0.624961\n",
      "Logloss: 0.6249614057215055, Confident objects accuracy: 0.750465549348231\n",
      "[100]\tvalid_0's binary_logloss: 0.652842\n",
      "[200]\tvalid_0's binary_logloss: 0.64605\n",
      "[300]\tvalid_0's binary_logloss: 0.638699\n",
      "[400]\tvalid_0's binary_logloss: 0.631823\n",
      "[500]\tvalid_0's binary_logloss: 0.629154\n",
      "[600]\tvalid_0's binary_logloss: 0.628308\n",
      "[700]\tvalid_0's binary_logloss: 0.626814\n",
      "[800]\tvalid_0's binary_logloss: 0.627901\n",
      "Logloss: 0.627900731694222, Confident objects accuracy: 0.7514450867052023\n",
      "[100]\tvalid_0's binary_logloss: 0.646047\n",
      "[200]\tvalid_0's binary_logloss: 0.633628\n",
      "[300]\tvalid_0's binary_logloss: 0.623739\n",
      "[400]\tvalid_0's binary_logloss: 0.617338\n",
      "[500]\tvalid_0's binary_logloss: 0.614082\n",
      "[600]\tvalid_0's binary_logloss: 0.613172\n",
      "[700]\tvalid_0's binary_logloss: 0.608333\n",
      "[800]\tvalid_0's binary_logloss: 0.608174\n",
      "Logloss: 0.6081739066391587, Confident objects accuracy: 0.8027210884353742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6226973017816237"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7686973749380882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "\n",
    "def model_train(df, task_type, how, n_folds): \n",
    "    oof = np.zeros([df['target'].shape[0], 1])\n",
    "    # features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern']]\n",
    "    features = res.groupby('Feature')['rank'].sum().sort_values().head(30).index.to_list()\n",
    "\n",
    "    X, groups = df[features], df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(df['pattern'], drop_first=True)], axis=1)\n",
    "    \n",
    "    if task_type == 'cls':\n",
    "        y = df['target'] >= df['close']\n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "    else:\n",
    "        y = (df['target'] - df['close']) / df['close']\n",
    "        kf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    oe_enc = OrdinalEncoder()\n",
    "    groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "    print(f\"Training with {len(features)} features\")\n",
    "    \n",
    "    if how == 'lreg':\n",
    "        scaler = StandardScaler()\n",
    "        X[X.columns] = scaler.fit_transform(X)\n",
    "    \n",
    "    for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        models = list()\n",
    "        if how == 'lgbm':\n",
    "            if task_type == 'cls':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='mse', callbacks = [lgb.log_evaluation(100)])\n",
    "                # best_iter = model.best_iteration_\n",
    "        elif how == 'lreg':\n",
    "            if task_type == 'cls':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "            else:\n",
    "                model = LinearRegression(positive=True)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            acc_score = confident_accuracy_score(y_val, val_preds[:,1])\n",
    "            print(f'Logloss: {val_score}, Confident objects accuracy: {acc_score}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "        else:\n",
    "            val_preds = model.predict(X_val)\n",
    "            val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            print('RMSE: {val_score}')\n",
    "            oof[val_idx, 0] = val_preds\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "    return oof, models\n",
    "\n",
    "def confident_accuracy_score(y, oof, low_bound=0.35, high_bound=0.65):\n",
    "    ''' Consider only high confident objects for accuracy score calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    pred_conf = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    y_conf = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "\n",
    "    return accuracy_score(y_conf, pred_conf)   \n",
    "\n",
    "# task_type = 'reg'\n",
    "task_type = 'cls'\n",
    "\n",
    "# best params for classification\n",
    "if task_type == 'cls':\n",
    "    params = {\n",
    "            'boosting_type': 'dart',\n",
    "            'n_estimators': 800,\n",
    "            'learning_rate': 0.02,\n",
    "            #   'early_stopping_round': 50,\n",
    "            'max_depth': 10,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'subsample': 0.9,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 26,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 255,\n",
    "            'reg_alpha': 1e-5,\n",
    "            'reg_lambda': 1e-7,\n",
    "            'objective': 'binary'\n",
    "            }\n",
    "else:\n",
    "    # best params for regression\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.021,\n",
    "            'early_stopping_round': 100,\n",
    "            'max_depth': 7,\n",
    "            'colsample_bytree': 0.75,\n",
    "            'subsample': 0.8,\n",
    "            'subsample_freq': 1,\n",
    "            'num_leaves': 27,\n",
    "            'verbosity': -1,\n",
    "            'max_bin': 511,\n",
    "            'reg_alpha': 1e-4,\n",
    "            'reg_lambda': 1e-4,\n",
    "            'objective': 'regression'\n",
    "            }\n",
    "\n",
    "oof, models = model_train(train_df, task_type=task_type, how='lgbm', n_folds=5) # 0.061096263508601985 / logloss: 0.6226973017816237 acc: 0.6424792139077853\n",
    "# oof, models = model_train(train_df, task_type=task_type, how='lreg', n_folds=5) # 0.06958035063954768 / logloss: 0.6985539186132326 acc: 0.587892049598833\n",
    "\n",
    "if task_type == 'cls':\n",
    "    y = train_df['target'] >= train_df['close']\n",
    "    low_bound, high_bound = 0.35, 0.65\n",
    "    display(log_loss(y, oof))\n",
    "    display(confident_accuracy_score(y, oof, low_bound, high_bound))\n",
    "else:\n",
    "    y = (train_df['target'] - train_df['close']) / train_df['close']\n",
    "    display(mean_squared_error(y, oof, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6226973017816237\n",
    "\n",
    "# 0.7686973749380882"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
