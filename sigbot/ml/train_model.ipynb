{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and set configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from optimizer.optimizer import Optimizer\n",
    "from os import environ\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, confusion_matrix, precision_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"optimize\"\n",
    "\n",
    "from config.config import ConfigFactory\n",
    "from indicators import indicators\n",
    "\n",
    "from api.tvdatafeed.main import TvDatafeed, Interval\n",
    "from constants.constants import tv_username, tv_password\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    load = True\n",
    "    historical = False\n",
    "    create_dataset = True\n",
    "    update_dataset = True\n",
    "    cls_target_ratio_tp = 1.03\n",
    "    cls_target_ratio_sl = 1.03\n",
    "    ttype = 'both'\n",
    "    patterns_to_filter = ['STOCH_RSI_Volume24']\n",
    "    select_features = False\n",
    "    optimize = False\n",
    "    n_repeats = 1\n",
    "    n_folds = 8\n",
    "    last_date = datetime.strptime(\"2024-04-19:10:00:00\", \"%Y-%m-%d:%H:%M:%S\")\n",
    "    agg_periods = [24, 72]\n",
    "    agg_funcs = [np.min, np.max, np.mean, np.median, np.std]\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=(FutureWarning, pd.errors.PerformanceWarning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "### Load STOCH_RSI buy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_timeframe = '1h'\n",
    "higher_timeframe = '4h'\n",
    "opt_limit = 100000\n",
    "\n",
    "ttype = 'buy'\n",
    "pattern = ['STOCH', 'RSI', 'Volume24']\n",
    "indicator_list = pattern\n",
    "indicator_list_higher = ['Trend', 'MACD']\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "configs['Indicator_list'] = indicator_list\n",
    "configs['Higher_TF_indicator_list'] = indicator_list_higher\n",
    "configs['Timeframes']['work_timeframe'] = work_timeframe\n",
    "configs['Timeframes']['higher_timeframe'] = higher_timeframe\n",
    "\n",
    "optim_dict = {\n",
    "                'RSI': {\n",
    "                        'timeperiod': [14], \n",
    "                        'low_bound': [35]\n",
    "                       },\n",
    "                'STOCH': {\n",
    "                          'fastk_period': [9],\n",
    "                          'slowk_period': [7],\n",
    "                          'slowd_period': [3], \n",
    "                          'low_bound': [25]\n",
    "                        }\n",
    "             }\n",
    "\n",
    "if CFG.load:\n",
    "  print(f'Timeframe is {work_timeframe}/{higher_timeframe}, trade type is {ttype}')\n",
    "  opt = Optimizer(pattern, optim_dict, clean=False, **configs)\n",
    "  min_time = datetime.now().replace(microsecond=0, second=0, minute=0) - pd.to_timedelta(365 * 10, unit='D')\n",
    "  stat = opt.optimize(pattern, ttype, opt_limit, load=True, op_type='ml', historical=CFG.historical, min_time=min_time)\n",
    "  display(stat) # 31060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load STOCH_RSI sell data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_timeframe = '1h'\n",
    "higher_timeframe = '4h'\n",
    "opt_limit = 100000\n",
    "\n",
    "ttype = 'sell'\n",
    "pattern = ['STOCH', 'RSI', 'Volume24']\n",
    "indicator_list = pattern\n",
    "indicator_list_higher = ['Trend', 'MACD']\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "configs['Indicator_list'] = indicator_list\n",
    "configs['Higher_TF_indicator_list'] = indicator_list_higher\n",
    "configs['Timeframes']['work_timeframe'] = work_timeframe\n",
    "configs['Timeframes']['higher_timeframe'] = higher_timeframe\n",
    "\n",
    "optim_dict = {\n",
    "                'RSI': {\n",
    "                        'timeperiod': [14], \n",
    "                        'low_bound': [35]\n",
    "                       },\n",
    "                'STOCH': {\n",
    "                          'fastk_period': [9],\n",
    "                          'slowk_period': [7],\n",
    "                          'slowd_period': [3], \n",
    "                          'low_bound': [25]\n",
    "                        }\n",
    "             }\n",
    "\n",
    "if CFG.load:\n",
    "  print(f'Timeframe is {work_timeframe}/{higher_timeframe}, trade type is {ttype}')\n",
    "  opt = Optimizer(pattern, optim_dict, clean=False, **configs)\n",
    "  min_time = datetime.now().replace(microsecond=0, second=0, minute=0) - pd.to_timedelta(365 * 10, unit='D')\n",
    "  stat = opt.optimize(pattern, ttype, opt_limit, load=False, op_type='ml', historical=CFG.historical, min_time=min_time)\n",
    "  display(stat) # 23629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test loaded data\n",
    "\n",
    "### Remove dataframe files with the biggest number of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 \n",
    "\n",
    "tickers_1h = glob('../optimizer/ticker_dataframes/*_1h.pkl')\n",
    "for ticker in tqdm(tickers_1h[3:]):\n",
    "    df = pd.read_pickle(ticker)\n",
    "    nunique = df['time'].diff()[1:].nunique()\n",
    "    if nunique > 1:\n",
    "        count = df[df['time'].diff().astype('timedelta64[h]') != 1].shape[0]\n",
    "        if count > 200:\n",
    "            print(ticker, nunique, count)\n",
    "            os.remove(ticker)\n",
    "            os.remove(ticker[:-6] + '4h.pkl')\n",
    "            \n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot time vs index for some of dataframe files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = 3\n",
    "figsize = (15, 4 * n_rows)\n",
    "\n",
    "tickers = glob('../optimizer/ticker_dataframes/*_1h.pkl')\n",
    "random_tickers = random.choices(tickers, k=n_cols*n_rows)\n",
    "random_tickers[:3] = [t for t in tickers if '/BTCUSDT_1h' in t or '/ETHUSDT_1h' in t or '/LTCUSDT_1h' in t]\n",
    "\n",
    "def plot_times(random_tickers, n_rows, n_cols):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    for idx in range(n_cols*n_rows):\n",
    "        ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        \n",
    "        df = pd.read_pickle(random_tickers[idx])\n",
    "        sns.lineplot(data=df['time'])\n",
    "\n",
    "        ax.set_ylabel('')\n",
    "        plt.yticks(fontsize=12) \n",
    "        ax.set_xlabel('')\n",
    "        plt.xticks(fontsize=12)\n",
    "        # ax.spines['right'].set_visible(False)\n",
    "        ax.set_title(f'{random_tickers[idx].split(\"/\")[-1][:-7]}', loc='right', weight='bold', fontsize=15)\n",
    "        ax.lines[0].set_linewidth(2)\n",
    "\n",
    "    \n",
    "    fig.suptitle(f'Time vs Index\\n', ha='center',  fontweight='bold', fontsize=21)\n",
    "    # fig.legend([1, 0], loc='upper center', bbox_to_anchor=(0.5, 0.96), fontsize=21, ncol=3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_times(sorted(random_tickers),  n_rows=n_rows, n_cols=n_cols) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all dataframe files has data for both timeframes 1h and 4h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "x = glob.glob('../optimizer/ticker_dataframes/*.pkl')\n",
    "y = [i[31:].split('_')[0] for i in x]\n",
    "z = ('').join(x)\n",
    "\n",
    "for i in y:\n",
    "    if f'{i}_1h' not in z:\n",
    "        print(i, '1h')\n",
    "    if f'{i}_4h' not in z:\n",
    "        print(i, '4h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and create train data\n",
    "\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "higher_features = ['time', 'linear_reg', 'linear_reg_angle', 'macd', 'macdhist', 'macdsignal']\n",
    "price_cols = ['open','high', 'low', 'close']\n",
    "real_price_cols = ['real_high', 'real_low', 'real_close']\n",
    "funding_cols = ['funding_rate']\n",
    "rsi_stoch_cols = ['rsi', 'stoch_diff', 'stoch_slowd', 'stoch_slowk']\n",
    "btcd_cols = ['time', 'btcd_open', 'btcd_high', 'btcd_low', 'btcd_close', 'btcd_volume']\n",
    "btcdom_cols = ['time', 'btcdom_open', 'btcdom_high', 'btcdom_low', 'btcdom_close', 'btcdom_volume']\n",
    "\n",
    "def get_file(ticker):\n",
    "    ''' Find files buy ticker names '''\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "        tmp_df_4h = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "    except FileNotFoundError:\n",
    "        return None, None\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "\n",
    "def add_indicators(df, df_higher, ttype, configs):\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, '', '', 0)\n",
    "    # add Stochastic\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, '', '', 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, '', '', 0)\n",
    "    # add CCI\n",
    "    cci = indicators.CCI(ttype, configs)\n",
    "    df = cci.get_indicator(df, '', '', 0)\n",
    "    # add SAR\n",
    "    sar = indicators.SAR(ttype, configs)\n",
    "    df = sar.get_indicator(df, '', '', 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df_higher = macd.get_indicator(df_higher, '', '', 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df_higher = trend.get_indicator(df_higher, '', '', 0)\n",
    "    # merge higher timeframe indicators with working timeframe\n",
    "    df_higher['time'] = df_higher['time'] + pd.to_timedelta(3, unit='h')\n",
    "    df[higher_features] = pd.merge(df[['time']], df_higher[higher_features], how='left', on='time')\n",
    "    df.drop(columns=['close_smooth'], inplace=True)\n",
    "    df.drop(columns=[c for c in df.columns if c.endswith('_dir')], inplace=True)\n",
    "    # merge with BTC.D dataframe\n",
    "    df[btcd_cols] = pd.merge(df[['time']], btcd[btcd_cols], how='left', on='time')\n",
    "    # merge with BTCDOM dataframe\n",
    "    df[btcdom_cols] = pd.merge(df[['time']], btcdom[btcdom_cols], how='left', on='time')\n",
    "    df.ffill(inplace=True)\n",
    "    df[btcdom_cols] = df[btcdom_cols].fillna(df[btcdom_cols].mean().round(1))\n",
    "    # price and MACD columns to pct difference\n",
    "    df[real_price_cols] = df[['high', 'low', 'close']]\n",
    "    cols_to_scale = ['open', 'high', 'low', 'close', 'macd', 'macdhist', 'macdsignal', 'atr']\n",
    "    for c in cols_to_scale:\n",
    "        df[c] = df[c].pct_change() * 100\n",
    "    # # add aggregate values\n",
    "    # for col in ['close', 'btcd_close', 'btcdom_close']:\n",
    "    #     for period in CFG.agg_periods:\n",
    "    #         for agg_func in CFG.agg_funcs:\n",
    "    #             df[f'{col}_{agg_func.__name__}_{period}'] = df[col].rolling(period, min_periods=1).agg({'func': agg_func})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step, train_df_prev=None):\n",
    "    ''' Create train dataset from signal statistics and ticker candle data'''\n",
    "    train_df = pd.DataFrame()\n",
    "    tickers = df['ticker'].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        \n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df['ticker'] == ticker]\n",
    "        times = signal_df['time']\n",
    "        \n",
    "        # load max time for that ticker from the previously created dataset\n",
    "        if train_df_prev is not None:\n",
    "            max_time = train_df_prev.loc[train_df_prev['ticker'] == ticker, 'time'].max()\n",
    "        else:\n",
    "            max_time = None\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        try:\n",
    "            tmp_df_1h = add_indicators(tmp_df_1h, tmp_df_4h, ttype, configs)\n",
    "        except TypeError:\n",
    "            # print(f'TypeError, ticker - {ticker}')\n",
    "            continue\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            if max_time and t <= max_time:\n",
    "                continue\n",
    "            \n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc('pattern')]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h['time'] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + 1, step):\n",
    "                # collect features every 4 hours, save difference between the current feature and the lagged features\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h['time'] == time_prev, [c for c in tmp_df_1h.columns if c not in real_price_cols]].reset_index(drop=True)\n",
    "                    if i % 8 != 0:\n",
    "                        row_tmp = row_tmp.drop(columns=funding_cols)\n",
    "                    if i % 24 != 0:\n",
    "                        row_tmp = row_tmp.drop(columns=btcd_cols)\n",
    "                    row_tmp.columns = [c + f'_prev_{i}' for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row['ticker'] = ticker\n",
    "                row['pattern'] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "\n",
    "            row['target'] = 0\n",
    "            row['ttype'] = ttype\n",
    "            \n",
    "            # If ttype = buy and during the selected period high price was higher than close_price * target_ratio\n",
    "            # and earlier low price wasn't lower than close_price / target_ratio, than target is True, else target is False.\n",
    "            # Similarly for ttype = sell \n",
    "            if pattern.startswith('MACD'):\n",
    "                close_price = tmp_df_1h.loc[tmp_df_1h['time'] == t + timedelta(hours=3), 'real_close']\n",
    "            else:\n",
    "                close_price = tmp_df_1h.loc[tmp_df_1h['time'] == t, 'real_close']\n",
    "\n",
    "            # move to the next ticker if can't find any data corresponding to time t\n",
    "            if close_price.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            close_price = close_price.values[0]\n",
    "            \n",
    "            for i in range(1, target_offset + 1):\n",
    "                if pattern.startswith('MACD'):\n",
    "                    time_next = t + timedelta(hours=3+i)\n",
    "                else:\n",
    "                    time_next = t + timedelta(hours=i)\n",
    "                    \n",
    "                target_buy = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'real_high']\n",
    "                target_sell = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'real_low']\n",
    "                \n",
    "                if target_buy.shape[0] == 0 or target_sell.shape[0] == 0:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                \n",
    "                target_buy = target_buy.values[0]\n",
    "                target_sell = target_sell.values[0]\n",
    "\n",
    "                # set \n",
    "                higher_price = close_price * CFG.cls_target_ratio_tp\n",
    "                lower_price = close_price * (2 - CFG.cls_target_ratio_tp)\n",
    "                target_buy_tp = 1 if target_buy > higher_price else 0\n",
    "                target_buy_sl = 1 if target_buy > higher_price else 0\n",
    "                target_sell_tp = 1 if target_sell < lower_price else 0\n",
    "                target_sell_sl = 1 if target_sell < lower_price else 0\n",
    "\n",
    "                pattern = row['pattern'].values[0]\n",
    "                ttype = row['ttype'].values[0]\n",
    "                \n",
    "                # set SL flag and exit cycle if price reaches stop-loss threshold before it reaches take-profit threshold\n",
    "                # (SL depends on ttype and pattern)\n",
    "                sl1 = pattern.startswith('STOCH') and ttype == 'buy' and target_buy_sl == 1\n",
    "                sl2 = pattern.startswith('STOCH') and ttype == 'sell' and target_sell_sl == 1\n",
    "\n",
    "                # set TP flag and exit cycle if price reaches take-profit threshold and doesn't reach stop-loss threshold before\n",
    "                # (TP depends on ttype and pattern)\n",
    "                tp1 = pattern.startswith('STOCH') and ttype == 'buy' and target_sell_tp == 1\n",
    "                tp2 = pattern.startswith('STOCH') and ttype == 'sell' and target_buy_tp == 1\n",
    "                    \n",
    "                # if both TP and SL flag is on - don't consider that trade\n",
    "                if (tp1 and sl1) or (tp2 and sl2):\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                elif sl1 or sl2:\n",
    "                    break\n",
    "                elif tp1 or tp2:\n",
    "                    row['target'] = 1\n",
    "                    break\n",
    "                \n",
    "                # if price doesn't reaches both TP and SL thresholds but price above / below enter price for buy / sell trade - set TP flag\n",
    "                # (depends on ttype and pattern)\n",
    "                if i == target_offset: \n",
    "                    last_price = tmp_df_1h.loc[tmp_df_1h['time'] == time_next, 'real_close'].values[0]\n",
    "                    l1 = pattern.startswith('STOCH') and ttype == 'buy' and last_price < close_price\n",
    "                    l2 = pattern.startswith('STOCH') and ttype == 'sell' and last_price > close_price\n",
    "\n",
    "                    if l1 or l2:\n",
    "                        row['target'] = 1\n",
    "            \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "            \n",
    "            # add data to the dataset\n",
    "            if train_df.shape[0] == 0:\n",
    "                train_df = row\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, row])\n",
    "    \n",
    "    train_df = train_df.drop(columns=real_price_cols)\n",
    "    return train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 272\n",
    "# for how long time (in hours) we want to predict\n",
    "target_offset = 96\n",
    "\n",
    "if CFG.create_dataset:\n",
    "    # Get BTC dominance\n",
    "    tv = TvDatafeed(username=tv_username, password=tv_password)\n",
    "    \n",
    "    btcd = tv.get_hist('BTC.D','CRYPTOCAP', interval=Interval.in_daily, n_bars=7000, extended_session=True).reset_index()\n",
    "    btcd = btcd.drop(columns='symbol')\n",
    "    btcd.columns = btcd_cols\n",
    "    btcd['time'] = btcd['time'] + pd.to_timedelta(23, unit='h')\n",
    "\n",
    "    btcdom = tv.get_hist('BTCDOMUSDT.P','BINANCE', interval=Interval.in_4_hour, n_bars=7000, extended_session=True).reset_index()\n",
    "    btcdom = btcdom.drop(columns='symbol')\n",
    "    btcdom.columns = btcdom_cols\n",
    "    btcdom['time'] = btcdom['time'] + pd.to_timedelta(3, unit='h')\n",
    "    \n",
    "    # first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "    first = 4\n",
    "    # step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "    step = 4\n",
    "\n",
    "    # Buy\n",
    "    # good hours: \n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "    df = df[df['time'].dt.year > 1970]\n",
    "    df = df[df['pattern'].isin(CFG.patterns_to_filter)]\n",
    "    buy_hours_to_save = [0, 1, 2, 3, 16, 17, 21, 22, 23]\n",
    "    df = df[df['time'].dt.hour.isin(buy_hours_to_save)]\n",
    "    # if previously generated dataset exists - update it, don't create it from scratch\n",
    "    train_df_prev = None\n",
    "    if CFG.update_dataset:\n",
    "        try:\n",
    "            train_df_prev = pd.read_pickle(f'signal_stat/train_buy_{last}.pkl')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    # dataset for model training\n",
    "    train_buy = create_train_df(df, 'buy', configs, target_offset, first, last, step, train_df_prev)\n",
    "    train_buy = train_buy.dropna()\n",
    "    if CFG.update_dataset and train_df_prev is not None:\n",
    "        train_buy = pd.concat([train_df_prev, train_buy]).reset_index(drop=True)\n",
    "    train_buy.to_pickle(f'signal_stat/train_buy_{last}.pkl')\n",
    "\n",
    "    display(train_buy.head())\n",
    "    display(train_buy.shape)\n",
    "\n",
    "    # Sell\n",
    "    # good hours \n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "    df = df[df['time'].dt.year > 1970]\n",
    "    df = df[df['pattern'].isin(CFG.patterns_to_filter)]\n",
    "    sell_hours_to_save = [1, 4, 14, 16, 17, 18, 19, 21, 22, 23]\n",
    "    df = df[df['time'].dt.hour.isin(sell_hours_to_save)]\n",
    "    # if previously generated dataset exists - update it, don't create it from scratch\n",
    "    train_df_prev = None\n",
    "    if CFG.update_dataset:\n",
    "        try:\n",
    "            train_df_prev = pd.read_pickle(f'signal_stat/train_sell_{last}.pkl')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    # dataset for model training\n",
    "    train_sell = create_train_df(df, 'sell', configs, target_offset, first, last, step, train_df_prev)\n",
    "    train_sell = train_sell.dropna()\n",
    "    if CFG.update_dataset and train_df_prev is not None:\n",
    "        train_sell = pd.concat([train_df_prev, train_sell]).reset_index(drop=True)\n",
    "    train_sell.to_pickle(f'signal_stat/train_sell_{last}.pkl')\n",
    "\n",
    "    display(train_sell.head())\n",
    "    display(train_sell.shape)\n",
    "    \n",
    "    # this is made for tests\n",
    "    df = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "    df = df[df['time'].dt.year > 1970]\n",
    "    df = df[df['pattern'].isin(CFG.patterns_to_filter)]\n",
    "\n",
    "    test_df_buy_1 = df[df['time'].dt.hour.isin(buy_hours_to_save[0:1])]\n",
    "    test_df_buy_1 = create_train_df(test_df_buy_1, 'buy', configs, target_offset, first, last, step)\n",
    "    test_df_buy_1 = test_df_buy_1.dropna().sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    df = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "    df = df[df['time'].dt.year > 1970]\n",
    "    df = df[df['pattern'].isin(CFG.patterns_to_filter)]\n",
    "    \n",
    "    test_df_sell_1 = df[df['time'].dt.hour.isin(sell_hours_to_save[0:1])]\n",
    "    test_df_sell_1 = create_train_df(test_df_sell_1, 'sell', configs, target_offset, first, last, step)\n",
    "    test_df_sell_1 = test_df_sell_1.dropna().sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    del df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buy = pd.read_pickle(f'signal_stat/train_buy_{last}.pkl')\n",
    "train_sell = pd.read_pickle(f'signal_stat/train_sell_{last}.pkl')\n",
    "\n",
    "train_df = pd.concat([train_buy, train_sell])\n",
    "train_df = train_df.sort_values('time')\n",
    "\n",
    "# do not consider the last signals - they may contain erroneus signals\n",
    "train_df = train_df[train_df['time'] < train_df['time'].max()].reset_index(drop=True)\n",
    "\n",
    "macd_cols = [c for c in train_df.columns if 'prev' not in c and 'sar' in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data tests\n",
    "\n",
    "### Test train dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "if CFG.create_dataset:\n",
    "    # check if train dataset has only columns that we expect\n",
    "    cols = set(re.sub(r'_prev_\\d+', '', c) for c in train_buy.columns)\n",
    "\n",
    "    agg_funcs = ['amin', 'amax', 'mean', 'median', 'std']\n",
    "    agg_cols = [c for c in cols if len(c.split('_')) > 2 and c.split('_')[-2] in agg_funcs]\n",
    "\n",
    "    expected_cols = set(price_cols + higher_features + funding_cols + rsi_stoch_cols +\n",
    "                        btcd_cols + btcdom_cols + agg_cols + ['atr', 'cci', 'sar', 'volume', 'pattern', 'target', 'ticker', 'ttype'])\n",
    "    assert expected_cols == cols\n",
    "\n",
    "    # check RSI and STOCH columns, their values must be in [0, 100] range \n",
    "    rsi_stoch_cols_ = [c for c in train_df.columns if ('rsi' in c or 'stoch' in c) and 'diff' not in c]\n",
    "    for r_s_c in rsi_stoch_cols_:\n",
    "        assert train_df[r_s_c].min() > -0.0001\n",
    "        assert train_df[r_s_c].max() < 100.0001\n",
    "\n",
    "    # check volume columns, their values must be >= 0\n",
    "    vol_cols_ = [c for c in train_df.columns if 'volume' in c]\n",
    "    for v_c in vol_cols_:\n",
    "        assert train_df[v_c].min() >= 0\n",
    "\n",
    "    # check funding columns, their period must be 8 hours\n",
    "    funding_cols_ = [c for c in train_df.columns if c.startswith('funding')]\n",
    "    for f_c in funding_cols_:\n",
    "        num = ''.join([i for i in f_c if i.isdigit()])\n",
    "        if len(num) > 0:\n",
    "            num = int(num)\n",
    "            assert num % 8 == 0\n",
    "\n",
    "    # check BTC dominance columns, their values must be in [0, 100] range \n",
    "    # and their period must be 24 hours\n",
    "    btcd_cols_ = [c for c in train_df.columns if c.startswith('btcd_') and 'volume' not in c]\n",
    "    for b_c in btcd_cols_:\n",
    "        pass_cycle = False\n",
    "        for a_c in agg_funcs:\n",
    "            if a_c in b_c:\n",
    "                pass_cycle = True\n",
    "                break\n",
    "        if pass_cycle:\n",
    "            continue\n",
    "        # check values\n",
    "        assert train_df[b_c].min() >= 0\n",
    "        assert train_df[b_c].max() <= 100\n",
    "        # check period\n",
    "        num = ''.join([i for i in b_c if i.isdigit()])\n",
    "        if len(num) > 0:\n",
    "            num = int(num)\n",
    "            assert num % 24 == 0\n",
    "\n",
    "    # check the rest columns, their period must be 4 hours\n",
    "    rest_cols_ = [c for c in train_df.columns if c not in funding_cols_ and c not in btcd_cols]\n",
    "    for r_c in rest_cols_:\n",
    "        num = ''.join([i for i in r_c if i.isdigit()])\n",
    "        if len(num) > 0:\n",
    "            num = int(num)\n",
    "            assert num % 4 == 0\n",
    "\n",
    "    # check if changing of source dataframe doesn't affect the resulting train dataframe\n",
    "    test_df_buy_2 = train_buy[train_buy['time'].dt.hour.isin(buy_hours_to_save[0:1])]\n",
    "    assert test_df_buy_1.shape == test_df_buy_2.shape\n",
    "    test_df_sell_2 = train_sell[train_sell['time'].dt.hour.isin(sell_hours_to_save[0:1])]\n",
    "    assert test_df_sell_1.shape == test_df_sell_2.shape\n",
    "\n",
    "    # plot time values \n",
    "    train_df['time'].plot(title='Signal Time distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test buy target corectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_ticker = None\n",
    "\n",
    "train_buy_ = train_buy[train_buy['ticker'] == 'MILOUSDT']\n",
    "\n",
    "if CFG.create_dataset:\n",
    "    for i in tqdm(range(train_buy.shape[0])):\n",
    "        x = train_buy[['ticker', 'ttype', 'pattern', 'time', 'close', 'target']]\n",
    "        y = x.iloc[i]\n",
    "        pattern, ticker, time_, target = y['pattern'], y['ticker'], y['time'], y['target']\n",
    "        if ticker != 'MILOUSDT':\n",
    "            continue\n",
    "\n",
    "        if ticker != prev_ticker:\n",
    "            tmp_df = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "            prev_ticker = ticker\n",
    "\n",
    "        tmp_df_1h = tmp_df.copy()\n",
    "        idx = tmp_df_1h[tmp_df_1h['time'] == time_].index[0]\n",
    "        close_price = tmp_df_1h.loc[tmp_df_1h['time'] == time_, 'close'].values[0]\n",
    "        last_idx = min(idx+target_offset, len(tmp_df_1h)-1)\n",
    "        last_price = tmp_df_1h.iloc[last_idx, tmp_df_1h.columns.get_loc('close')]\n",
    "        low_price = close_price * (2 - CFG.cls_target_ratio_tp)\n",
    "        high_price = close_price * CFG.cls_target_ratio_tp\n",
    "        tmp_df_1h['low_price'] = low_price\n",
    "        tmp_df_1h['high_price'] = high_price\n",
    "\n",
    "        tmp_df_1h = tmp_df_1h.iloc[idx+1:idx+target_offset+1][['time', 'close', 'high', 'high_price', 'low', 'low_price']]\n",
    "        tmp_df_1h['signal'] = tmp_df_1h['low'] < tmp_df_1h['low_price']\n",
    "        tmp_df_1h['anti_signal'] = tmp_df_1h['high'] > tmp_df_1h['high_price']\n",
    "        \n",
    "        first_signal = tmp_df_1h['signal'].argmax()\n",
    "        first_anti_signal = tmp_df_1h['anti_signal'].argmax()\n",
    "        \n",
    "        if tmp_df_1h['signal'].max() == 0 and tmp_df_1h['anti_signal'].max() == 0:\n",
    "            if pattern.startswith('STOCH'): \n",
    "                if last_price < close_price:\n",
    "                    assert target == 1\n",
    "                else:\n",
    "                    assert target == 0\n",
    "        elif tmp_df_1h['signal'].max() == 0:\n",
    "            assert target == 0\n",
    "        elif tmp_df_1h['anti_signal'].max() == 0:\n",
    "            assert target == 1\n",
    "        else:\n",
    "            if target == 1:\n",
    "                assert first_signal < first_anti_signal\n",
    "            else:\n",
    "                try:\n",
    "                    assert first_signal > first_anti_signal\n",
    "                except AssertionError:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sell target corectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.create_dataset:\n",
    "    for i in tqdm(range(train_sell.shape[0])):\n",
    "        x = train_sell[['ticker', 'ttype', 'pattern', 'time', 'close', 'target']]\n",
    "        y = x.iloc[i]\n",
    "        pattern, ticker, time_, target = y['pattern'], y['ticker'], y['time'], y['target']\n",
    "\n",
    "        if ticker != prev_ticker:\n",
    "            tmp_df = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "            prev_ticker = ticker\n",
    "\n",
    "        tmp_df_1h = tmp_df.copy()\n",
    "        idx = tmp_df_1h[tmp_df_1h['time'] == time_].index[0]\n",
    "        close_price = tmp_df_1h.loc[tmp_df_1h['time'] == time_, 'close'].values[0]\n",
    "        last_idx = min(idx+target_offset, len(tmp_df_1h)-1)\n",
    "        last_price = tmp_df_1h.iloc[last_idx, tmp_df_1h.columns.get_loc('close')]\n",
    "        low_price = close_price * (2 - CFG.cls_target_ratio_tp)\n",
    "        high_price = close_price * CFG.cls_target_ratio_tp\n",
    "        tmp_df_1h['low_price'] = low_price\n",
    "        tmp_df_1h['high_price'] = high_price\n",
    "\n",
    "        tmp_df_1h = tmp_df_1h.iloc[idx+1:idx+target_offset+1][['time', 'close', 'high', 'high_price', 'low', 'low_price']]\n",
    "        tmp_df_1h['signal'] = tmp_df_1h['high'] > tmp_df_1h['high_price']\n",
    "        tmp_df_1h['anti_signal'] = tmp_df_1h['low'] < tmp_df_1h['low_price']\n",
    "\n",
    "        first_signal = tmp_df_1h['signal'].argmax()\n",
    "        first_anti_signal = tmp_df_1h['anti_signal'].argmax()\n",
    "        \n",
    "        if tmp_df_1h['signal'].max() == 0 and tmp_df_1h['anti_signal'].max() == 0:\n",
    "            if pattern.startswith('STOCH'):\n",
    "                if last_price > close_price:\n",
    "                    assert target == 1\n",
    "                else:\n",
    "                    assert target == 0\n",
    "        elif tmp_df_1h['signal'].max() == 0:\n",
    "            assert target == 0\n",
    "        elif tmp_df_1h['anti_signal'].max() == 0:\n",
    "            assert target == 1\n",
    "        else:\n",
    "            if target == 1:\n",
    "                assert first_signal < first_anti_signal\n",
    "            else:\n",
    "                assert first_signal > first_anti_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pattern / target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trust_interval(row, z=1.95):\n",
    "    \"\"\" Calculate trust interval for Bernulli distribution \"\"\"\n",
    "    sum, val1 = row['total'], row['count']\n",
    "    val2 = sum - val1\n",
    "    n = val1 + val2\n",
    "    p = val1 / n\n",
    "    low_bound = p - z * np.sqrt(p * (1-p) / n)\n",
    "    high_bound = p + z * np.sqrt(p * (1-p) / n)\n",
    "    return round(low_bound, 4), round(high_bound, 4)\n",
    "\n",
    "pvt = train_buy\n",
    "pvt = pvt[['target', 'pattern', 'time']]\n",
    "pvt['hour'] = pvt['time'].dt.hour\n",
    "pvt = pvt.pivot_table(index=['hour', 'target'], values='pattern', aggfunc='count').reset_index()\n",
    "pvt['total'] = pvt.groupby('hour')['pattern'].transform('sum')\n",
    "pvt.rename(columns={'pattern': 'count'}, inplace=True)\n",
    "pvt['pct'] = pvt['count'] / pvt['total']\n",
    "pvt = pvt[pvt['target']==1]\n",
    "pvt['trust_interval'] = pvt.apply(trust_interval, axis=1)\n",
    "\n",
    "print('Buy')\n",
    "display(pvt)\n",
    "display(train_buy['ttype'].value_counts())\n",
    "display(train_buy[['target', 'pattern']].value_counts())\n",
    "display(train_buy[['target', 'pattern']].value_counts(normalize=True))\n",
    "\n",
    "pvt = train_sell\n",
    "pvt = pvt[['target', 'pattern', 'time']]\n",
    "pvt['hour'] = pvt['time'].dt.hour\n",
    "pvt = pvt.pivot_table(index=['hour', 'target'], values='pattern', aggfunc='count').reset_index()\n",
    "pvt['total'] = pvt.groupby('hour')['pattern'].transform('sum')\n",
    "pvt.rename(columns={'pattern': 'count'}, inplace=True)\n",
    "pvt['pct'] = pvt['count'] / pvt['total']\n",
    "pvt = pvt[pvt['target']==1]\n",
    "pvt['trust_interval'] = pvt.apply(trust_interval, axis=1)\n",
    "\n",
    "print('Sell')\n",
    "display(pvt)\n",
    "display(train_sell['ttype'].value_counts())\n",
    "display(train_sell[['target', 'pattern']].value_counts())\n",
    "display(train_sell[['target', 'pattern']].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization\n",
    "\n",
    "### Plot ratio of class 1 for every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['date_month'] = train_df['time'].dt.to_period('M')\n",
    "group_df = train_df[train_df['time'].dt.year >= 2020]\n",
    "ax = group_df.groupby('date_month')['target'].mean().plot()\n",
    "ax.lines[0].set_linewidth(2)\n",
    "ax.set_xlabel('Month & year')\n",
    "ax.set_ylabel('Class 1 ratio')\n",
    "ax.set_title('Class 1 ratio vs Month')\n",
    "train_df = train_df.drop(columns='date_month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize buy trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "\n",
    "if CFG.create_dataset:\n",
    "\n",
    "    plt_num = 3\n",
    "    j = 1\n",
    "    fig = plt.figure(figsize=(30, 6 * plt_num))\n",
    "\n",
    "    buy_idxs = train_buy.index\n",
    "    test_buy = train_buy.sample(plt_num, axis=0)\n",
    "\n",
    "\n",
    "    for i, row in test_buy.iterrows():\n",
    "        ticker = row['ticker']\n",
    "        time = row['time']\n",
    "        target = row['target']\n",
    "        ttype = row['ttype']\n",
    "        pattern = row['pattern']\n",
    "\n",
    "        df_1h, _ = get_file(ticker)\n",
    "        df_1h = df_1h[(df_1h['time'] >= time) & (df_1h['time'] <= time + timedelta(hours=target_offset))]\n",
    "\n",
    "        ohlc = df_1h[['time', 'open', 'high', 'low', 'close', 'volume']].set_index('time')\n",
    "        \n",
    "        price = df_1h.iloc[0]['close']\n",
    "        high_price = price * CFG.cls_target_ratio_tp\n",
    "        low_price = price * (2 - CFG.cls_target_ratio_tp)\n",
    "\n",
    "        ax = fig.add_subplot(plt_num, 1, j)\n",
    "        mpf.plot(ohlc, type='candle', warn_too_much_data=1001, style='yahoo', ylabel='', tz_localize=True, ax=ax)\n",
    "        \n",
    "        if ttype == 'buy':\n",
    "            ax.axhline(high_price, color='g')\n",
    "            ax.axhline(low_price, color='r')\n",
    "        else:\n",
    "            ax.axhline(high_price, color='r')\n",
    "            ax.axhline(low_price, color='g')\n",
    "        ax.set_title(f'Ticker: {ticker}, price: {price}, pattern {pattern}, target: {target}', fontsize=20)\n",
    "        j += 1\n",
    "        \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sell trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.create_dataset:\n",
    "    plt_num = 3\n",
    "    j = 1\n",
    "    fig = plt.figure(figsize=(30, 6 * plt_num))\n",
    "\n",
    "    buy_idxs = train_sell.index\n",
    "    test_sell = train_sell.sample(plt_num, axis=0)\n",
    "\n",
    "    for i, row in test_sell.iterrows():\n",
    "        ticker = row['ticker']\n",
    "        time = row['time']\n",
    "        target = row['target']\n",
    "        ttype = row['ttype']\n",
    "        pattern = row['pattern']\n",
    "\n",
    "        df_1h, _ = get_file(ticker)\n",
    "        df_1h = df_1h[(df_1h['time'] >= time) & (df_1h['time'] <= time + timedelta(hours=target_offset))]\n",
    "        \n",
    "        ohlc = df_1h[['time', 'open', 'high', 'low', 'close', 'volume']].set_index('time')\n",
    "        \n",
    "        price = df_1h.iloc[0]['close']\n",
    "        high_price = price * CFG.cls_target_ratio_tp\n",
    "        low_price = price * (2 - CFG.cls_target_ratio_tp)\n",
    "\n",
    "        ax = fig.add_subplot(plt_num, 1, j)\n",
    "        mpf.plot(ohlc, type='candle', warn_too_much_data=1001, style='yahoo', ylabel='', tz_localize=True, ax=ax)\n",
    "        \n",
    "        if ttype == 'buy':\n",
    "            ax.axhline(high_price, color='g')\n",
    "            ax.axhline(low_price, color='r')\n",
    "        else:\n",
    "            ax.axhline(high_price, color='r')\n",
    "            ax.axhline(low_price, color='g')\n",
    "        ax.set_title(f'Ticker: {ticker}, price: {price}, pattern {pattern}, target: {target}', fontsize=20)\n",
    "        j += 1\n",
    "        \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.read_csv(f'model/feature_importance.csv')['Feature'].to_list()\n",
    "cols = [c for c in fi if 'prev' not in c]\n",
    "figsize = (20, 30)\n",
    "\n",
    "def plot_target_violine(df, df_cols, n_rows, n_cols, target):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    for idx, col in enumerate(df_cols):\n",
    "        ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        try:\n",
    "            sns.violinplot(x=target, y=col, data=df)\n",
    "        except ValueError:\n",
    "            print(f\"Can't find {col} in the dataframe\")\n",
    "\n",
    "        ax.set_ylabel(''); ax.spines['top'].set_visible(False), \n",
    "        ax.set_xlabel(''); ax.spines['right'].set_visible(False)\n",
    "        ax.set_title(f'{col}', loc='right', weight='bold', fontsize=21)\n",
    "\n",
    "    \n",
    "    fig.suptitle(f'Features vs Target ({target})\\n\\n\\n', ha='center',  fontweight='bold', fontsize=21)\n",
    "    # fig.legend([1, 0], loc='upper center', bbox_to_anchor=(0.5, 0.96), fontsize=21, ncol=3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "n_cols = 3\n",
    "n_rows = len(cols) // n_cols + 1\n",
    "plot_target_violine(train_df, cols, n_rows=n_rows, n_cols=n_cols, target='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the last signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_pickle('signal_stat/buy_stat_1h.pkl')\n",
    "x['ttype'] = 'buy'\n",
    "y = pd.read_pickle('signal_stat/sell_stat_1h.pkl')\n",
    "y['ttype'] = 'sell'\n",
    "x = pd.concat([x, y]).sort_values('time').reset_index(drop=True)\n",
    "x.loc[x['pattern'] == 'STOCH_RSI_Volume24', ['time', 'ticker', 'ttype', 'pattern']].tail(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "### Select features with BORUTA feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shaphypetune import BoostBoruta\n",
    "  \n",
    "params = {\n",
    "          'boosting_type': 'dart',\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.02,\n",
    "          'max_depth': 10,\n",
    "          'subsample' : 0.7,\n",
    "          'colsample_bytree': 0.85,\n",
    "          'num_leaves': 24,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain',\n",
    "          'max_bin': 255,\n",
    "          'reg_alpha': 1e-6,\n",
    "          'reg_lambda': 1e-7,\n",
    "          'objective': 'binary',\n",
    "          'metric': 'auc'\n",
    "        }\n",
    "features = [c for c in train_df.columns if c not in ['time', 'target', 'ticker', 'pattern', 'ttype']]\n",
    "\n",
    "\n",
    "def ppv_npv_acc(y_true, y_pred):\n",
    "    \"\"\" Calculate confusion matrix and return harmonic mean score of Positive Predictive Value (PPV, precisoin) and Negative Predictive Value (NPV) \"\"\"\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    return (tp + tn) / (tp + fp + tn + fn + 1e-8)\n",
    "\n",
    "def ppv_npv_acc_lgbm(y_true, y_pred):\n",
    "    \"\"\" Metric for LGBM \"\"\"\n",
    "    return 'ppv_npv_acc', ppv_npv_acc(y_true, y_pred), False\n",
    "\n",
    "\n",
    "def boruta_selction(df):\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "\n",
    "    X, y, time = df[features], df['target'], df['time']\n",
    "\n",
    "    tss = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=CFG.n_folds, test_size=len(X) // int(CFG.n_folds * 1.5))\n",
    "    eval_metric = 'logloss'\n",
    "\n",
    "    # Stratify based on Class and Alpha (3 types of conditions)\n",
    "    for fold, (train_idx, val_idx) in enumerate(tss.split(time)):\n",
    "\n",
    "        print(f'Fold: {fold}')\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**params)\n",
    "        model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False, max_iter=1000)\n",
    "        try:\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                      eval_metric='logloss', \n",
    "                      callbacks=[lgb.log_evaluation(100)])\n",
    "        except RuntimeError:\n",
    "            break\n",
    "\n",
    "        boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, index=X_train.columns).sort_index()\n",
    "        if boruta_df_.shape[0] == 0:\n",
    "            boruta_df_ = boruta_importance_df.copy()\n",
    "        else:\n",
    "            boruta_df_ += boruta_importance_df\n",
    "\n",
    "    boruta_df_ = boruta_df_.sort_values('importance')\n",
    "    boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "\n",
    "    return boruta_df_\n",
    "\n",
    "\n",
    "if CFG.select_features:\n",
    "    boruta_df_ = boruta_selction(train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features with permutation importance and GBM feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "def lgbm_tuning(df, permut=False):\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    \n",
    "    for fold in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{fold+1}')\n",
    "\n",
    "        X, y, time = df[features], df['target'], df['time']\n",
    "        tss = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=CFG.n_folds, test_size=len(X) // int(CFG.n_folds * 1.5))\n",
    "        \n",
    "        oof = np.zeros(len(df))\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(tss.split(time)):\n",
    "            if fold == 0:\n",
    "                first_val_idx = val_idx[0]\n",
    "            \n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            clf = lgb.LGBMClassifier(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                    eval_metric='logloss', \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            val_preds = clf.predict_proba(X_val)[:,1]\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                      columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "            \n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "        outer_cv = log_loss(y[first_val_idx:], oof[first_val_idx:])\n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "if CFG.select_features:\n",
    "    perm_df_, feature_importances_, outer_cv_score = lgbm_tuning(train_df, permut=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def rfe_selection(df):\n",
    "    params = {\n",
    "          'penalty': 'l2',\n",
    "          'max_iter': 10000,\n",
    "          'C': 1\n",
    "        }\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X, y = df[features], df['target']\n",
    "    X = scaler.fit_transform(X)\n",
    "        \n",
    "    estimator = LogisticRegression(**params)\n",
    "    selector = RFECV(estimator, min_features_to_select=50, step=0.025, cv=5, verbose=1)\n",
    "    selector = selector.fit(X, y)\n",
    "    rfe_df_ = pd.DataFrame({'importance': selector.ranking_}, index=features).sort_index()\n",
    "    rfe_df_ = rfe_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "    return rfe_df_\n",
    "\n",
    "if CFG.select_features:\n",
    "    rfe_df_ = rfe_selection(train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.select_features:\n",
    "    boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "    perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "    rfe_df_['rank'] = rfe_df_['importance']\n",
    "    feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "    fi = pd.concat([perm_df_[['Feature','rank']], feature_importances_[['Feature','rank']], rfe_df_[['Feature','rank']],\n",
    "                    boruta_df_[['Feature','rank']]])\n",
    "    fi = fi.groupby('Feature')['rank'].sum().sort_values().reset_index()\n",
    "    fi.to_csv(f'model/feature_importance.csv')\n",
    "else:\n",
    "    fi = pd.read_csv(f'model/feature_importance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "### Load selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_corr_features(features, corr_thresh):\n",
    "    features_to_select = features.copy()\n",
    "    correlations = train_df.loc[:, features_to_select].corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "    correlations = correlations[correlations['level_0'] != correlations['level_1']] \n",
    "    correlations.columns = ['feature_1', 'feature_2', 'corr']\n",
    "\n",
    "    correlations = pd.merge(left=correlations, right=fi, how='left', left_on='feature_1', right_on='Feature')\n",
    "    correlations = correlations.drop(columns='Feature')\n",
    "    correlations = correlations.sort_values(['corr', 'rank'], ascending=[False, True])\n",
    "    correlations = correlations[::2]\n",
    "\n",
    "    features_to_exclude = set()\n",
    "    correlations = correlations[correlations['corr'] > corr_thresh]\n",
    "\n",
    "    for _, row in correlations.iterrows():\n",
    "        feature_1 = row['feature_1']\n",
    "        feature_2 = row['feature_2']\n",
    "\n",
    "        if feature_1 in features_to_exclude:\n",
    "            continue\n",
    "\n",
    "        features_to_exclude.add(feature_2)\n",
    "\n",
    "    return features_to_exclude\n",
    "\n",
    "\n",
    "def prepare_features(fi, feature_num, corr_thresh):\n",
    "    ''' Get features, sort them by their time appearance and return for using in train and inference'''\n",
    "    # exclude some features \n",
    "    fi = fi['Feature']\n",
    "    fi = fi[:feature_num]\n",
    "    feature_dict = defaultdict(list)\n",
    "    features = list()\n",
    "    \n",
    "    for f in fi:\n",
    "        if f == 'volume_24':\n",
    "            feature_dict[0].append(f)\n",
    "            continue\n",
    "        period = f.split('_')\n",
    "        if period[-1].isdigit() and period[-2] == 'prev':\n",
    "            feature_dict[int(period[-1])].append('_'.join(period[:-2]))\n",
    "        else:\n",
    "            feature_dict[0].append(f)\n",
    "\n",
    "    feature_dict = dict(sorted(feature_dict.items()))\n",
    "    \n",
    "    for item in feature_dict.items():\n",
    "        if item[0] > 0:\n",
    "            features.extend([i + f'_prev_{item[0]}' for i in item[1]])\n",
    "        else:\n",
    "            features.extend([i for i in item[1]])\n",
    "    \n",
    "    # select only features with low correlation\n",
    "    features_to_exclude = exclude_corr_features(features, corr_thresh)\n",
    "    features = [f for f in features if f not in features_to_exclude]\n",
    "    features = list(features) + ['weekday']\n",
    "    \n",
    "    # remove highly correlated features from the feature dict\n",
    "    feature_dict['features'] = features\n",
    "\n",
    "    for item in feature_dict.items():\n",
    "        if not isinstance(item[0], int):\n",
    "            continue\n",
    "\n",
    "        features_to_remove = list()\n",
    "\n",
    "        for f in item[1]:\n",
    "            if item[0] > 0:\n",
    "                f_ = f'{f}_prev_{item[0]}'\n",
    "            else:\n",
    "                f_ = f\n",
    "            \n",
    "            if f_ not in features:\n",
    "                assert f_ in features_to_exclude\n",
    "                features_to_remove.append(f)\n",
    "        \n",
    "        feature_dict[item[0]] = [f for f in feature_dict[item[0]] if f not in features_to_remove]\n",
    "\n",
    "    # remove empty lists from feature_dict\n",
    "    empty_list_keys = list()\n",
    "    \n",
    "    for key in feature_dict:\n",
    "        if not feature_dict[key]:\n",
    "            empty_list_keys.append(key)\n",
    "            \n",
    "    for key in empty_list_keys:\n",
    "        del feature_dict[key]\n",
    "\n",
    "    return features, feature_dict\n",
    "\n",
    "feature_num = 470 # 569\n",
    "corr_thresh = 0.9654300044402008 # 0.9895527773275344\n",
    "\n",
    "train_df['weekday'] = train_df['time'].dt.weekday\n",
    "fi = pd.read_csv('model/feature_importance.csv')\n",
    "features, feature_dict = prepare_features(fi, feature_num, corr_thresh)\n",
    "assert len(features) == len(set(features))\n",
    "\n",
    "display(features, len(features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for train and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idxs = list()\n",
    "score_list = list()\n",
    "with open(f'model/bybit_tickers.json', 'r') as f:\n",
    "    bybit_tickers = json.load(f)\n",
    "df_len = train_df.shape[0]\n",
    "\n",
    "def conf_ppv_npv_acc_score(y, oof, low_bound, high_bound):\n",
    "    \"\"\" Consider only high confident objects and low confident objects for PPV and NPV score calculation \"\"\"\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof >= high_bound] = 1\n",
    "    pred_conf = pred_conf[(oof >= high_bound) | (oof <= low_bound)]\n",
    "    y_conf = y.values.reshape(-1,1)[(oof >= high_bound) | (oof <= low_bound)]\n",
    "    if y_conf.shape[0] == 0:\n",
    "        return 0, 0, 0\n",
    "    return ppv_npv_acc(y_conf, pred_conf), y_conf.shape[0], y_conf.shape[0]/y.shape[0]\n",
    "\n",
    "def model_train(df, features, params, n_folds, low_bound, high_bound, train_test, max_train_size=None, verbose=False): \n",
    "    X, time = df[features], df['time']\n",
    "    y = df['target']\n",
    "    \n",
    "    if train_test == 'fold':\n",
    "        oof = np.zeros([len(df), 1])\n",
    "\n",
    "        tss = TimeSeriesSplit(gap=0, max_train_size=max_train_size, n_splits=n_folds, test_size=(len(df) * 2) // (n_folds * 3))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Training with {len(features)} features\")\n",
    "        \n",
    "        for fold, (fit_idx, val_idx) in enumerate(tss.split(time)):\n",
    "            # select only those val_idxs that correspond to \n",
    "            # time = max train dataset time + 96 hours to prevent data leakage\n",
    "            max_time = time[fit_idx].max() + pd.to_timedelta(96, unit='h')\n",
    "            # also select only tickers from ByBit for validation because we trade on ByBit only\n",
    "            val_idx = time[(time > max_time) & (df['ticker'].isin(bybit_tickers))].index.tolist()\n",
    "            \n",
    "            val_idxs.extend(val_idx)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'Fold #{fold + 1}')\n",
    "            \n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[fit_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[fit_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if verbose:\n",
    "                display(y_val.value_counts(normalize=True))\n",
    "                display(df.loc[val_idx[0], 'time'])\n",
    "                display(df.loc[val_idx[-1], 'time'])\n",
    "                callbacks = [lgb.log_evaluation(100)]\n",
    "            else:\n",
    "                callbacks = []\n",
    "            \n",
    "            model_lgb = lgb.LGBMClassifier(**params)\n",
    "            model_lgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                          eval_metric='logloss', callbacks=callbacks)\n",
    "\n",
    "            val_preds = model_lgb.predict_proba(X_val)\n",
    "        \n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            ppv_npv_acc_score, conf_obj_num, conf_obj_pct = conf_ppv_npv_acc_score(y_val, val_preds[:, 1], low_bound, high_bound)\n",
    "            score_list.append(ppv_npv_acc_score)\n",
    "            if verbose:\n",
    "                print(f'Logloss: {val_score}, Confident objects score: {ppv_npv_acc_score}\\n'\n",
    "                      f'Number of confident objects {conf_obj_num}, % of confident objects: {conf_obj_pct}\\n'\n",
    "                      f'Number of profitable objects: {round((2 * ppv_npv_acc_score -  1) * conf_obj_num)}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "        \n",
    "        return oof, model_lgb, sorted(list(set(val_idxs)))\n",
    "    elif train_test == 'full' or train_test == 'inference':\n",
    "        print('Train on full data')\n",
    "        X, y = df[features], df['target']\n",
    "        \n",
    "        model_lgb = lgb.LGBMClassifier(**params)\n",
    "        model_lgb.fit(X, y, eval_set=[(X, y)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        \n",
    "        return np.zeros([df.shape[0], 1]), model_lgb, None\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        # Main parameters\n",
    "#                     'device': 'gpu',\n",
    "#                     'gpu_platform_id': 0,\n",
    "#                     'gpu_device_id': 0,\n",
    "        'objective': 'binary',\n",
    "        'metric': 'average_precison',\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt']), # ['dart', 'goss', 'gbdt']),   !!!\n",
    "        # Hyperparamters (in order of importance decreasing)\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 3000),  # max number of trees in model\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 3e-1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True), # L1,  alias: lambda_l1\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True), # L2, alias: lambda_l2\n",
    "         # decrease to deal with overfit\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10), # tree max depth \n",
    "         # decrease to deal with overfit\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 4, 512),  # Max number of leaves in one tree\n",
    "                                                                # should be ~ 2**(max_depth-1)\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.9), # Randomly select a subset of features \n",
    "                                                                   # if colsample_bytree < 1.0\n",
    "                                                                   # alias:feature_fraction\n",
    "        # increase for accuracy, decrease to deal with overfit\n",
    "        'max_bin': trial.suggest_int('max_bin', 32, 255), # Max number of bins that feature values will be bucketed in\n",
    "        # increase to deal with overfit\n",
    "        'subsample_freq': 1, # Perform bagging at every k iteration, alias: bagging_freq\n",
    "\n",
    "        # 'subsample_for_bin': 200000, # Number of data that sampled to construct feature discrete bins; setting this \n",
    "                                       # to larger value will give better training result but may increase train time \n",
    "        # 'cat_smooth': trial.suggest_float('cat_smooth', 10.0, 100.0),  # this can reduce the effect of noises in \n",
    "                                                                         # categorical features, especially for \n",
    "                                                                         # categories with few data                                  \n",
    "        'is_unbalance': trial.suggest_categorical('is_unbalance', [True, False]),                                            \n",
    "        'verbose': -1,\n",
    "        'importance_type': 'gain',\n",
    "        'high_bound': trial.suggest_float('high_bound', 0.5, 0.65),\n",
    "        'low_bound': trial.suggest_float('low_bound', 0.0, 0.1),\n",
    "        'feature_num': trial.suggest_int('feature_num', 30, 600),\n",
    "        'corr_thresh': trial.suggest_float('corr_thresh', 0.5, 0.99)\n",
    "    }\n",
    "    \n",
    "    if params['boosting_type'] != 'goss':\n",
    "        params['subsample'] = trial.suggest_float('subsample', 0.3, 0.9)\n",
    "\n",
    "    if params['is_unbalance'] == 'True':\n",
    "        params['class_weight'] = trial.suggest_categorical('class_weight', ['balanced', None]), \n",
    "\n",
    "    high_bound = params['high_bound']\n",
    "    del params['high_bound']\n",
    "\n",
    "    low_bound = params['low_bound']\n",
    "    del params['low_bound']\n",
    "\n",
    "    corr_thresh = params['corr_thresh']\n",
    "    del params['corr_thresh']\n",
    "    \n",
    "    fi = pd.read_csv(f'model/feature_importance.csv')\n",
    "    feature_num = params['feature_num']\n",
    "    del params['feature_num']\n",
    "    \n",
    "    train_df['weekday'] = train_df['time'].dt.weekday\n",
    "    features, _ = prepare_features(fi, feature_num, corr_thresh)\n",
    "    \n",
    "    oof, __, val_idxs = model_train(train_df[train_df['time'] < CFG.last_date], features, params, max_train_size=None, \n",
    "                                    n_folds=8, low_bound=low_bound, high_bound=high_bound, \n",
    "                                    train_test='fold', verbose=False)\n",
    "    y = train_df['target'][val_idxs]\n",
    "    oof = oof[val_idxs]\n",
    "    oof_conf_score, oof_conf_obj_num, oof_conf_obj_pct = conf_ppv_npv_acc_score(y, oof, low_bound, high_bound)\n",
    "    oof_val_score = log_loss(y, oof)\n",
    "    res = oof_conf_obj_num * (oof_conf_score-0.575) * 100\n",
    "    # save dataframe with additional information\n",
    "    df_optuna_more_info = pd.read_csv('optuna/optuna_lgbm_info.csv')\n",
    "    profit_objects = round(oof_conf_obj_num * (2 * oof_conf_score - 1))\n",
    "    tmp = pd.DataFrame({'value': [res], 'oof_conf_score': [oof_conf_score], \n",
    "                        'profit_objects': [profit_objects], 'oof_val_score': [oof_val_score], \n",
    "                        'oof_conf_obj_num': [oof_conf_obj_num], 'oof_conf_obj_pct': [oof_conf_obj_pct], \n",
    "                        })\n",
    "    df_optuna_more_info = pd.concat([df_optuna_more_info, tmp])\n",
    "    df_optuna_more_info.to_csv('optuna/optuna_lgbm_info.csv', index=False)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / optimize process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': 2199,\n",
    "    'learning_rate': 0.0021888119746491506,\n",
    "    'reg_alpha': 0.0002981624556905146,\n",
    "    'reg_lambda': 3.7892507774352097e-07,\n",
    "    'max_depth': 4,\n",
    "    'num_leaves': 107,\n",
    "    'colsample_bytree': 0.8672690606321763,\n",
    "    'max_bin': 241,\n",
    "    'is_unbalance': False,\n",
    "    'subsample': 0.6439343950294815,\n",
    "    'subsample_freq': 1,\n",
    "    'objective': 'binary',\n",
    "    'importance_type': 'gain',\n",
    "    'metric': 'average_precison',\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "train_test = 'test' # fold, inference, test\n",
    "max_train_size = None\n",
    "\n",
    "high_bound = 0.6203827236837305 # 0.51\n",
    "low_bound = 0 #.0413800710255353\n",
    "\n",
    "if CFG.optimize:\n",
    "    df_optuna_more_info = pd.DataFrame(columns=['value', 'oof_conf_score', 'profit_objects', \n",
    "                                                'oof_val_score', 'oof_conf_obj_num', 'oof_conf_obj_pct'])\n",
    "    df_optuna_more_info.to_csv('optuna/optuna_lgbm_info.csv', index=False)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=1000)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    df_optuna = study.trials_dataframe()\n",
    "    df_optuna = df_optuna.sort_values('value', ascending=False)\n",
    "    df_optuna.to_csv(f'optuna/optuna_lgbm.csv')\n",
    "\n",
    "    display(df_optuna.head(10))\n",
    "elif train_test == 'fold':\n",
    "    oof, _, val_idxs = model_train(train_df[train_df['time'] < CFG.last_date], features, params, n_folds=9, low_bound=low_bound, high_bound=high_bound, \n",
    "                                   train_test=train_test, max_train_size=max_train_size, verbose=True)\n",
    "    y = train_df['target'][val_idxs]\n",
    "    oof = oof[val_idxs]\n",
    "    oof_val_score = log_loss(y, oof)\n",
    "    oof_conf_score, oof_conf_obj_num, oof_conf_obj_pct = conf_ppv_npv_acc_score(y, oof, low_bound, high_bound)\n",
    "\n",
    "    print(f'Total fold Logloss: {oof_val_score}, Total confident objects score: {oof_conf_score}\\n'\n",
    "            f'Number of confident objects: {oof_conf_obj_num}, Total % of confident objects: {oof_conf_obj_pct}\\n' \n",
    "            f'Number of profitable objects: {round(( 2 * oof_conf_score - 1) * oof_conf_obj_num)}')\n",
    "elif train_test == 'inference': \n",
    "    oof, model, val_idxs = model_train(train_df, features, params, n_folds=1, low_bound=low_bound, \n",
    "                                       high_bound=high_bound, train_test=train_test, verbose=True)\n",
    "    joblib.dump(model, f'model/lgbm.pkl')\n",
    "    # save feature dictionary for further inference\n",
    "    with open(f'model/features.json', 'w') as f:\n",
    "        json.dump(feature_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with 387 features\n",
    "# Fold #1\n",
    "# 1    0.553066\n",
    "# 0    0.446934\n",
    "# Name: target, dtype: float64Timestamp('2022-09-05 23:00:00')Timestamp('2024-04-14 02:00:00')\n",
    "# [100]\ttraining's binary_logloss: 0.67555\tvalid_1's binary_logloss: 0.687585\n",
    "# [200]\ttraining's binary_logloss: 0.663418\tvalid_1's binary_logloss: 0.687801\n",
    "# [300]\ttraining's binary_logloss: 0.653303\tvalid_1's binary_logloss: 0.687987\n",
    "# [400]\ttraining's binary_logloss: 0.644629\tvalid_1's binary_logloss: 0.688283\n",
    "# [500]\ttraining's binary_logloss: 0.636965\tvalid_1's binary_logloss: 0.688467\n",
    "# [600]\ttraining's binary_logloss: 0.630304\tvalid_1's binary_logloss: 0.688646\n",
    "# [700]\ttraining's binary_logloss: 0.624285\tvalid_1's binary_logloss: 0.689114\n",
    "# [800]\ttraining's binary_logloss: 0.618825\tvalid_1's binary_logloss: 0.689669\n",
    "# [900]\ttraining's binary_logloss: 0.613587\tvalid_1's binary_logloss: 0.69008\n",
    "# [1000]\ttraining's binary_logloss: 0.608676\tvalid_1's binary_logloss: 0.69047\n",
    "# [1100]\ttraining's binary_logloss: 0.604135\tvalid_1's binary_logloss: 0.691047\n",
    "# [1200]\ttraining's binary_logloss: 0.599563\tvalid_1's binary_logloss: 0.691683\n",
    "# [1300]\ttraining's binary_logloss: 0.59518\tvalid_1's binary_logloss: 0.692345\n",
    "# [1400]\ttraining's binary_logloss: 0.59088\tvalid_1's binary_logloss: 0.692958\n",
    "# [1500]\ttraining's binary_logloss: 0.586842\tvalid_1's binary_logloss: 0.693482\n",
    "# [1600]\ttraining's binary_logloss: 0.58274\tvalid_1's binary_logloss: 0.694142\n",
    "# [1700]\ttraining's binary_logloss: 0.578907\tvalid_1's binary_logloss: 0.694806\n",
    "# [1800]\ttraining's binary_logloss: 0.575149\tvalid_1's binary_logloss: 0.695349\n",
    "# [1900]\ttraining's binary_logloss: 0.571378\tvalid_1's binary_logloss: 0.695782\n",
    "# [2000]\ttraining's binary_logloss: 0.567805\tvalid_1's binary_logloss: 0.69642\n",
    "# [2100]\ttraining's binary_logloss: 0.56416\tvalid_1's binary_logloss: 0.696848\n",
    "# Logloss: 0.6974799941460057, Confident objects score: 0.5681511470969821\n",
    "# Number of confident objects 3705, % of confident objects: 0.41921249151391715\n",
    "# Number of profitable objects: 505\n",
    "# Fold #2\n",
    "# 1    0.541893\n",
    "# 0    0.458107\n",
    "# Name: target, dtype: float64Timestamp('2022-12-20 02:00:00')Timestamp('2024-04-14 02:00:00')\n",
    "# [100]\ttraining's binary_logloss: 0.676396\tvalid_1's binary_logloss: 0.689929\n",
    "# [200]\ttraining's binary_logloss: 0.665809\tvalid_1's binary_logloss: 0.690178\n",
    "# [300]\ttraining's binary_logloss: 0.656915\tvalid_1's binary_logloss: 0.690109\n",
    "# [400]\ttraining's binary_logloss: 0.648919\tvalid_1's binary_logloss: 0.690023\n",
    "# [500]\ttraining's binary_logloss: 0.642362\tvalid_1's binary_logloss: 0.69007\n",
    "# [600]\ttraining's binary_logloss: 0.636297\tvalid_1's binary_logloss: 0.690155\n",
    "# [700]\ttraining's binary_logloss: 0.630846\tvalid_1's binary_logloss: 0.690733\n",
    "# [800]\ttraining's binary_logloss: 0.62592\tvalid_1's binary_logloss: 0.691104\n",
    "# [900]\ttraining's binary_logloss: 0.621228\tvalid_1's binary_logloss: 0.69154\n",
    "# [1000]\ttraining's binary_logloss: 0.616831\tvalid_1's binary_logloss: 0.691802\n",
    "# [1100]\ttraining's binary_logloss: 0.612695\tvalid_1's binary_logloss: 0.692049\n",
    "# [1200]\ttraining's binary_logloss: 0.608582\tvalid_1's binary_logloss: 0.692372\n",
    "# [1300]\ttraining's binary_logloss: 0.604763\tvalid_1's binary_logloss: 0.692879\n",
    "# [1400]\ttraining's binary_logloss: 0.600888\tvalid_1's binary_logloss: 0.693067\n",
    "# [1500]\ttraining's binary_logloss: 0.597303\tvalid_1's binary_logloss: 0.693404\n",
    "# [1600]\ttraining's binary_logloss: 0.593708\tvalid_1's binary_logloss: 0.693825\n",
    "# [1700]\ttraining's binary_logloss: 0.59025\tvalid_1's binary_logloss: 0.694021\n",
    "# [1800]\ttraining's binary_logloss: 0.586835\tvalid_1's binary_logloss: 0.694299\n",
    "# [1900]\ttraining's binary_logloss: 0.583566\tvalid_1's binary_logloss: 0.694551\n",
    "# [2000]\ttraining's binary_logloss: 0.580364\tvalid_1's binary_logloss: 0.694858\n",
    "# [2100]\ttraining's binary_logloss: 0.577164\tvalid_1's binary_logloss: 0.695063\n",
    "# Logloss: 0.6951293346549227, Confident objects score: 0.5822168087674291\n",
    "# Number of confident objects 2463, % of confident objects: 0.31846392552366176\n",
    "# Number of profitable objects: 405\n",
    "# Fold #3\n",
    "# 1    0.549211\n",
    "# 0    0.450789\n",
    "# Name: target, dtype: float64Timestamp('2023-03-24 04:00:00')Timestamp('2024-04-14 02:00:00')\n",
    "# [100]\ttraining's binary_logloss: 0.67695\tvalid_1's binary_logloss: 0.687826\n",
    "# [200]\ttraining's binary_logloss: 0.666605\tvalid_1's binary_logloss: 0.687303\n",
    "# [300]\ttraining's binary_logloss: 0.657799\tvalid_1's binary_logloss: 0.686652\n",
    "# [400]\ttraining's binary_logloss: 0.650531\tvalid_1's binary_logloss: 0.686307\n",
    "# [500]\ttraining's binary_logloss: 0.644343\tvalid_1's binary_logloss: 0.686196\n",
    "# [600]\ttraining's binary_logloss: 0.638558\tvalid_1's binary_logloss: 0.686115\n",
    "# [700]\ttraining's binary_logloss: 0.633385\tvalid_1's binary_logloss: 0.686424\n",
    "# [800]\ttraining's binary_logloss: 0.628654\tvalid_1's binary_logloss: 0.686614\n",
    "# [900]\ttraining's binary_logloss: 0.62425\tvalid_1's binary_logloss: 0.686875\n",
    "# [1000]\ttraining's binary_logloss: 0.620123\tvalid_1's binary_logloss: 0.687013\n",
    "# [1100]\ttraining's binary_logloss: 0.616125\tvalid_1's binary_logloss: 0.687427\n",
    "# [1200]\ttraining's binary_logloss: 0.612188\tvalid_1's binary_logloss: 0.687687\n",
    "# [1300]\ttraining's binary_logloss: 0.608558\tvalid_1's binary_logloss: 0.688045\n",
    "# [1400]\ttraining's binary_logloss: 0.604921\tvalid_1's binary_logloss: 0.688367\n",
    "# [1500]\ttraining's binary_logloss: 0.601679\tvalid_1's binary_logloss: 0.688648\n",
    "# [1600]\ttraining's binary_logloss: 0.598367\tvalid_1's binary_logloss: 0.688952\n",
    "# [1700]\ttraining's binary_logloss: 0.595074\tvalid_1's binary_logloss: 0.689163\n",
    "# [1800]\ttraining's binary_logloss: 0.591897\tvalid_1's binary_logloss: 0.689258\n",
    "# [1900]\ttraining's binary_logloss: 0.588763\tvalid_1's binary_logloss: 0.689412\n",
    "# [2000]\ttraining's binary_logloss: 0.58579\tvalid_1's binary_logloss: 0.689435\n",
    "# [2100]\ttraining's binary_logloss: 0.582817\tvalid_1's binary_logloss: 0.689505\n",
    "# Logloss: 0.6897219657989503, Confident objects score: 0.6032849020808384\n",
    "# Number of confident objects 1583, % of confident objects: 0.23786626596543953\n",
    "# Number of profitable objects: 327\n",
    "# Fold #4\n",
    "# 1    0.559905\n",
    "# 0    0.440095\n",
    "# Name: target, dtype: float64Timestamp('2023-05-28 23:00:00')Timestamp('2024-04-14 02:00:00')\n",
    "# [100]\ttraining's binary_logloss: 0.678869\tvalid_1's binary_logloss: 0.687208\n",
    "# [200]\ttraining's binary_logloss: 0.669304\tvalid_1's binary_logloss: 0.687853\n",
    "# [300]\ttraining's binary_logloss: 0.661187\tvalid_1's binary_logloss: 0.688524\n",
    "# [400]\ttraining's binary_logloss: 0.654273\tvalid_1's binary_logloss: 0.688913\n",
    "# [500]\ttraining's binary_logloss: 0.648408\tvalid_1's binary_logloss: 0.689211\n",
    "# [600]\ttraining's binary_logloss: 0.642988\tvalid_1's binary_logloss: 0.689088\n",
    "# [700]\ttraining's binary_logloss: 0.638137\tvalid_1's binary_logloss: 0.689207\n",
    "# [800]\ttraining's binary_logloss: 0.63376\tvalid_1's binary_logloss: 0.689128\n",
    "# [900]\ttraining's binary_logloss: 0.629534\tvalid_1's binary_logloss: 0.689195\n",
    "# [1000]\ttraining's binary_logloss: 0.625561\tvalid_1's binary_logloss: 0.689546\n",
    "# [1100]\ttraining's binary_logloss: 0.621908\tvalid_1's binary_logloss: 0.689852\n",
    "# [1200]\ttraining's binary_logloss: 0.6183\tvalid_1's binary_logloss: 0.689879\n",
    "# [1300]\ttraining's binary_logloss: 0.614899\tvalid_1's binary_logloss: 0.689856\n",
    "# [1400]\ttraining's binary_logloss: 0.611603\tvalid_1's binary_logloss: 0.690119\n",
    "# [1500]\ttraining's binary_logloss: 0.608474\tvalid_1's binary_logloss: 0.689811\n",
    "# [1600]\ttraining's binary_logloss: 0.605373\tvalid_1's binary_logloss: 0.689963\n",
    "# [1700]\ttraining's binary_logloss: 0.602431\tvalid_1's binary_logloss: 0.690108\n",
    "# [1800]\ttraining's binary_logloss: 0.599486\tvalid_1's binary_logloss: 0.690339\n",
    "# [1900]\ttraining's binary_logloss: 0.596609\tvalid_1's binary_logloss: 0.690805\n",
    "# [2000]\ttraining's binary_logloss: 0.59392\tvalid_1's binary_logloss: 0.690766\n",
    "# [2100]\ttraining's binary_logloss: 0.591137\tvalid_1's binary_logloss: 0.690783\n",
    "# Logloss: 0.6909285182141214, Confident objects score: 0.7179487179257068\n",
    "# Number of confident objects 312, % of confident objects: 0.05680990531682447\n",
    "# Number of profitable objects: 136\n",
    "# Fold #5\n",
    "# 1    0.570106\n",
    "# 0    0.429894\n",
    "# Name: target, dtype: float64Timestamp('2023-08-19 14:00:00')Timestamp('2024-04-14 02:00:00')\n",
    "# [100]\ttraining's binary_logloss: 0.680718\tvalid_1's binary_logloss: 0.685502\n",
    "# [200]\ttraining's binary_logloss: 0.671949\tvalid_1's binary_logloss: 0.685142\n",
    "# [300]\ttraining's binary_logloss: 0.664499\tvalid_1's binary_logloss: 0.684385\n",
    "# [400]\ttraining's binary_logloss: 0.658277\tvalid_1's binary_logloss: 0.683571\n",
    "# [500]\ttraining's binary_logloss: 0.652881\tvalid_1's binary_logloss: 0.683109\n",
    "# [600]\ttraining's binary_logloss: 0.647962\tvalid_1's binary_logloss: 0.682568\n",
    "# [700]\ttraining's binary_logloss: 0.64348\tvalid_1's binary_logloss: 0.682387\n",
    "# [800]\ttraining's binary_logloss: 0.639306\tvalid_1's binary_logloss: 0.682007\n",
    "# [900]\ttraining's binary_logloss: 0.635578\tvalid_1's binary_logloss: 0.68202\n",
    "# [1000]\ttraining's binary_logloss: 0.631863\tvalid_1's binary_logloss: 0.681872\n",
    "# [1100]\ttraining's binary_logloss: 0.628482\tvalid_1's binary_logloss: 0.681859\n",
    "# [1200]\ttraining's binary_logloss: 0.625123\tvalid_1's binary_logloss: 0.681686\n",
    "# [1300]\ttraining's binary_logloss: 0.62197\tvalid_1's binary_logloss: 0.681797\n",
    "# [1400]\ttraining's binary_logloss: 0.618929\tvalid_1's binary_logloss: 0.68188\n",
    "# [1500]\ttraining's binary_logloss: 0.616096\tvalid_1's binary_logloss: 0.681889\n",
    "# [1600]\ttraining's binary_logloss: 0.613276\tvalid_1's binary_logloss: 0.681861\n",
    "# [1700]\ttraining's binary_logloss: 0.610552\tvalid_1's binary_logloss: 0.681934\n",
    "# [1800]\ttraining's binary_logloss: 0.607846\tvalid_1's binary_logloss: 0.681961\n",
    "# [1900]\ttraining's binary_logloss: 0.605205\tvalid_1's binary_logloss: 0.682013\n",
    "# [2000]\ttraining's binary_logloss: 0.602643\tvalid_1's binary_logloss: 0.682081\n",
    "# [2100]\ttraining's binary_logloss: 0.600088\tvalid_1's binary_logloss: 0.682274\n",
    "# Logloss: 0.6823972533194677, Confident objects score: 0.6657223795845404\n",
    "# Number of confident objects 353, % of confident objects: 0.07970196432603296\n",
    "# Number of profitable objects: 117\n",
    "# Fold #6\n",
    "# 1    0.570359\n",
    "# 0    0.429641\n",
    "# Name: target, dtype: float64Timestamp('2023-10-25 21:00:00')Timestamp('2024-04-14 02:00:00')\n",
    "# [100]\ttraining's binary_logloss: 0.681135\tvalid_1's binary_logloss: 0.683927\n",
    "# [200]\ttraining's binary_logloss: 0.673058\tvalid_1's binary_logloss: 0.682611\n",
    "# [300]\ttraining's binary_logloss: 0.666192\tvalid_1's binary_logloss: 0.681495\n",
    "# [400]\ttraining's binary_logloss: 0.660601\tvalid_1's binary_logloss: 0.68076\n",
    "# [500]\ttraining's binary_logloss: 0.655592\tvalid_1's binary_logloss: 0.680001\n",
    "# [600]\ttraining's binary_logloss: 0.651\tvalid_1's binary_logloss: 0.679382\n",
    "# [700]\ttraining's binary_logloss: 0.646835\tvalid_1's binary_logloss: 0.679006\n",
    "# [800]\ttraining's binary_logloss: 0.642963\tvalid_1's binary_logloss: 0.678706\n",
    "# [900]\ttraining's binary_logloss: 0.63945\tvalid_1's binary_logloss: 0.678535\n",
    "# [1000]\ttraining's binary_logloss: 0.636117\tvalid_1's binary_logloss: 0.678466\n",
    "# [1100]\ttraining's binary_logloss: 0.632936\tvalid_1's binary_logloss: 0.678316\n",
    "# [1200]\ttraining's binary_logloss: 0.629816\tvalid_1's binary_logloss: 0.677996\n",
    "# [1300]\ttraining's binary_logloss: 0.626908\tvalid_1's binary_logloss: 0.677904\n",
    "# [1400]\ttraining's binary_logloss: 0.623988\tvalid_1's binary_logloss: 0.677884\n",
    "# [1500]\ttraining's binary_logloss: 0.621254\tvalid_1's binary_logloss: 0.677947\n",
    "# [1600]\ttraining's binary_logloss: 0.61857\tvalid_1's binary_logloss: 0.677868\n",
    "# [1700]\ttraining's binary_logloss: 0.616022\tvalid_1's binary_logloss: 0.677848\n",
    "# [1800]\ttraining's binary_logloss: 0.613442\tvalid_1's binary_logloss: 0.677709\n",
    "# [1900]\ttraining's binary_logloss: 0.610902\tvalid_1's binary_logloss: 0.677725\n",
    "# [2000]\ttraining's binary_logloss: 0.608488\tvalid_1's binary_logloss: 0.677909\n",
    "# [2100]\ttraining's binary_logloss: 0.606117\tvalid_1's binary_logloss: 0.677862\n",
    "# Logloss: 0.6778079636721536, Confident objects score: 0.6809651474348266\n",
    "# Number of confident objects 373, % of confident objects: 0.11167664670658682\n",
    "# Number of profitable objects: 135\n",
    "# Fold #7\n",
    "# 1    0.569853\n",
    "# 0    0.430147\n",
    "# Name: target, dtype: float64Timestamp('2023-12-29 21:00:00')Timestamp('2024-04-14 02:00:00')\n",
    "# [100]\ttraining's binary_logloss: 0.681992\tvalid_1's binary_logloss: 0.683903\n",
    "# [200]\ttraining's binary_logloss: 0.674976\tvalid_1's binary_logloss: 0.683054\n",
    "# [300]\ttraining's binary_logloss: 0.668974\tvalid_1's binary_logloss: 0.681891\n",
    "# [400]\ttraining's binary_logloss: 0.663807\tvalid_1's binary_logloss: 0.681006\n",
    "# [500]\ttraining's binary_logloss: 0.659266\tvalid_1's binary_logloss: 0.68058\n",
    "# [600]\ttraining's binary_logloss: 0.65513\tvalid_1's binary_logloss: 0.679792\n",
    "# [700]\ttraining's binary_logloss: 0.651498\tvalid_1's binary_logloss: 0.679281\n",
    "# [800]\ttraining's binary_logloss: 0.647982\tvalid_1's binary_logloss: 0.679041\n",
    "# [900]\ttraining's binary_logloss: 0.644713\tvalid_1's binary_logloss: 0.678806\n",
    "# [1000]\ttraining's binary_logloss: 0.641561\tvalid_1's binary_logloss: 0.678565\n",
    "# [1100]\ttraining's binary_logloss: 0.638628\tvalid_1's binary_logloss: 0.678375\n",
    "# [1200]\ttraining's binary_logloss: 0.63577\tvalid_1's binary_logloss: 0.678095\n",
    "# [1300]\ttraining's binary_logloss: 0.633043\tvalid_1's binary_logloss: 0.677893\n",
    "# [1400]\ttraining's binary_logloss: 0.630403\tvalid_1's binary_logloss: 0.677877\n",
    "# [1500]\ttraining's binary_logloss: 0.627858\tvalid_1's binary_logloss: 0.677837\n",
    "# [1600]\ttraining's binary_logloss: 0.625417\tvalid_1's binary_logloss: 0.677825\n",
    "# [1700]\ttraining's binary_logloss: 0.622954\tvalid_1's binary_logloss: 0.677579\n",
    "# [1800]\ttraining's binary_logloss: 0.62058\tvalid_1's binary_logloss: 0.677466\n",
    "# [1900]\ttraining's binary_logloss: 0.618286\tvalid_1's binary_logloss: 0.677415\n",
    "# [2000]\ttraining's binary_logloss: 0.615998\tvalid_1's binary_logloss: 0.677576\n",
    "# [2100]\ttraining's binary_logloss: 0.613866\tvalid_1's binary_logloss: 0.677524\n",
    "# Logloss: 0.6775058593260603, Confident objects score: 0.6972789115409088\n",
    "# Number of confident objects 294, % of confident objects: 0.12009803921568628\n",
    "# Number of profitable objects: 116\n",
    "# Fold #8\n",
    "# 1    0.575513\n",
    "# 0    0.424487\n",
    "# Name: target, dtype: float64Timestamp('2024-02-20 17:00:00')Timestamp('2024-04-14 02:00:00')\n",
    "# [100]\ttraining's binary_logloss: 0.682214\tvalid_1's binary_logloss: 0.683266\n",
    "# [200]\ttraining's binary_logloss: 0.675601\tvalid_1's binary_logloss: 0.682333\n",
    "# [300]\ttraining's binary_logloss: 0.669889\tvalid_1's binary_logloss: 0.68135\n",
    "# [400]\ttraining's binary_logloss: 0.665032\tvalid_1's binary_logloss: 0.680936\n",
    "# [500]\ttraining's binary_logloss: 0.660727\tvalid_1's binary_logloss: 0.680651\n",
    "# [600]\ttraining's binary_logloss: 0.656888\tvalid_1's binary_logloss: 0.679948\n",
    "# [700]\ttraining's binary_logloss: 0.653455\tvalid_1's binary_logloss: 0.679619\n",
    "# [800]\ttraining's binary_logloss: 0.650164\tvalid_1's binary_logloss: 0.679085\n",
    "# [900]\ttraining's binary_logloss: 0.647042\tvalid_1's binary_logloss: 0.678837\n",
    "# [1000]\ttraining's binary_logloss: 0.644137\tvalid_1's binary_logloss: 0.678554\n",
    "# [1100]\ttraining's binary_logloss: 0.641347\tvalid_1's binary_logloss: 0.678156\n",
    "# [1200]\ttraining's binary_logloss: 0.63864\tvalid_1's binary_logloss: 0.677988\n",
    "# [1300]\ttraining's binary_logloss: 0.636088\tvalid_1's binary_logloss: 0.677913\n",
    "# [1400]\ttraining's binary_logloss: 0.63354\tvalid_1's binary_logloss: 0.677597\n",
    "# [1500]\ttraining's binary_logloss: 0.631153\tvalid_1's binary_logloss: 0.677315\n",
    "# [1600]\ttraining's binary_logloss: 0.628855\tvalid_1's binary_logloss: 0.67747\n",
    "# [1700]\ttraining's binary_logloss: 0.62659\tvalid_1's binary_logloss: 0.677434\n",
    "# [1800]\ttraining's binary_logloss: 0.624323\tvalid_1's binary_logloss: 0.677131\n",
    "# [1900]\ttraining's binary_logloss: 0.622145\tvalid_1's binary_logloss: 0.677523\n",
    "# [2000]\ttraining's binary_logloss: 0.620033\tvalid_1's binary_logloss: 0.677777\n",
    "# [2100]\ttraining's binary_logloss: 0.617951\tvalid_1's binary_logloss: 0.678087\n",
    "# Logloss: 0.6779128479351424, Confident objects score: 0.7968749998754884\n",
    "# Number of confident objects 64, % of confident objects: 0.039776258545680544\n",
    "# Number of profitable objects: 38\n",
    "# Fold #9\n",
    "# 1    0.589372\n",
    "# 0    0.410628\n",
    "# Name: target, dtype: float64Timestamp('2024-03-29 22:00:00')Timestamp('2024-04-14 02:00:00')\n",
    "# [100]\ttraining's binary_logloss: 0.682684\tvalid_1's binary_logloss: 0.679259\n",
    "# [200]\ttraining's binary_logloss: 0.67666\tvalid_1's binary_logloss: 0.677232\n",
    "# [300]\ttraining's binary_logloss: 0.671355\tvalid_1's binary_logloss: 0.675347\n",
    "# [400]\ttraining's binary_logloss: 0.666687\tvalid_1's binary_logloss: 0.674095\n",
    "# [500]\ttraining's binary_logloss: 0.662706\tvalid_1's binary_logloss: 0.67306\n",
    "# [600]\ttraining's binary_logloss: 0.65906\tvalid_1's binary_logloss: 0.671306\n",
    "# [700]\ttraining's binary_logloss: 0.655739\tvalid_1's binary_logloss: 0.670403\n",
    "# [800]\ttraining's binary_logloss: 0.652625\tvalid_1's binary_logloss: 0.668899\n",
    "# [900]\ttraining's binary_logloss: 0.649707\tvalid_1's binary_logloss: 0.668093\n",
    "# [1000]\ttraining's binary_logloss: 0.647004\tvalid_1's binary_logloss: 0.66759\n",
    "# [1100]\ttraining's binary_logloss: 0.644376\tvalid_1's binary_logloss: 0.666875\n",
    "# [1200]\ttraining's binary_logloss: 0.641896\tvalid_1's binary_logloss: 0.666684\n",
    "# [1300]\ttraining's binary_logloss: 0.639499\tvalid_1's binary_logloss: 0.666511\n",
    "# [1400]\ttraining's binary_logloss: 0.637112\tvalid_1's binary_logloss: 0.666115\n",
    "# [1500]\ttraining's binary_logloss: 0.63484\tvalid_1's binary_logloss: 0.66574\n",
    "# [1600]\ttraining's binary_logloss: 0.632671\tvalid_1's binary_logloss: 0.665262\n",
    "# [1700]\ttraining's binary_logloss: 0.630551\tvalid_1's binary_logloss: 0.665319\n",
    "# [1800]\ttraining's binary_logloss: 0.628473\tvalid_1's binary_logloss: 0.66516\n",
    "# [1900]\ttraining's binary_logloss: 0.626449\tvalid_1's binary_logloss: 0.664901\n",
    "# [2000]\ttraining's binary_logloss: 0.624488\tvalid_1's binary_logloss: 0.66515\n",
    "# [2100]\ttraining's binary_logloss: 0.6225\tvalid_1's binary_logloss: 0.665377\n",
    "# Logloss: 0.6655431793266343, Confident objects score: 0.9436619716980759\n",
    "# Number of confident objects 71, % of confident objects: 0.08574879227053141\n",
    "# Number of profitable objects: 63\n",
    "# Total fold Logloss: 0.6852659553968125, Total confident objects score: 0.6736263736189712\n",
    "# Number of confident objects: 910, Total % of confident objects: 0.10296447159990949\n",
    "# Number of profitable objects: 316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.optimize and train_test == 'fold':\n",
    "    sns.lineplot(x=list(range(1, 10)), y=score_list)\n",
    "\n",
    "    plt.title('Model score by folds')\n",
    "    plt.xlabel('Folds')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.ylabel('Score')\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of model prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.optimize and train_test == 'fold':\n",
    "    sns.displot(oof)\n",
    "\n",
    "    plt.xlabel('Model prediction scores')\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.ylabel('Number of predictions')\n",
    "    plt.yticks(fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display PR curve for fold predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "if not CFG.optimize and train_test == 'fold':\n",
    "    disp = PrecisionRecallDisplay.from_predictions(\n",
    "        y.values, oof, name=\"PR AUC\"\n",
    "    )\n",
    "    plt.legend(loc='upper right')\n",
    "    _ = disp.ax_.set_title(\"2-class Precision-Recall curve\")\n",
    "    disp.ax_.lines[0].set_linewidth(2)\n",
    "\n",
    "# vol 1e6 AP=0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best threshold for fold predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "\n",
    "if not CFG.optimize and train_test == 'fold':\n",
    "    figsize = (10, 5)\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    score_list = list()\n",
    "    obj_num_list = list()\n",
    "    obj_pct_list = list()\n",
    "    obj_profit_list = list()\n",
    "    max_obj_profit = 0\n",
    "    for hb in np.arange(0.41, 0.61, 0.001):\n",
    "        score, obj_num, obj_pct = conf_ppv_npv_acc_score(y.reset_index(drop=True), oof, 0, hb)\n",
    "        if score == 0:\n",
    "            obj_num = 0\n",
    "            obj_pct = 0\n",
    "        bound, score, obj_num, obj_pct = round(hb, 4), round(score, 5), round(obj_num, 2), round(obj_pct, 2)\n",
    "        obj_profit = round((2 * score - 1) * obj_num)\n",
    "        score_list.append(score)\n",
    "        obj_num_list.append(obj_num)\n",
    "        obj_pct_list.append(obj_pct)\n",
    "        obj_profit_list.append(obj_profit)\n",
    "        max_obj_profit = max(max_obj_profit, obj_profit)\n",
    "        ic(bound, score, obj_num, obj_pct, obj_profit)\n",
    "\n",
    "    obj_profit_list = [o / max_obj_profit for o in obj_profit_list]\n",
    "    line1 = plt.plot(np.arange(0.41, 0.61, 0.001), score_list, label='precison score', linewidth=2)\n",
    "    line2 = plt.plot(np.arange(0.41, 0.61, 0.001), obj_pct_list, label='object pct', linewidth=2)\n",
    "    line3 = plt.plot(np.arange(0.41, 0.61, 0.001), obj_profit_list, label='number of profit objects', linewidth=2)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model\n",
    "\n",
    "### Test model predictions on the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snx - volume enough, exists in bybit tickers, maybe RSI is inconsistent or there are NaNs in snx datafreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-04-25 02:00:00')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-04-25 03:00:00')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    40\n",
       "0     2\n",
       "Name: target, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    0.952381\n",
       "0    0.047619\n",
       "Name: target, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>ticker</th>\n",
       "      <th>ttype</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_proba</th>\n",
       "      <th>target</th>\n",
       "      <th>pattern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-25 02:00:00</td>\n",
       "      <td>ARBUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.628171</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>EGLDUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.634007</td>\n",
       "      <td>0</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>DAOUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.626848</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>DYDXUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.625557</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>AAVEUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.627085</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>DGBUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.632198</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>BADGERUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.646413</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>APEUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.632222</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>CETUSUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.631084</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>AEVOUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.638028</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>CFXUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.622035</td>\n",
       "      <td>0</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>COMPUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.639367</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>CRVUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.625186</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>ALTUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.636610</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>AUDIOUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.625160</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>XRDUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.627067</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>ALPACAUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.641544</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>FORTHUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.622117</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>XNOUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.637530</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>WAVESUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.633229</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>SUIUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.632880</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>TAOUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.632173</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>SLPUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.622596</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>SCRTUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.636893</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>LINAUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.628063</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>RUNEUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.622841</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>SANDUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.629130</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>MAVUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.631547</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>MANTAUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623905</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>MAGICUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.626868</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>LUNA2USDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.624901</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>LITUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.624671</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>KEYUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.644921</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>INJUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.639146</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>GFTUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630599</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>MOVRUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.624826</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>GLMRUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.634940</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>RLCUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.624190</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>QIUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.626147</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>POWRUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.621937</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>RPLUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.646753</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2024-04-25 03:00:00</td>\n",
       "      <td>RONUSDT</td>\n",
       "      <td>buy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.631486</td>\n",
       "      <td>1</td>\n",
       "      <td>STOCH_RSI_Volume24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time      ticker ttype  pred  pred_proba  target  \\\n",
       "0  2024-04-25 02:00:00     ARBUSDT   buy     1    0.628171       1   \n",
       "1  2024-04-25 03:00:00    EGLDUSDT   buy     1    0.634007       0   \n",
       "2  2024-04-25 03:00:00     DAOUSDT   buy     1    0.626848       1   \n",
       "3  2024-04-25 03:00:00    DYDXUSDT   buy     1    0.625557       1   \n",
       "4  2024-04-25 03:00:00    AAVEUSDT   buy     1    0.627085       1   \n",
       "5  2024-04-25 03:00:00     DGBUSDT   buy     1    0.632198       1   \n",
       "6  2024-04-25 03:00:00  BADGERUSDT   buy     1    0.646413       1   \n",
       "7  2024-04-25 03:00:00     APEUSDT   buy     1    0.632222       1   \n",
       "8  2024-04-25 03:00:00   CETUSUSDT   buy     1    0.631084       1   \n",
       "9  2024-04-25 03:00:00    AEVOUSDT   buy     1    0.638028       1   \n",
       "10 2024-04-25 03:00:00     CFXUSDT   buy     1    0.622035       0   \n",
       "11 2024-04-25 03:00:00    COMPUSDT   buy     1    0.639367       1   \n",
       "12 2024-04-25 03:00:00     CRVUSDT   buy     1    0.625186       1   \n",
       "13 2024-04-25 03:00:00     ALTUSDT   buy     1    0.636610       1   \n",
       "14 2024-04-25 03:00:00   AUDIOUSDT   buy     1    0.625160       1   \n",
       "15 2024-04-25 03:00:00     XRDUSDT   buy     1    0.627067       1   \n",
       "16 2024-04-25 03:00:00  ALPACAUSDT   buy     1    0.641544       1   \n",
       "17 2024-04-25 03:00:00   FORTHUSDT   buy     1    0.622117       1   \n",
       "18 2024-04-25 03:00:00     XNOUSDT   buy     1    0.637530       1   \n",
       "19 2024-04-25 03:00:00   WAVESUSDT   buy     1    0.633229       1   \n",
       "20 2024-04-25 03:00:00     SUIUSDT   buy     1    0.632880       1   \n",
       "21 2024-04-25 03:00:00     TAOUSDT   buy     1    0.632173       1   \n",
       "22 2024-04-25 03:00:00     SLPUSDT   buy     1    0.622596       1   \n",
       "23 2024-04-25 03:00:00    SCRTUSDT   buy     1    0.636893       1   \n",
       "24 2024-04-25 03:00:00    LINAUSDT   buy     1    0.628063       1   \n",
       "25 2024-04-25 03:00:00    RUNEUSDT   buy     1    0.622841       1   \n",
       "26 2024-04-25 03:00:00    SANDUSDT   buy     1    0.629130       1   \n",
       "27 2024-04-25 03:00:00     MAVUSDT   buy     1    0.631547       1   \n",
       "28 2024-04-25 03:00:00   MANTAUSDT   buy     1    0.623905       1   \n",
       "29 2024-04-25 03:00:00   MAGICUSDT   buy     1    0.626868       1   \n",
       "30 2024-04-25 03:00:00   LUNA2USDT   buy     1    0.624901       1   \n",
       "31 2024-04-25 03:00:00     LITUSDT   buy     1    0.624671       1   \n",
       "32 2024-04-25 03:00:00     KEYUSDT   buy     1    0.644921       1   \n",
       "33 2024-04-25 03:00:00     INJUSDT   buy     1    0.639146       1   \n",
       "34 2024-04-25 03:00:00     GFTUSDT   buy     1    0.630599       1   \n",
       "35 2024-04-25 03:00:00    MOVRUSDT   buy     1    0.624826       1   \n",
       "36 2024-04-25 03:00:00    GLMRUSDT   buy     1    0.634940       1   \n",
       "37 2024-04-25 03:00:00     RLCUSDT   buy     1    0.624190       1   \n",
       "38 2024-04-25 03:00:00      QIUSDT   buy     1    0.626147       1   \n",
       "39 2024-04-25 03:00:00    POWRUSDT   buy     1    0.621937       1   \n",
       "40 2024-04-25 03:00:00     RPLUSDT   buy     1    0.646753       1   \n",
       "41 2024-04-25 03:00:00     RONUSDT   buy     1    0.631486       1   \n",
       "\n",
       "               pattern  \n",
       "0   STOCH_RSI_Volume24  \n",
       "1   STOCH_RSI_Volume24  \n",
       "2   STOCH_RSI_Volume24  \n",
       "3   STOCH_RSI_Volume24  \n",
       "4   STOCH_RSI_Volume24  \n",
       "5   STOCH_RSI_Volume24  \n",
       "6   STOCH_RSI_Volume24  \n",
       "7   STOCH_RSI_Volume24  \n",
       "8   STOCH_RSI_Volume24  \n",
       "9   STOCH_RSI_Volume24  \n",
       "10  STOCH_RSI_Volume24  \n",
       "11  STOCH_RSI_Volume24  \n",
       "12  STOCH_RSI_Volume24  \n",
       "13  STOCH_RSI_Volume24  \n",
       "14  STOCH_RSI_Volume24  \n",
       "15  STOCH_RSI_Volume24  \n",
       "16  STOCH_RSI_Volume24  \n",
       "17  STOCH_RSI_Volume24  \n",
       "18  STOCH_RSI_Volume24  \n",
       "19  STOCH_RSI_Volume24  \n",
       "20  STOCH_RSI_Volume24  \n",
       "21  STOCH_RSI_Volume24  \n",
       "22  STOCH_RSI_Volume24  \n",
       "23  STOCH_RSI_Volume24  \n",
       "24  STOCH_RSI_Volume24  \n",
       "25  STOCH_RSI_Volume24  \n",
       "26  STOCH_RSI_Volume24  \n",
       "27  STOCH_RSI_Volume24  \n",
       "28  STOCH_RSI_Volume24  \n",
       "29  STOCH_RSI_Volume24  \n",
       "30  STOCH_RSI_Volume24  \n",
       "31  STOCH_RSI_Volume24  \n",
       "32  STOCH_RSI_Volume24  \n",
       "33  STOCH_RSI_Volume24  \n",
       "34  STOCH_RSI_Volume24  \n",
       "35  STOCH_RSI_Volume24  \n",
       "36  STOCH_RSI_Volume24  \n",
       "37  STOCH_RSI_Volume24  \n",
       "38  STOCH_RSI_Volume24  \n",
       "39  STOCH_RSI_Volume24  \n",
       "40  STOCH_RSI_Volume24  \n",
       "41  STOCH_RSI_Volume24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not CFG.optimize and train_test == 'test':\n",
    "    model = joblib.load('model/lgbm.pkl')\n",
    "        \n",
    "    with open(f'model/features.json', 'r') as f:\n",
    "        features = json.load(f)['features']\n",
    "\n",
    "    with open(f'model/bybit_tickers.json', 'r') as f:\n",
    "        bybit_tickers = json.load(f)\n",
    "\n",
    "    X = train_df[(train_df['ticker'].isin(bybit_tickers))]\n",
    "    X['pred_proba'] = model.predict_proba(X[features])[:,1]\n",
    "    X['pred'] = 0\n",
    "    X.loc[X['pred_proba'] >= high_bound, 'pred'] = 1\n",
    "    res = X.loc[((X['pred_proba'] >= high_bound)) & (X['time'] > CFG.last_date), ['time', 'ticker', 'ttype', 'pred', 'pred_proba', 'target', 'pattern']]\n",
    "    res = res.reset_index(drop=True)\n",
    "    display(res['time'].min(), res['time'].max())\n",
    "    display(res['target'].value_counts())\n",
    "    display(res['target'].value_counts(normalize=True))\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot rolling mean ppv_npv_acc score of the model predictions for the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.optimize and train_test == 'test':\n",
    "    figsize = (12, 5)\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    tmp = res[res['pred'] == res['target']]\n",
    "    tmp['ppv_npv_acc'] = tmp['target'].rolling(len(tmp), min_periods=1).count() / (tmp.index + 1)\n",
    "\n",
    "    ax = sns.lineplot(x=tmp['time'], y=tmp['ppv_npv_acc'].values)\n",
    "    ax.lines[0].set_linewidth(2)\n",
    "    plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare indicator / signal values for bot and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from signals.find_signal import SignalFactory\n",
    "\n",
    "# ttype = 'buy'\n",
    "# ticker = 'METHUSDT'\n",
    "# month = 1\n",
    "# day = 25\n",
    "# hour = 6\n",
    "# configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "# x = pd.read_csv(f'../bot/ticker_dataframes/{ticker}_1h_{ttype}_{month}_{day}_{hour}.csv').drop(columns=['Unnamed: 0']).reset_index(drop=True)\n",
    "# y = pd.read_csv(f'../bot/ticker_dataframes/{ticker}_4h_{ttype}_{month}_{day}_3.csv').drop(columns=['Unnamed: 0']).reset_index(drop=True)\n",
    "\n",
    "# # add Volume24\n",
    "# vol24 = indicators.Volume24(ttype, configs)\n",
    "# x = vol24.get_indicator(x, '', '1h', 0)\n",
    "# # add Pattern\n",
    "# pattern = indicators.Pattern(ttype, configs)\n",
    "# x = pattern.get_indicator(x, '', '', 0)\n",
    "# # add trend\n",
    "# trend = indicators.Trend(ttype, configs)\n",
    "# y = trend.get_indicator(y, '', '', 0)\n",
    "\n",
    "# # cols = ['time', 'open', 'high', 'low', 'close', 'volume', 'rsi', 'stoch_slowk', 'stoch_slowd', 'linear_reg', 'linear_reg_angle', 'macd', 'macdsignal', 'macdhist']\n",
    "# cols = ['time', 'open', 'high', 'low', 'close', 'volume', 'linear_reg', 'linear_reg_angle', 'high_max', 'low_min', 'volume_24']\n",
    "\n",
    "# higher_features = ['time_4h', 'linear_reg', 'linear_reg_angle', 'macd', 'macdhist',  'macd_dir', \n",
    "#                    'macdsignal', 'macdsignal_dir']\n",
    "# y['time_4h'] = y['time'] + pd.to_timedelta(3, unit='h')\n",
    "# x[['time'] + higher_features] = pd.merge(x[['time']], y[higher_features], how='left', left_on='time', right_on='time_4h')\n",
    "\n",
    "# x.drop(columns=['time_4h'], inplace=True)\n",
    "# y.drop(columns=['time_4h'], inplace=True)\n",
    "# x.ffill(inplace=True)\n",
    "# x.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # get Swing pattern\n",
    "# pattern = SignalFactory().factory('Pattern', ttype, configs)\n",
    "# pattern_points = pattern.find_signal(x)\n",
    "# trend = SignalFactory().factory('Trend', ttype, configs)\n",
    "# trend_points = trend.find_signal(x)\n",
    "# idxs = np.where((pattern_points > 0) & (trend_points > 0))\n",
    "# display(x.loc[idxs[0], cols])\n",
    "\n",
    "# z = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_1h.pkl')\n",
    "# v = pd.read_pickle(f'../optimizer/ticker_dataframes/{ticker}_4h.pkl')\n",
    "\n",
    "# # add Volume24\n",
    "# vol24 = indicators.Volume24(ttype, configs)\n",
    "# z = vol24.get_indicator(z, '', '1h', 0)\n",
    "# # add Pattern\n",
    "# pattern = indicators.Pattern(ttype, configs)\n",
    "# z = pattern.get_indicator(z, '', '', 0)\n",
    "# # add Trend\n",
    "# trend = indicators.Trend(ttype, configs)\n",
    "# v = trend.get_indicator(v, '', '', 0)\n",
    "# z.tail(48)\n",
    "\n",
    "# v['time_4h'] = v['time'] + pd.to_timedelta(3, unit='h')\n",
    "# z[['time'] + higher_features] = pd.merge(z[['time']], y[higher_features], how='left', left_on='time', right_on='time_4h')\n",
    "\n",
    "# z.drop(columns=['time_4h'], inplace=True)\n",
    "# v.drop(columns=['time_4h'], inplace=True)\n",
    "# z.ffill(inplace=True)\n",
    "# z.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # get Swing pattern\n",
    "# pattern = SignalFactory().factory('Pattern', ttype, configs)\n",
    "# pattern_points = pattern.find_signal(z)\n",
    "# trend = SignalFactory().factory('Trend', ttype, configs)\n",
    "# trend_points = trend.find_signal(z)\n",
    "# idxs = np.where((pattern_points > 0) & (trend_points > 0))\n",
    "# display(z.loc[idxs[0], cols][-1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
