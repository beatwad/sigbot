{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import log_loss, mean_squared_error, precision_score\n",
    "\n",
    "import torch\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    ttype = 'buy'\n",
    "    select_features = False\n",
    "    train_NN = False\n",
    "    train_LGBM = True\n",
    "    n_repeats = 1\n",
    "    n_folds = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_angle_prev_96</th>\n",
       "      <th>macd_prev_96</th>\n",
       "      <th>macdsignal_prev_96</th>\n",
       "      <th>macdhist_prev_96</th>\n",
       "      <th>macd_dir_prev_96</th>\n",
       "      <th>macdsignal_dir_prev_96</th>\n",
       "      <th>atr_prev_96</th>\n",
       "      <th>close_smooth_prev_96</th>\n",
       "      <th>target</th>\n",
       "      <th>ttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-26 01:00:00</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>23.7600</td>\n",
       "      <td>23.8400</td>\n",
       "      <td>13772.65</td>\n",
       "      <td>64.054892</td>\n",
       "      <td>83.517327</td>\n",
       "      <td>87.090858</td>\n",
       "      <td>-0.029809</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.369665</td>\n",
       "      <td>-0.020052</td>\n",
       "      <td>-0.052992</td>\n",
       "      <td>0.032940</td>\n",
       "      <td>-0.201703</td>\n",
       "      <td>-0.103966</td>\n",
       "      <td>0.133742</td>\n",
       "      <td>22.383333</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-26 05:00:00</td>\n",
       "      <td>24.0400</td>\n",
       "      <td>24.1000</td>\n",
       "      <td>23.9800</td>\n",
       "      <td>24.0300</td>\n",
       "      <td>8515.96</td>\n",
       "      <td>68.293118</td>\n",
       "      <td>54.118435</td>\n",
       "      <td>61.220840</td>\n",
       "      <td>-0.108235</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.869113</td>\n",
       "      <td>-0.017024</td>\n",
       "      <td>-0.033512</td>\n",
       "      <td>0.016488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.106127</td>\n",
       "      <td>0.121932</td>\n",
       "      <td>22.381250</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-27 09:00:00</td>\n",
       "      <td>0.3615</td>\n",
       "      <td>0.3691</td>\n",
       "      <td>0.3614</td>\n",
       "      <td>0.3688</td>\n",
       "      <td>19678900.00</td>\n",
       "      <td>69.684900</td>\n",
       "      <td>34.733219</td>\n",
       "      <td>35.353221</td>\n",
       "      <td>-0.096031</td>\n",
       "      <td>...</td>\n",
       "      <td>7.045257</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159713</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.346717</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-12-27 10:00:00</td>\n",
       "      <td>0.1785</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.1785</td>\n",
       "      <td>2157869.00</td>\n",
       "      <td>71.380013</td>\n",
       "      <td>34.564017</td>\n",
       "      <td>34.117011</td>\n",
       "      <td>-0.041376</td>\n",
       "      <td>...</td>\n",
       "      <td>6.730134</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.171348</td>\n",
       "      <td>-0.088223</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.165029</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-12-27 11:00:00</td>\n",
       "      <td>116.1000</td>\n",
       "      <td>116.7000</td>\n",
       "      <td>115.3000</td>\n",
       "      <td>115.5000</td>\n",
       "      <td>3039.31</td>\n",
       "      <td>74.723751</td>\n",
       "      <td>81.513130</td>\n",
       "      <td>83.198644</td>\n",
       "      <td>-0.014239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832463</td>\n",
       "      <td>0.377508</td>\n",
       "      <td>0.098834</td>\n",
       "      <td>0.278674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.866341</td>\n",
       "      <td>0.978604</td>\n",
       "      <td>104.266667</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time      open      high       low     close       volume  \\\n",
       "0 2022-10-26 01:00:00   24.1500   24.1500   23.7600   23.8400     13772.65   \n",
       "1 2022-10-26 05:00:00   24.0400   24.1000   23.9800   24.0300      8515.96   \n",
       "4 2022-12-27 09:00:00    0.3615    0.3691    0.3614    0.3688  19678900.00   \n",
       "5 2022-12-27 10:00:00    0.1785    0.1794    0.1777    0.1785   2157869.00   \n",
       "6 2022-12-27 11:00:00  116.1000  116.7000  115.3000  115.5000      3039.31   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  64.054892    83.517327    87.090858        -0.029809  ...   \n",
       "1  68.293118    54.118435    61.220840        -0.108235  ...   \n",
       "4  69.684900    34.733219    35.353221        -0.096031  ...   \n",
       "5  71.380013    34.564017    34.117011        -0.041376  ...   \n",
       "6  74.723751    81.513130    83.198644        -0.014239  ...   \n",
       "\n",
       "   linear_reg_angle_prev_96  macd_prev_96  macdsignal_prev_96  \\\n",
       "0                -10.369665     -0.020052           -0.052992   \n",
       "1                -11.869113     -0.017024           -0.033512   \n",
       "4                  7.045257      0.001233            0.000785   \n",
       "5                  6.730134      0.000123            0.000277   \n",
       "6                  0.832463      0.377508            0.098834   \n",
       "\n",
       "   macdhist_prev_96  macd_dir_prev_96  macdsignal_dir_prev_96  atr_prev_96  \\\n",
       "0          0.032940         -0.201703               -0.103966     0.133742   \n",
       "1          0.016488          0.000000               -0.106127     0.121932   \n",
       "4          0.000447          0.000000                0.159713     0.002303   \n",
       "5         -0.000155         -0.171348               -0.088223     0.002358   \n",
       "6          0.278674          0.000000               -0.866341     0.978604   \n",
       "\n",
       "   close_smooth_prev_96  target  ttype  \n",
       "0             22.383333       1    buy  \n",
       "1             22.381250       1    buy  \n",
       "4              0.346717       0    buy  \n",
       "5              0.165029       1    buy  \n",
       "6            104.266667       0    buy  \n",
       "\n",
       "[5 rows x 505 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6677, 505)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 96\n",
    "\n",
    "if CFG.ttype == 'buy':\n",
    "    df = pd.read_pickle(f'signal_stat/train_buy_{last}.pkl')\n",
    "else:\n",
    "    df = pd.read_pickle(f'signal_stat/train_sell_{last}.pkl')\n",
    "\n",
    "# df = df[df['pattern'].isin(['MACD', 'STOCH_RSI'])]\n",
    "\n",
    "display(df.head())\n",
    "display(df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split by ticker group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.1, n_splits=2, random_state = 7)\n",
    "\n",
    "split = splitter.split(df, groups=df['ticker'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_df = df.iloc[train_inds]\n",
    "test_df = df.iloc[test_inds]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_size=0.2\n",
    "\n",
    "x_data = train_df.drop(['target', 'time', 'ticker', 'pattern', 'ttype'], axis=1)\n",
    "y_data = train_df['target']\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n",
    "input_channels = x_data.shape[1]\n",
    "output_size = len(set(y_data))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train[x_train.columns] = scaler.fit_transform(x_train)\n",
    "x_valid[x_valid.columns] = scaler.transform(x_valid)\n",
    "\n",
    "x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "x_valid = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_train), type(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(SigModel, self, ).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(input_channels, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "        self.layers.add_module('sigmoid', torch.nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train).squeeze()\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train)\n",
    "    train_acc = (train_preds.round() == y_train).float().mean()\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad() # ~ model.zero_grad()\n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid).squeeze()\n",
    "        val_loss = criterion(val_preds, y_valid)\n",
    "        val_acc = (val_preds.round() == y_valid).float().mean()\n",
    "    \n",
    "    # update weights according to gradient value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# Initialize model\n",
    "if CFG.train_NN:\n",
    "    model = SigModel(input_channels).to(device)\n",
    "\n",
    "    # Number of epochs\n",
    "    epochs = 100000\n",
    "\n",
    "    # Send data to the device\n",
    "    x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "    y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "    train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    # Empty loss lists to track values\n",
    "    epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    learning_rate = 1e-6\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2000, threshold=1e-2)\n",
    "\n",
    "    # Loop through the data\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc, val_loss, val_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "        # Print progress a total of 20 times\n",
    "        if epoch % int(epochs / 20) == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.5f}\\\n",
    "                Validation Loss: {val_loss:.5f}, Val Acc: {val_acc:.5f}\\\n",
    "                    LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]}')\n",
    "\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(train_loss.cpu().detach().numpy())\n",
    "            valid_loss_values.append(val_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NN train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CFG.train_NN:\n",
    "    plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "    plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss Curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern', 'ttype']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta and boruta_df_.shape[0] > 0:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 2000,\n",
    "          'learning_rate': 0.02,\n",
    "        #   'early_stopping_round': 100,\n",
    "          'max_depth': 10,\n",
    "          'subsample' : 0.7,\n",
    "          'colsample_bytree': 0.85,\n",
    "          'num_leaves': 24,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain',\n",
    "          'max_bin': 255,\n",
    "          'reg_alpha': 1e-6,\n",
    "          'reg_lambda': 1e-8\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "if CFG.select_features:\n",
    "    perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(df, permut=True, boruta=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.select_features:\n",
    "    perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "    boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "    feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "    fi = pd.concat([perm_df_[['Feature','rank']], feature_importances_[['Feature','rank']], boruta_df_[['Feature','rank']]])\n",
    "    fi = fi.groupby('Feature')['rank'].sum().sort_values().reset_index()\n",
    "    fi.to_csv(f'feature_importance_{CFG.ttype}.csv')\n",
    "else:\n",
    "    fi = pd.read_csv(f'feature_importance_all.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize feature distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test split based on ticker groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on full data\n",
      "[100]\ttraining's binary_logloss: 0.643439\ttraining's average_precision: 0.846616\n",
      "[200]\ttraining's binary_logloss: 0.620303\ttraining's average_precision: 0.868642\n",
      "[300]\ttraining's binary_logloss: 0.596127\ttraining's average_precision: 0.901756\n",
      "[400]\ttraining's binary_logloss: 0.573402\ttraining's average_precision: 0.926093\n",
      "[500]\ttraining's binary_logloss: 0.550141\ttraining's average_precision: 0.946718\n",
      "[600]\ttraining's binary_logloss: 0.5406\ttraining's average_precision: 0.959144\n",
      "[700]\ttraining's binary_logloss: 0.522762\ttraining's average_precision: 0.970621\n",
      "[800]\ttraining's binary_logloss: 0.51051\ttraining's average_precision: 0.976859\n",
      "[900]\ttraining's binary_logloss: 0.493934\ttraining's average_precision: 0.982936\n",
      "[1000]\ttraining's binary_logloss: 0.48207\ttraining's average_precision: 0.987618\n",
      "[1100]\ttraining's binary_logloss: 0.46784\ttraining's average_precision: 0.990962\n",
      "[1200]\ttraining's binary_logloss: 0.459573\ttraining's average_precision: 0.992904\n",
      "[1300]\ttraining's binary_logloss: 0.448521\ttraining's average_precision: 0.994853\n",
      "[1400]\ttraining's binary_logloss: 0.43342\ttraining's average_precision: 0.996621\n",
      "[1500]\ttraining's binary_logloss: 0.426114\ttraining's average_precision: 0.997493\n",
      "[1600]\ttraining's binary_logloss: 0.413553\ttraining's average_precision: 0.998244\n",
      "[1700]\ttraining's binary_logloss: 0.400934\ttraining's average_precision: 0.998835\n",
      "[1800]\ttraining's binary_logloss: 0.39051\ttraining's average_precision: 0.999238\n",
      "[1900]\ttraining's binary_logloss: 0.379624\ttraining's average_precision: 0.999505\n",
      "[2000]\ttraining's binary_logloss: 0.370622\ttraining's average_precision: 0.999672\n"
     ]
    }
   ],
   "source": [
    "def model_train(df, train_df, test_df, features, task_type, how, n_folds, low_bound, high_bound, train_test): \n",
    "    X, groups = train_df[features], train_df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(train_df[['pattern']], drop_first=True)], axis=1)\n",
    "    y = train_df['target']\n",
    "    \n",
    "    if train_test == 'fold':\n",
    "        oof = np.zeros([train_df['target'].shape[0], 1])\n",
    "        \n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=24082023)\n",
    "\n",
    "        oe_enc = OrdinalEncoder()\n",
    "        groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "        print(f\"Training with {len(features)} features\")\n",
    "        \n",
    "        for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            print(f'Fold #{fold + 1}')\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[fit_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[fit_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "            \n",
    "            models = list()\n",
    "            if how == 'lgbm':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "            elif how == 'lreg':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            prec_score, prec_obj_pct = confident_score(y_val, val_preds[:,1], low_bound, high_bound)\n",
    "            print(f'Logloss: {val_score}, Confident objects precision: {prec_score}, % of confident objects: {prec_obj_pct}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "\n",
    "        return oof, model\n",
    "    elif train_test == 'full':\n",
    "        print('Train on full data')\n",
    "        X, y = df[features], df['target']\n",
    "        X = pd.concat([X, pd.get_dummies(df[['pattern']], drop_first=True)], axis=1)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X, y, eval_set=[(X, y)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        return np.zeros([df.shape[0], 1]), model\n",
    "    else:\n",
    "        # fit model on full dataset and predict on test\n",
    "        print(\"Test fold\")\n",
    "        X_test, y_test = test_df[features], test_df['target']\n",
    "        X_test = pd.concat([X_test, pd.get_dummies(test_df[['pattern']], drop_first=True)], axis=1)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X, y, eval_set=[(X_test, y_test)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        oof_test = np.zeros([test_df['target'].shape[0], 1])\n",
    "        oof_test[:,0] = model.predict_proba(X_test)[:,1]\n",
    "        return oof_test, model\n",
    "\n",
    "def prepare_features(fi, feature_num):\n",
    "    ''' Get features, sort them by their time appearance and return for using in train and inference'''\n",
    "    fi = fi['Feature'].head(feature_num)\n",
    "    feature_dict = defaultdict(list)\n",
    "    features = list()\n",
    "\n",
    "    for f in fi:\n",
    "        period = f.split('_')\n",
    "        if period[-1].isdigit():\n",
    "            feature_dict[int(period[-1])].append('_'.join(period[:-2]))\n",
    "        else:\n",
    "            feature_dict[0].append(f)\n",
    "\n",
    "    feature_dict = dict(sorted(feature_dict.items()))\n",
    "    \n",
    "    for item in feature_dict.items():\n",
    "        if item[0] > 0:\n",
    "            features.extend([i + f'_prev_{item[0]}' for i in item[1]])\n",
    "        else:\n",
    "            features.extend([i for i in item[1]])\n",
    "\n",
    "    feature_dict['features'] = features + ['Pattern_Trend', 'STOCH_RSI']\n",
    "\n",
    "    return features, feature_dict\n",
    "\n",
    "def confident_score(y, oof, low_bound, high_bound):\n",
    "    ''' Consider only high confident objects for accuracy and precision scores calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    # pred_conf_acc = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "    pred_conf_prec = pred_conf[(oof > high_bound)]\n",
    "    # y_conf_acc = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "    y_conf_prec = y.values.reshape(-1,1)[(oof > high_bound)]\n",
    "\n",
    "    return precision_score(y_conf_prec, pred_conf_prec), y_conf_prec.shape[0]/y.shape[0]\n",
    "\n",
    "train_test = 'full' # fold, test, full, inference\n",
    "low_bound, high_bound = 0.31, 0.69\n",
    "feature_num = 160\n",
    "\n",
    "\n",
    "if CFG.ttype == 'buy':\n",
    "    low_bound, high_bound = 0.31, 0.69\n",
    "    params = {\n",
    "        'boosting_type': 'dart',\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.02,\n",
    "        #   'early_stopping_round': 50,\n",
    "        'max_depth': 10,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'subsample': 0.85,\n",
    "        'subsample_freq': 1,\n",
    "        'num_leaves': 24,\n",
    "        'verbosity': -1,\n",
    "        'max_bin': 255,\n",
    "        'reg_alpha': 1e-6,\n",
    "        'reg_lambda': 1e-8,\n",
    "        'objective': 'binary',\n",
    "        # 'is_unbalance': True,\n",
    "        # 'class_weight': 'balanced',\n",
    "        'metric': 'average_precision'\n",
    "        }\n",
    "else:\n",
    "    low_bound, high_bound = 0.3, 0.7\n",
    "    params = {\n",
    "        'boosting_type': 'dart',\n",
    "        'n_estimators': 2200,\n",
    "        'learning_rate': 0.02,\n",
    "        #   'early_stopping_round': 50,\n",
    "        'max_depth': 10,\n",
    "        'colsample_bytree': 0.75,\n",
    "        'subsample': 0.85,\n",
    "        'subsample_freq': 1,\n",
    "        'num_leaves': 25,\n",
    "        'verbosity': -1,\n",
    "        'max_bin': 255,\n",
    "        'reg_alpha': 1e-6,\n",
    "        'reg_lambda': 1e-8,\n",
    "        'objective': 'binary',\n",
    "        # 'is_unbalance': True,\n",
    "        # 'class_weight': 'balanced',\n",
    "        'metric': 'average_precision'\n",
    "        }\n",
    "\n",
    "if CFG.train_LGBM:\n",
    "    fi = pd.read_csv(f'feature_importance_{CFG.ttype}.csv')\n",
    "    features, feature_dict = prepare_features(fi, feature_num)\n",
    "    if train_test != 'inference':\n",
    "        oof, model = model_train(df, train_df, test_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound, train_test=train_test)\n",
    "\n",
    "    if train_test == 'fold':\n",
    "        y = train_df['target']\n",
    "        oof_val_score = log_loss(y, oof)\n",
    "        oof_conf_prec_score, oof_conf_obj_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "        print(f'Total fold Logloss: {oof_val_score}, Total confident objects precision: {oof_conf_prec_score}, Total % of confident objects: {oof_conf_obj_pct}')\n",
    "    elif train_test == 'test':\n",
    "        y_test = test_df['target']\n",
    "        test_val_score = log_loss(y_test, oof)\n",
    "        test_conf_prec_score, test_conf_obj_pct = confident_score(y_test, oof, low_bound, high_bound)\n",
    "        print(f'Total test Logloss: {test_val_score}, Total test confident objects precision: {test_conf_prec_score}, Total % of test confident objects: {test_conf_obj_pct}')\n",
    "        # save feature dictionary for further inference\n",
    "        joblib.dump(model, f'lgbm_{CFG.ttype}.pkl')\n",
    "    elif train_test == 'full':\n",
    "        joblib.dump(model, f'lgbm_{CFG.ttype}.pkl')\n",
    "        # save feature dictionary for further inference\n",
    "        with open(f'features_{CFG.ttype}.json', 'w') as f:\n",
    "            json.dump(feature_dict, f)\n",
    "    elif train_test == 'inference':\n",
    "        model = joblib.load(f'lgbm_{CFG.ttype}.pkl')\n",
    "        X_test, y_test = test_df[features], test_df['target']\n",
    "        X_test = pd.concat([X_test, pd.get_dummies(test_df[['pattern']], drop_first=True)], axis=1)\n",
    "        oof = np.nan_to_num(model.predict_proba(X_test)[:,1])\n",
    "        test_val_score = log_loss(y_test, oof)\n",
    "        test_conf_prec_score, test_conf_obj_pct = confident_score(y_test, oof, low_bound, high_bound)\n",
    "        print(f'Loaded model test Logloss: {test_val_score}, Loaded model test confident objects precision: {test_conf_prec_score}, Total % of test confident objects: {test_conf_obj_pct}')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buy\n",
    "\n",
    "Total fold Logloss: 0.6702365645667008, Total confident objects precision: 0.7347767253044655, Total % of confident objects: 0.12053498613603002\n",
    "\n",
    "Total test Logloss: 0.667474687137724, Total test confident objects precision: 0.7719298245614035, Total % of test confident objects: 0.1043956043956044\n",
    "\n",
    "Sell\n",
    "\n",
    "Total fold Logloss: 0.6309185725054093, Total confident objects precision: 0.7591888466413181, Total % of confident objects: 0.26529926025554806\n",
    "\n",
    "Total test Logloss: 0.6428094828596689, Total test confident objects precision: 0.7267441860465116, Total % of test confident objects: 0.23243243243243245"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count predictions according to pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3189\n",
       "0    2942\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_test == 'fold':\n",
    "    train_df.loc[:,'oof'] = oof >= high_bound\n",
    "    display(train_df.groupby('pattern')['oof'].agg(['mean', 'count']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
