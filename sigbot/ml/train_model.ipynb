{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import log_loss, mean_squared_error, precision_score\n",
    "\n",
    "import torch\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    ttype = 'both'\n",
    "    patterns_to_filter = ['MACD', 'STOCH_RSI']\n",
    "    select_features = False\n",
    "    train_NN = False\n",
    "    train_LGBM = True\n",
    "    n_repeats = 1\n",
    "    n_folds = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_angle_prev_192</th>\n",
       "      <th>macd_prev_192</th>\n",
       "      <th>macdsignal_prev_192</th>\n",
       "      <th>macdhist_prev_192</th>\n",
       "      <th>macd_dir_prev_192</th>\n",
       "      <th>macdsignal_dir_prev_192</th>\n",
       "      <th>atr_prev_192</th>\n",
       "      <th>close_smooth_prev_192</th>\n",
       "      <th>target</th>\n",
       "      <th>ttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-31 15:00:00</td>\n",
       "      <td>145.9000</td>\n",
       "      <td>146.1000</td>\n",
       "      <td>145.800</td>\n",
       "      <td>145.9000</td>\n",
       "      <td>987.752</td>\n",
       "      <td>53.009345</td>\n",
       "      <td>56.771800</td>\n",
       "      <td>52.945269</td>\n",
       "      <td>0.090417</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.533125</td>\n",
       "      <td>-1.228202</td>\n",
       "      <td>-0.762369</td>\n",
       "      <td>-0.465833</td>\n",
       "      <td>0.158853</td>\n",
       "      <td>0.157152</td>\n",
       "      <td>1.135257</td>\n",
       "      <td>144.495833</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-31 19:00:00</td>\n",
       "      <td>3.6420</td>\n",
       "      <td>3.6460</td>\n",
       "      <td>3.634</td>\n",
       "      <td>3.6350</td>\n",
       "      <td>39534.000</td>\n",
       "      <td>61.238560</td>\n",
       "      <td>82.150627</td>\n",
       "      <td>81.754145</td>\n",
       "      <td>0.012704</td>\n",
       "      <td>...</td>\n",
       "      <td>6.972738</td>\n",
       "      <td>0.013083</td>\n",
       "      <td>0.015791</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>-0.140132</td>\n",
       "      <td>-0.011339</td>\n",
       "      <td>0.034240</td>\n",
       "      <td>3.614208</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 11:00:00</td>\n",
       "      <td>0.1221</td>\n",
       "      <td>0.1221</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>271447.100</td>\n",
       "      <td>54.286723</td>\n",
       "      <td>75.980296</td>\n",
       "      <td>73.911589</td>\n",
       "      <td>0.087652</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.046410</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.131526</td>\n",
       "      <td>-0.036094</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.116646</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-02 15:00:00</td>\n",
       "      <td>246.8000</td>\n",
       "      <td>247.6000</td>\n",
       "      <td>246.800</td>\n",
       "      <td>247.1000</td>\n",
       "      <td>5653.787</td>\n",
       "      <td>64.061825</td>\n",
       "      <td>83.941481</td>\n",
       "      <td>81.322513</td>\n",
       "      <td>0.039338</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.808189</td>\n",
       "      <td>-0.054825</td>\n",
       "      <td>-0.071140</td>\n",
       "      <td>0.016315</td>\n",
       "      <td>0.394717</td>\n",
       "      <td>-0.071718</td>\n",
       "      <td>0.754032</td>\n",
       "      <td>244.741667</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-02 15:00:00</td>\n",
       "      <td>4.1250</td>\n",
       "      <td>4.1400</td>\n",
       "      <td>4.111</td>\n",
       "      <td>4.1140</td>\n",
       "      <td>43988.110</td>\n",
       "      <td>60.923762</td>\n",
       "      <td>73.832294</td>\n",
       "      <td>77.156245</td>\n",
       "      <td>-0.037848</td>\n",
       "      <td>...</td>\n",
       "      <td>3.921538</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>-0.003401</td>\n",
       "      <td>-0.121655</td>\n",
       "      <td>-0.052706</td>\n",
       "      <td>0.021675</td>\n",
       "      <td>3.736542</td>\n",
       "      <td>0</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 985 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time      open      high      low     close      volume  \\\n",
       "0 2022-12-31 15:00:00  145.9000  146.1000  145.800  145.9000     987.752   \n",
       "1 2022-12-31 19:00:00    3.6420    3.6460    3.634    3.6350   39534.000   \n",
       "2 2023-01-01 11:00:00    0.1221    0.1221    0.121    0.1214  271447.100   \n",
       "3 2023-01-02 15:00:00  246.8000  247.6000  246.800  247.1000    5653.787   \n",
       "4 2023-01-02 15:00:00    4.1250    4.1400    4.111    4.1140   43988.110   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  53.009345    56.771800    52.945269         0.090417  ...   \n",
       "1  61.238560    82.150627    81.754145         0.012704  ...   \n",
       "2  54.286723    75.980296    73.911589         0.087652  ...   \n",
       "3  64.061825    83.941481    81.322513         0.039338  ...   \n",
       "4  60.923762    73.832294    77.156245        -0.037848  ...   \n",
       "\n",
       "   linear_reg_angle_prev_192  macd_prev_192  macdsignal_prev_192  \\\n",
       "0                 -17.533125      -1.228202            -0.762369   \n",
       "1                   6.972738       0.013083             0.015791   \n",
       "2                  -1.046410       0.000169             0.000246   \n",
       "3                  -4.808189      -0.054825            -0.071140   \n",
       "4                   3.921538       0.006973             0.010375   \n",
       "\n",
       "   macdhist_prev_192  macd_dir_prev_192  macdsignal_dir_prev_192  \\\n",
       "0          -0.465833           0.158853                 0.157152   \n",
       "1          -0.002709          -0.140132                -0.011339   \n",
       "2          -0.000077          -0.131526                -0.036094   \n",
       "3           0.016315           0.394717                -0.071718   \n",
       "4          -0.003401          -0.121655                -0.052706   \n",
       "\n",
       "   atr_prev_192  close_smooth_prev_192  target  ttype  \n",
       "0      1.135257             144.495833       1    buy  \n",
       "1      0.034240               3.614208       1    buy  \n",
       "2      0.000844               0.116646       1    buy  \n",
       "3      0.754032             244.741667       1    buy  \n",
       "4      0.021675               3.736542       0    buy  \n",
       "\n",
       "[5 rows x 985 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4776, 985)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 192\n",
    "\n",
    "if CFG.ttype == 'both':\n",
    "    df_buy = pd.read_pickle(f'signal_stat/train_buy_{last}.pkl')\n",
    "    df_sell = pd.read_pickle(f'signal_stat/train_sell_{last}.pkl')\n",
    "    df = pd.concat([df_buy, df_sell])\n",
    "elif CFG.ttype == 'buy':\n",
    "    df = pd.read_pickle(f'signal_stat/train_buy_{last}.pkl')\n",
    "else:\n",
    "    df = pd.read_pickle(f'signal_stat/train_sell_{last}.pkl')\n",
    "\n",
    "\n",
    "df = df[df['pattern'].isin(CFG.patterns_to_filter)]\n",
    "\n",
    "display(df.head())\n",
    "display(df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split by ticker group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.1, n_splits=2, random_state = 7)\n",
    "\n",
    "split = splitter.split(df, groups=df['ticker'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_df = df.iloc[train_inds]\n",
    "test_df = df.iloc[test_inds]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_size=0.2\n",
    "\n",
    "x_data = train_df.drop(['target', 'time', 'ticker', 'pattern', 'ttype'], axis=1)\n",
    "y_data = train_df['target']\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n",
    "input_channels = x_data.shape[1]\n",
    "output_size = len(set(y_data))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train[x_train.columns] = scaler.fit_transform(x_train)\n",
    "x_valid[x_valid.columns] = scaler.transform(x_valid)\n",
    "\n",
    "x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "x_valid = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_train), type(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(SigModel, self, ).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(input_channels, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "        self.layers.add_module('sigmoid', torch.nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train).squeeze()\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train)\n",
    "    train_acc = (train_preds.round() == y_train).float().mean()\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad() # ~ model.zero_grad()\n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid).squeeze()\n",
    "        val_loss = criterion(val_preds, y_valid)\n",
    "        val_acc = (val_preds.round() == y_valid).float().mean()\n",
    "    \n",
    "    # update weights according to gradient value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# Initialize model\n",
    "if CFG.train_NN:\n",
    "    model = SigModel(input_channels).to(device)\n",
    "\n",
    "    # Number of epochs\n",
    "    epochs = 100000\n",
    "\n",
    "    # Send data to the device\n",
    "    x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "    y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "    train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    # Empty loss lists to track values\n",
    "    epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    learning_rate = 1e-6\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2000, threshold=1e-2)\n",
    "\n",
    "    # Loop through the data\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc, val_loss, val_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "        # Print progress a total of 20 times\n",
    "        if epoch % int(epochs / 20) == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.5f}\\\n",
    "                Validation Loss: {val_loss:.5f}, Val Acc: {val_acc:.5f}\\\n",
    "                    LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]}')\n",
    "\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(train_loss.cpu().detach().numpy())\n",
    "            valid_loss_values.append(val_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NN train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CFG.train_NN:\n",
    "    plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "    plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss Curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern', 'ttype']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta and boruta_df_.shape[0] > 0:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.02,\n",
    "        #   'early_stopping_round': 100,\n",
    "          'max_depth': 10,\n",
    "          'subsample' : 0.7,\n",
    "          'colsample_bytree': 0.85,\n",
    "          'num_leaves': 24,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain',\n",
    "          'max_bin': 255,\n",
    "          'reg_alpha': 1e-6,\n",
    "          'reg_lambda': 1e-8\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "if CFG.select_features:\n",
    "    perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(df, permut=True, boruta=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.select_features:\n",
    "    perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "    boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "    feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "    fi = pd.concat([perm_df_[['Feature','rank']], feature_importances_[['Feature','rank']], boruta_df_[['Feature','rank']]])\n",
    "    fi = fi.groupby('Feature')['rank'].sum().sort_values().reset_index()\n",
    "    fi.to_csv(f'feature_importance_{CFG.ttype}.csv')\n",
    "else:\n",
    "    fi = pd.read_csv(f'feature_importance_{CFG.ttype}.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test split based on ticker groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on full data\n",
      "[100]\ttraining's binary_logloss: 0.599278\ttraining's average_precision: 0.928744\n",
      "[200]\ttraining's binary_logloss: 0.565041\ttraining's average_precision: 0.95053\n",
      "[300]\ttraining's binary_logloss: 0.527877\ttraining's average_precision: 0.968034\n",
      "[400]\ttraining's binary_logloss: 0.495235\ttraining's average_precision: 0.978598\n",
      "[500]\ttraining's binary_logloss: 0.462248\ttraining's average_precision: 0.987016\n",
      "[600]\ttraining's binary_logloss: 0.449212\ttraining's average_precision: 0.991027\n",
      "[700]\ttraining's binary_logloss: 0.425396\ttraining's average_precision: 0.994157\n",
      "[800]\ttraining's binary_logloss: 0.409788\ttraining's average_precision: 0.995963\n",
      "[900]\ttraining's binary_logloss: 0.388909\ttraining's average_precision: 0.99734\n",
      "[1000]\ttraining's binary_logloss: 0.374579\ttraining's average_precision: 0.998102\n",
      "[1100]\ttraining's binary_logloss: 0.357302\ttraining's average_precision: 0.998832\n",
      "[1200]\ttraining's binary_logloss: 0.347744\ttraining's average_precision: 0.999136\n",
      "[1300]\ttraining's binary_logloss: 0.333922\ttraining's average_precision: 0.999391\n",
      "[1400]\ttraining's binary_logloss: 0.316775\ttraining's average_precision: 0.999613\n"
     ]
    }
   ],
   "source": [
    "def model_train(df, train_df, test_df, features, task_type, how, n_folds, low_bound, high_bound, train_test): \n",
    "    X, groups = train_df[features], train_df['ticker']\n",
    "    # X = pd.concat([X, pd.get_dummies(train_df[['ttype']], drop_first=True)], axis=1)\n",
    "    y = train_df['target']\n",
    "    \n",
    "    if train_test == 'fold':\n",
    "        oof = np.zeros([train_df['target'].shape[0], 1])\n",
    "        \n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=24082023)\n",
    "\n",
    "        oe_enc = OrdinalEncoder()\n",
    "        groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "        print(f\"Training with {len(features)} features\")\n",
    "        \n",
    "        for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            print(f'Fold #{fold + 1}')\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[fit_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[fit_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "            \n",
    "            models = list()\n",
    "            if how == 'lgbm':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                          eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "            elif how == 'lreg':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            prec_score, prec_obj_pct = confident_score(y_val, val_preds[:,1], low_bound, high_bound)\n",
    "            print(f'Logloss: {val_score}, Confident objects precision: {prec_score}, % of confident objects: {prec_obj_pct}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "\n",
    "        return oof, model\n",
    "    elif train_test == 'full':\n",
    "        print('Train on full data')\n",
    "        X, y = df[features], df['target']\n",
    "        # X = pd.concat([X, pd.get_dummies(df[['ttype']], drop_first=True)], axis=1)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X, y, eval_set=[(X, y)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        return np.zeros([df.shape[0], 1]), model\n",
    "    else:\n",
    "        # fit model on full dataset and predict on test\n",
    "        print(\"Test fold\")\n",
    "        X_test, y_test = test_df[features], test_df['target']\n",
    "        # X_test = pd.concat([X_test, pd.get_dummies(test_df[['ttype']], drop_first=True)], axis=1)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X, y, eval_set=[(X, y), (X_test, y_test)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        oof_test = np.zeros([test_df['target'].shape[0], 1])\n",
    "        oof_test[:,0] = model.predict_proba(X_test)[:,1]\n",
    "        return oof_test, model\n",
    "\n",
    "def prepare_features(fi, feature_num):\n",
    "    ''' Get features, sort them by their time appearance and return for using in train and inference'''\n",
    "    fi = fi['Feature'].head(feature_num)\n",
    "    feature_dict = defaultdict(list)\n",
    "    features = list()\n",
    "\n",
    "    for f in fi:\n",
    "        period = f.split('_')\n",
    "        if period[-1].isdigit():\n",
    "            feature_dict[int(period[-1])].append('_'.join(period[:-2]))\n",
    "        else:\n",
    "            feature_dict[0].append(f)\n",
    "\n",
    "    feature_dict = dict(sorted(feature_dict.items()))\n",
    "    \n",
    "    for item in feature_dict.items():\n",
    "        if item[0] > 0:\n",
    "            features.extend([i + f'_prev_{item[0]}' for i in item[1]])\n",
    "        else:\n",
    "            features.extend([i for i in item[1]])\n",
    "\n",
    "    feature_dict['features'] = features # + ['Pattern_Trend', 'STOCH_RSI']\n",
    "\n",
    "    return features, feature_dict\n",
    "\n",
    "def confident_score(y, oof, low_bound, high_bound):\n",
    "    ''' Consider only high confident objects for accuracy and precision scores calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    # pred_conf_acc = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "    pred_conf_prec = pred_conf[(oof > high_bound)]\n",
    "    # y_conf_acc = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "    y_conf_prec = y.values.reshape(-1,1)[(oof > high_bound)]\n",
    "\n",
    "    return precision_score(y_conf_prec, pred_conf_prec), y_conf_prec.shape[0]/y.shape[0]\n",
    "\n",
    "train_test = 'full' # fold, test, full, inference\n",
    "feature_num = 160\n",
    "\n",
    "\n",
    "# if CFG.ttype == 'buy':\n",
    "#     low_bound, high_bound = 0.35, 0.65\n",
    "#     params = {\n",
    "#         'boosting_type': 'dart',\n",
    "#         'n_estimators': 1200,\n",
    "#         'learning_rate': 0.02,\n",
    "#         # 'early_stopping_round': 50,\n",
    "#         'max_depth': 10,\n",
    "#         'colsample_bytree': 0.75,\n",
    "#         'subsample': 0.8,\n",
    "#         'subsample_freq': 1,\n",
    "#         'num_leaves': 25,\n",
    "#         'verbosity': -1,\n",
    "#         'max_bin': 127,\n",
    "#         'reg_alpha': 1e-6,\n",
    "#         'reg_lambda': 1e-8,\n",
    "#         'objective': 'binary',\n",
    "#         # 'is_unbalance': True,\n",
    "#         # 'class_weight': 'balanced',\n",
    "#         'metric': 'average_precision'\n",
    "#         }\n",
    "# else:\n",
    "low_bound, high_bound = 0.35, 0.65\n",
    "params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'n_estimators': 1400,\n",
    "    'learning_rate': 0.02,\n",
    "    # 'early_stopping_round': 50,\n",
    "    'max_depth': 10,\n",
    "    'colsample_bytree': 0.75,\n",
    "    'subsample': 0.85,\n",
    "    'subsample_freq': 1,\n",
    "    'num_leaves': 25,\n",
    "    'verbosity': -1,\n",
    "    'max_bin': 255,\n",
    "    'reg_alpha': 1e-6,\n",
    "    'reg_lambda': 1e-8,\n",
    "    'objective': 'binary',\n",
    "    # 'is_unbalance': True,\n",
    "    # 'class_weight': 'balanced',\n",
    "    'metric': 'average_precision'\n",
    "    }\n",
    "\n",
    "if CFG.train_LGBM:\n",
    "    fi = pd.read_csv(f'feature_importance_{CFG.ttype}.csv')\n",
    "    features, feature_dict = prepare_features(fi, feature_num)\n",
    "    if train_test != 'inference':\n",
    "        oof, model = model_train(df, train_df, test_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound, train_test=train_test)\n",
    "\n",
    "    if train_test == 'fold':\n",
    "        y = train_df['target']\n",
    "        oof_val_score = log_loss(y, oof)\n",
    "        oof_conf_prec_score, oof_conf_obj_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "        print(f'Total fold Logloss: {oof_val_score}, Total confident objects precision: {oof_conf_prec_score}, Total % of confident objects: {oof_conf_obj_pct}')\n",
    "    elif train_test == 'test':\n",
    "        y_test = test_df['target']\n",
    "        test_val_score = log_loss(y_test, oof)\n",
    "        test_conf_prec_score, test_conf_obj_pct = confident_score(y_test, oof, low_bound, high_bound)\n",
    "        print(f'Total test Logloss: {test_val_score}, Total test confident objects precision: {test_conf_prec_score}, Total % of test confident objects: {test_conf_obj_pct}')\n",
    "        # save feature dictionary for further inference\n",
    "        joblib.dump(model, f'lgbm_{CFG.ttype}.pkl')\n",
    "    elif train_test == 'full':\n",
    "        joblib.dump(model, f'lgbm_{CFG.ttype}.pkl')\n",
    "        # save feature dictionary for further inference\n",
    "        with open(f'features_{CFG.ttype}.json', 'w') as f:\n",
    "            json.dump(feature_dict, f)\n",
    "    elif train_test == 'inference':\n",
    "        model = joblib.load(f'lgbm_{CFG.ttype}.pkl')\n",
    "        X_test, y_test = test_df[features], test_df['target']\n",
    "        # X_test = pd.concat([X_test, pd.get_dummies(test_df[['ttype']], drop_first=True)], axis=1)\n",
    "        oof = np.nan_to_num(model.predict_proba(X_test)[:,1])\n",
    "        test_val_score = log_loss(y_test, oof)\n",
    "        test_conf_prec_score, test_conf_obj_pct = confident_score(y_test, oof, low_bound, high_bound)\n",
    "        print(f'Loaded model test Logloss: {test_val_score}, Loaded model test confident objects precision: {test_conf_prec_score}, Total % of test confident objects: {test_conf_obj_pct}')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total fold Logloss: 0.6034148508680187, Total confident objects precision: 0.7413333333333333, Total % of confident objects: 0.4390072582533365\n",
    "\n",
    "Total test Logloss: 0.6107962529031711, Total test confident objects precision: 0.7263681592039801, Total % of test confident objects: 0.39801980198019804"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count predictions according to pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2529\n",
       "0    1742\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pattern</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MACD</th>\n",
       "      <td>0.439007</td>\n",
       "      <td>4271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean  count\n",
       "pattern                 \n",
       "MACD     0.439007   4271"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_test == 'fold':\n",
    "    train_df.loc[:,'oof'] = oof >= high_bound\n",
    "    display(train_df.groupby('pattern')['oof'].agg(['mean', 'count']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
