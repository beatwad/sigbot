{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import log_loss, mean_squared_error, precision_score\n",
    "\n",
    "import torch\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    ttype = 'both'\n",
    "    patterns_to_filter = ['MACD', 'STOCH_RSI']\n",
    "    select_features = False\n",
    "    train_NN = False\n",
    "    train_LGBM = True\n",
    "    n_repeats = 1\n",
    "    n_folds = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_angle_prev_192</th>\n",
       "      <th>macd_prev_192</th>\n",
       "      <th>macdsignal_prev_192</th>\n",
       "      <th>macdhist_prev_192</th>\n",
       "      <th>macd_dir_prev_192</th>\n",
       "      <th>macdsignal_dir_prev_192</th>\n",
       "      <th>atr_prev_192</th>\n",
       "      <th>close_smooth_prev_192</th>\n",
       "      <th>target</th>\n",
       "      <th>ttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-31 15:00:00</td>\n",
       "      <td>145.9000</td>\n",
       "      <td>146.1000</td>\n",
       "      <td>145.8000</td>\n",
       "      <td>145.9000</td>\n",
       "      <td>987.752</td>\n",
       "      <td>53.009345</td>\n",
       "      <td>56.771800</td>\n",
       "      <td>52.945269</td>\n",
       "      <td>0.090417</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.533125</td>\n",
       "      <td>-1.228202</td>\n",
       "      <td>-0.762369</td>\n",
       "      <td>-0.465833</td>\n",
       "      <td>0.158853</td>\n",
       "      <td>0.157152</td>\n",
       "      <td>1.135257</td>\n",
       "      <td>144.495833</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-31 19:00:00</td>\n",
       "      <td>3.6420</td>\n",
       "      <td>3.6460</td>\n",
       "      <td>3.6340</td>\n",
       "      <td>3.6350</td>\n",
       "      <td>39534.000</td>\n",
       "      <td>61.238560</td>\n",
       "      <td>82.150627</td>\n",
       "      <td>81.754145</td>\n",
       "      <td>0.012704</td>\n",
       "      <td>...</td>\n",
       "      <td>6.972738</td>\n",
       "      <td>0.013083</td>\n",
       "      <td>0.015791</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>-0.140132</td>\n",
       "      <td>-0.011339</td>\n",
       "      <td>0.034240</td>\n",
       "      <td>3.614208</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 11:00:00</td>\n",
       "      <td>0.1221</td>\n",
       "      <td>0.1221</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>271447.100</td>\n",
       "      <td>54.286723</td>\n",
       "      <td>75.980296</td>\n",
       "      <td>73.911589</td>\n",
       "      <td>0.087652</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.046410</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.131526</td>\n",
       "      <td>-0.036094</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.116646</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-02 15:00:00</td>\n",
       "      <td>0.1077</td>\n",
       "      <td>0.1081</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>325371.000</td>\n",
       "      <td>68.903445</td>\n",
       "      <td>84.714390</td>\n",
       "      <td>78.244813</td>\n",
       "      <td>0.086191</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.433600</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.520190</td>\n",
       "      <td>-0.095311</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.105829</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-02 15:00:00</td>\n",
       "      <td>246.8000</td>\n",
       "      <td>247.6000</td>\n",
       "      <td>246.8000</td>\n",
       "      <td>247.1000</td>\n",
       "      <td>5653.787</td>\n",
       "      <td>64.061825</td>\n",
       "      <td>83.941481</td>\n",
       "      <td>81.322513</td>\n",
       "      <td>0.039338</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.808189</td>\n",
       "      <td>-0.054825</td>\n",
       "      <td>-0.071140</td>\n",
       "      <td>0.016315</td>\n",
       "      <td>0.394717</td>\n",
       "      <td>-0.071718</td>\n",
       "      <td>0.754032</td>\n",
       "      <td>244.741667</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 985 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time      open      high       low     close      volume  \\\n",
       "0 2022-12-31 15:00:00  145.9000  146.1000  145.8000  145.9000     987.752   \n",
       "1 2022-12-31 19:00:00    3.6420    3.6460    3.6340    3.6350   39534.000   \n",
       "2 2023-01-01 11:00:00    0.1221    0.1221    0.1210    0.1214  271447.100   \n",
       "3 2023-01-02 15:00:00    0.1077    0.1081    0.1076    0.1080  325371.000   \n",
       "4 2023-01-02 15:00:00  246.8000  247.6000  246.8000  247.1000    5653.787   \n",
       "\n",
       "         rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "0  53.009345    56.771800    52.945269         0.090417  ...   \n",
       "1  61.238560    82.150627    81.754145         0.012704  ...   \n",
       "2  54.286723    75.980296    73.911589         0.087652  ...   \n",
       "3  68.903445    84.714390    78.244813         0.086191  ...   \n",
       "4  64.061825    83.941481    81.322513         0.039338  ...   \n",
       "\n",
       "   linear_reg_angle_prev_192  macd_prev_192  macdsignal_prev_192  \\\n",
       "0                 -17.533125      -1.228202            -0.762369   \n",
       "1                   6.972738       0.013083             0.015791   \n",
       "2                  -1.046410       0.000169             0.000246   \n",
       "3                  -7.433600       0.000007             0.000042   \n",
       "4                  -4.808189      -0.054825            -0.071140   \n",
       "\n",
       "   macdhist_prev_192  macd_dir_prev_192  macdsignal_dir_prev_192  \\\n",
       "0          -0.465833           0.158853                 0.157152   \n",
       "1          -0.002709          -0.140132                -0.011339   \n",
       "2          -0.000077          -0.131526                -0.036094   \n",
       "3          -0.000035          -0.520190                -0.095311   \n",
       "4           0.016315           0.394717                -0.071718   \n",
       "\n",
       "   atr_prev_192  close_smooth_prev_192  target  ttype  \n",
       "0      1.135257             144.495833       1    buy  \n",
       "1      0.034240               3.614208       1    buy  \n",
       "2      0.000844               0.116646       1    buy  \n",
       "3      0.000385               0.105829       1    buy  \n",
       "4      0.754032             244.741667       1    buy  \n",
       "\n",
       "[5 rows x 985 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5112, 985)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 192\n",
    "\n",
    "if CFG.ttype == 'both':\n",
    "    df_buy = pd.read_pickle(f'signal_stat/train_buy_{last}.pkl')\n",
    "    df_sell = pd.read_pickle(f'signal_stat/train_sell_{last}.pkl')\n",
    "    df = pd.concat([df_buy, df_sell])\n",
    "elif CFG.ttype == 'buy':\n",
    "    df = pd.read_pickle(f'signal_stat/train_buy_{last}.pkl')\n",
    "else:\n",
    "    df = pd.read_pickle(f'signal_stat/train_sell_{last}.pkl')\n",
    "\n",
    "\n",
    "df = df[df['pattern'].isin(CFG.patterns_to_filter)]\n",
    "\n",
    "display(df.head())\n",
    "display(df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split by ticker group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.1, n_splits=2, random_state = 7)\n",
    "\n",
    "split = splitter.split(df, groups=df['ticker'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_df = df.iloc[train_inds]\n",
    "test_df = df.iloc[test_inds]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_size=0.2\n",
    "\n",
    "x_data = train_df.drop(['target', 'time', 'ticker', 'pattern', 'ttype'], axis=1)\n",
    "y_data = train_df['target']\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n",
    "input_channels = x_data.shape[1]\n",
    "output_size = len(set(y_data))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train[x_train.columns] = scaler.fit_transform(x_train)\n",
    "x_valid[x_valid.columns] = scaler.transform(x_valid)\n",
    "\n",
    "x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "x_valid = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_train), type(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(SigModel, self, ).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(input_channels, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "        self.layers.add_module('sigmoid', torch.nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train).squeeze()\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train)\n",
    "    train_acc = (train_preds.round() == y_train).float().mean()\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad() # ~ model.zero_grad()\n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid).squeeze()\n",
    "        val_loss = criterion(val_preds, y_valid)\n",
    "        val_acc = (val_preds.round() == y_valid).float().mean()\n",
    "    \n",
    "    # update weights according to gradient value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# Initialize model\n",
    "if CFG.train_NN:\n",
    "    model = SigModel(input_channels).to(device)\n",
    "\n",
    "    # Number of epochs\n",
    "    epochs = 100000\n",
    "\n",
    "    # Send data to the device\n",
    "    x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "    y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "    train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    # Empty loss lists to track values\n",
    "    epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    learning_rate = 1e-6\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2000, threshold=1e-2)\n",
    "\n",
    "    # Loop through the data\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc, val_loss, val_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "        # Print progress a total of 20 times\n",
    "        if epoch % int(epochs / 20) == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.5f}\\\n",
    "                Validation Loss: {val_loss:.5f}, Val Acc: {val_acc:.5f}\\\n",
    "                    LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]}')\n",
    "\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(train_loss.cpu().detach().numpy())\n",
    "            valid_loss_values.append(val_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NN train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CFG.train_NN:\n",
    "    plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "    plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss Curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern', 'ttype']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta and boruta_df_.shape[0] > 0:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 1000,\n",
    "          'learning_rate': 0.02,\n",
    "        #   'early_stopping_round': 100,\n",
    "          'max_depth': 10,\n",
    "          'subsample' : 0.7,\n",
    "          'colsample_bytree': 0.85,\n",
    "          'num_leaves': 24,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain',\n",
    "          'max_bin': 255,\n",
    "          'reg_alpha': 1e-6,\n",
    "          'reg_lambda': 1e-8\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "if CFG.select_features:\n",
    "    perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(df, permut=True, boruta=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.select_features:\n",
    "    perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "    boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "    feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "    fi = pd.concat([perm_df_[['Feature','rank']], feature_importances_[['Feature','rank']], boruta_df_[['Feature','rank']]])\n",
    "    fi = fi.groupby('Feature')['rank'].sum().sort_values().reset_index()\n",
    "    fi.to_csv(f'feature_importance_{CFG.ttype}.csv')\n",
    "else:\n",
    "    fi = pd.read_csv(f'feature_importance_{CFG.ttype}.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test split based on ticker groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fold\n",
      "[100]\ttraining's binary_logloss: 0.596541\ttraining's average_precision: 0.92951\tvalid_1's binary_logloss: 0.649888\tvalid_1's average_precision: 0.757359\n",
      "[200]\ttraining's binary_logloss: 0.563582\ttraining's average_precision: 0.949461\tvalid_1's binary_logloss: 0.638055\tvalid_1's average_precision: 0.759786\n",
      "[300]\ttraining's binary_logloss: 0.527839\ttraining's average_precision: 0.966231\tvalid_1's binary_logloss: 0.628427\tvalid_1's average_precision: 0.774536\n",
      "[400]\ttraining's binary_logloss: 0.495276\ttraining's average_precision: 0.978163\tvalid_1's binary_logloss: 0.623878\tvalid_1's average_precision: 0.778984\n",
      "[500]\ttraining's binary_logloss: 0.46309\ttraining's average_precision: 0.987827\tvalid_1's binary_logloss: 0.618802\tvalid_1's average_precision: 0.776593\n",
      "[600]\ttraining's binary_logloss: 0.449083\ttraining's average_precision: 0.991849\tvalid_1's binary_logloss: 0.617116\tvalid_1's average_precision: 0.776545\n",
      "[700]\ttraining's binary_logloss: 0.425576\ttraining's average_precision: 0.994587\tvalid_1's binary_logloss: 0.61457\tvalid_1's average_precision: 0.774885\n",
      "[800]\ttraining's binary_logloss: 0.409596\ttraining's average_precision: 0.996183\tvalid_1's binary_logloss: 0.61061\tvalid_1's average_precision: 0.782728\n",
      "[900]\ttraining's binary_logloss: 0.389053\ttraining's average_precision: 0.997548\tvalid_1's binary_logloss: 0.606846\tvalid_1's average_precision: 0.78254\n",
      "[1000]\ttraining's binary_logloss: 0.374095\ttraining's average_precision: 0.998344\tvalid_1's binary_logloss: 0.604597\tvalid_1's average_precision: 0.783821\n",
      "[1100]\ttraining's binary_logloss: 0.356681\ttraining's average_precision: 0.998996\tvalid_1's binary_logloss: 0.603867\tvalid_1's average_precision: 0.780464\n",
      "[1200]\ttraining's binary_logloss: 0.346486\ttraining's average_precision: 0.999332\tvalid_1's binary_logloss: 0.601069\tvalid_1's average_precision: 0.785275\n",
      "[1300]\ttraining's binary_logloss: 0.332868\ttraining's average_precision: 0.999547\tvalid_1's binary_logloss: 0.600937\tvalid_1's average_precision: 0.784429\n",
      "[1400]\ttraining's binary_logloss: 0.315284\ttraining's average_precision: 0.999739\tvalid_1's binary_logloss: 0.599959\tvalid_1's average_precision: 0.784233\n",
      "[1500]\ttraining's binary_logloss: 0.307385\ttraining's average_precision: 0.99986\tvalid_1's binary_logloss: 0.597131\tvalid_1's average_precision: 0.789351\n",
      "Total test Logloss: 0.5971307830306146, Total test confident objects precision: 0.7631578947368421, Total % of test confident objects: 0.37924151696606784\n"
     ]
    }
   ],
   "source": [
    "def model_train(df, train_df, test_df, features, task_type, how, n_folds, low_bound, high_bound, train_test): \n",
    "    X, groups = train_df[features], train_df['ticker']\n",
    "    # X = pd.concat([X, pd.get_dummies(train_df[['ttype']], drop_first=True)], axis=1)\n",
    "    y = train_df['target']\n",
    "    \n",
    "    if train_test == 'fold':\n",
    "        oof = np.zeros([train_df['target'].shape[0], 1])\n",
    "        \n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=24082023)\n",
    "\n",
    "        oe_enc = OrdinalEncoder()\n",
    "        groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "        print(f\"Training with {len(features)} features\")\n",
    "        \n",
    "        for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            print(f'Fold #{fold + 1}')\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[fit_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[fit_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "            \n",
    "            models = list()\n",
    "            if how == 'lgbm':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                          eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "            elif how == 'lreg':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            prec_score, prec_obj_pct = confident_score(y_val, val_preds[:,1], low_bound, high_bound)\n",
    "            print(f'Logloss: {val_score}, Confident objects precision: {prec_score}, % of confident objects: {prec_obj_pct}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "\n",
    "        return oof, model\n",
    "    elif train_test == 'full':\n",
    "        print('Train on full data')\n",
    "        X, y = df[features], df['target']\n",
    "        # X = pd.concat([X, pd.get_dummies(df[['ttype']], drop_first=True)], axis=1)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X, y, eval_set=[(X, y)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        return np.zeros([df.shape[0], 1]), model\n",
    "    else:\n",
    "        # fit model on full dataset and predict on test\n",
    "        print(\"Test fold\")\n",
    "        X_test, y_test = test_df[features], test_df['target']\n",
    "        # X_test = pd.concat([X_test, pd.get_dummies(test_df[['ttype']], drop_first=True)], axis=1)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X, y, eval_set=[(X, y), (X_test, y_test)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        oof_test = np.zeros([test_df['target'].shape[0], 1])\n",
    "        oof_test[:,0] = model.predict_proba(X_test)[:,1]\n",
    "        return oof_test, model\n",
    "\n",
    "def prepare_features(fi, feature_num):\n",
    "    ''' Get features, sort them by their time appearance and return for using in train and inference'''\n",
    "    fi = fi['Feature'].head(feature_num)\n",
    "    feature_dict = defaultdict(list)\n",
    "    features = list()\n",
    "\n",
    "    for f in fi:\n",
    "        period = f.split('_')\n",
    "        if period[-1].isdigit():\n",
    "            feature_dict[int(period[-1])].append('_'.join(period[:-2]))\n",
    "        else:\n",
    "            feature_dict[0].append(f)\n",
    "\n",
    "    feature_dict = dict(sorted(feature_dict.items()))\n",
    "    \n",
    "    for item in feature_dict.items():\n",
    "        if item[0] > 0:\n",
    "            features.extend([i + f'_prev_{item[0]}' for i in item[1]])\n",
    "        else:\n",
    "            features.extend([i for i in item[1]])\n",
    "\n",
    "    feature_dict['features'] = features # + ['Pattern_Trend', 'STOCH_RSI']\n",
    "\n",
    "    return features, feature_dict\n",
    "\n",
    "def confident_score(y, oof, low_bound, high_bound):\n",
    "    ''' Consider only high confident objects for accuracy and precision scores calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    # pred_conf_acc = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "    pred_conf_prec = pred_conf[(oof > high_bound)]\n",
    "    # y_conf_acc = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "    y_conf_prec = y.values.reshape(-1,1)[(oof > high_bound)]\n",
    "\n",
    "    return precision_score(y_conf_prec, pred_conf_prec), y_conf_prec.shape[0]/y.shape[0]\n",
    "\n",
    "train_test = 'test' # fold, test, full, inference\n",
    "feature_num = 160\n",
    "\n",
    "\n",
    "# if CFG.ttype == 'buy':\n",
    "#     low_bound, high_bound = 0.35, 0.65\n",
    "#     params = {\n",
    "#         'boosting_type': 'dart',\n",
    "#         'n_estimators': 1200,\n",
    "#         'learning_rate': 0.02,\n",
    "#         # 'early_stopping_round': 50,\n",
    "#         'max_depth': 10,\n",
    "#         'colsample_bytree': 0.75,\n",
    "#         'subsample': 0.8,\n",
    "#         'subsample_freq': 1,\n",
    "#         'num_leaves': 25,\n",
    "#         'verbosity': -1,\n",
    "#         'max_bin': 127,\n",
    "#         'reg_alpha': 1e-6,\n",
    "#         'reg_lambda': 1e-8,\n",
    "#         'objective': 'binary',\n",
    "#         # 'is_unbalance': True,\n",
    "#         # 'class_weight': 'balanced',\n",
    "#         'metric': 'average_precision'\n",
    "#         }\n",
    "# else:\n",
    "low_bound, high_bound = 0.34, 0.66\n",
    "params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'n_estimators': 1500,\n",
    "    'learning_rate': 0.02,\n",
    "    # 'early_stopping_round': 50,\n",
    "    'max_depth': 10,\n",
    "    'colsample_bytree': 0.75,\n",
    "    'subsample': 0.85,\n",
    "    'subsample_freq': 1,\n",
    "    'num_leaves': 25,\n",
    "    'verbosity': -1,\n",
    "    'max_bin': 255,\n",
    "    'reg_alpha': 1e-6,\n",
    "    'reg_lambda': 1e-8,\n",
    "    'objective': 'binary',\n",
    "    # 'is_unbalance': True,\n",
    "    # 'class_weight': 'balanced',\n",
    "    'metric': 'average_precision'\n",
    "    }\n",
    "\n",
    "if CFG.train_LGBM:\n",
    "    fi = pd.read_csv(f'feature_importance_{CFG.ttype}.csv')\n",
    "    features, feature_dict = prepare_features(fi, feature_num)\n",
    "    if train_test != 'inference':\n",
    "        oof, model = model_train(df, train_df, test_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound, train_test=train_test)\n",
    "\n",
    "    if train_test == 'fold':\n",
    "        y = train_df['target']\n",
    "        oof_val_score = log_loss(y, oof)\n",
    "        oof_conf_prec_score, oof_conf_obj_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "        print(f'Total fold Logloss: {oof_val_score}, Total confident objects precision: {oof_conf_prec_score}, Total % of confident objects: {oof_conf_obj_pct}')\n",
    "    elif train_test == 'test':\n",
    "        y_test = test_df['target']\n",
    "        test_val_score = log_loss(y_test, oof)\n",
    "        test_conf_prec_score, test_conf_obj_pct = confident_score(y_test, oof, low_bound, high_bound)\n",
    "        print(f'Total test Logloss: {test_val_score}, Total test confident objects precision: {test_conf_prec_score}, Total % of test confident objects: {test_conf_obj_pct}')\n",
    "        # save feature dictionary for further inference\n",
    "        joblib.dump(model, f'lgbm.pkl')\n",
    "    elif train_test == 'full':\n",
    "        joblib.dump(model, f'lgbm.pkl')\n",
    "        # save feature dictionary for further inference\n",
    "        with open(f'features_{CFG.ttype}.json', 'w') as f:\n",
    "            json.dump(feature_dict, f)\n",
    "    elif train_test == 'inference':\n",
    "        model = joblib.load(f'lgbm.pkl')\n",
    "        X_test, y_test = test_df[features], test_df['target']\n",
    "        # X_test = pd.concat([X_test, pd.get_dummies(test_df[['ttype']], drop_first=True)], axis=1)\n",
    "        oof = np.nan_to_num(model.predict_proba(X_test)[:,1])\n",
    "        test_val_score = log_loss(y_test, oof)\n",
    "        test_conf_prec_score, test_conf_obj_pct = confident_score(y_test, oof, low_bound, high_bound)\n",
    "        print(f'Loaded model test Logloss: {test_val_score}, Loaded model test confident objects precision: {test_conf_prec_score}, Total % of test confident objects: {test_conf_obj_pct}')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total fold Logloss: 0.6136672327432618, Total confident objects precision: 0.7402723735408561, Total % of confident objects: 0.44589026241596186\n",
    "\n",
    "Total test Logloss: 0.5971307830306146, Total test confident objects precision: 0.7631578947368421, Total % of test confident objects: 0.37924151696606784"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count predictions according to pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2787\n",
       "0    1824\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_test == 'fold':\n",
    "    train_df.loc[:,'oof'] = oof >= high_bound\n",
    "    display(train_df.groupby('pattern')['oof'].agg(['mean', 'count']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
