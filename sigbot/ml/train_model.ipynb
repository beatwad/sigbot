{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import log_loss, mean_squared_error, precision_score\n",
    "\n",
    "import torch\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    select_features = False\n",
    "    train_NN = False\n",
    "    train_LGBM = True\n",
    "    n_repeats = 1\n",
    "    n_folds = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_angle_prev_96</th>\n",
       "      <th>macd_prev_96</th>\n",
       "      <th>macdsignal_prev_96</th>\n",
       "      <th>macdhist_prev_96</th>\n",
       "      <th>macd_dir_prev_96</th>\n",
       "      <th>macdsignal_dir_prev_96</th>\n",
       "      <th>atr_prev_96</th>\n",
       "      <th>close_smooth_prev_96</th>\n",
       "      <th>target</th>\n",
       "      <th>ttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-18 23:00:00</td>\n",
       "      <td>23.4600</td>\n",
       "      <td>23.7000</td>\n",
       "      <td>23.4400</td>\n",
       "      <td>23.6200</td>\n",
       "      <td>3346.24</td>\n",
       "      <td>43.727673</td>\n",
       "      <td>25.327885</td>\n",
       "      <td>24.045347</td>\n",
       "      <td>-0.007431</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.562901</td>\n",
       "      <td>0.035983</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>-0.071544</td>\n",
       "      <td>-0.288045</td>\n",
       "      <td>-0.100355</td>\n",
       "      <td>0.188287</td>\n",
       "      <td>23.758333</td>\n",
       "      <td>1</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-26 01:00:00</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>23.7600</td>\n",
       "      <td>23.8400</td>\n",
       "      <td>13772.65</td>\n",
       "      <td>64.054892</td>\n",
       "      <td>83.517327</td>\n",
       "      <td>87.090858</td>\n",
       "      <td>-0.029809</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.369665</td>\n",
       "      <td>-0.020052</td>\n",
       "      <td>-0.052992</td>\n",
       "      <td>0.032940</td>\n",
       "      <td>-0.201703</td>\n",
       "      <td>-0.103966</td>\n",
       "      <td>0.133742</td>\n",
       "      <td>22.383333</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-11-06 11:00:00</td>\n",
       "      <td>23.1300</td>\n",
       "      <td>23.6400</td>\n",
       "      <td>22.8200</td>\n",
       "      <td>23.4700</td>\n",
       "      <td>195485.06</td>\n",
       "      <td>33.589815</td>\n",
       "      <td>17.450236</td>\n",
       "      <td>12.459531</td>\n",
       "      <td>0.221789</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.240717</td>\n",
       "      <td>-0.018542</td>\n",
       "      <td>-0.006710</td>\n",
       "      <td>-0.011832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075655</td>\n",
       "      <td>0.200640</td>\n",
       "      <td>25.909167</td>\n",
       "      <td>1</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-11-08 06:00:00</td>\n",
       "      <td>19.2700</td>\n",
       "      <td>19.5500</td>\n",
       "      <td>17.0400</td>\n",
       "      <td>17.9400</td>\n",
       "      <td>2324262.07</td>\n",
       "      <td>16.426697</td>\n",
       "      <td>21.681996</td>\n",
       "      <td>20.663002</td>\n",
       "      <td>0.096723</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.781879</td>\n",
       "      <td>-0.209272</td>\n",
       "      <td>-0.227469</td>\n",
       "      <td>0.018198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016908</td>\n",
       "      <td>0.208816</td>\n",
       "      <td>24.600833</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2022-12-27 07:00:00</td>\n",
       "      <td>0.3229</td>\n",
       "      <td>0.3241</td>\n",
       "      <td>0.3227</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>235279.00</td>\n",
       "      <td>34.957293</td>\n",
       "      <td>35.958102</td>\n",
       "      <td>36.687176</td>\n",
       "      <td>-0.013187</td>\n",
       "      <td>...</td>\n",
       "      <td>5.007458</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.396215</td>\n",
       "      <td>1.688159</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.307438</td>\n",
       "      <td>1</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time     open     high      low    close      volume  \\\n",
       "2  2022-10-18 23:00:00  23.4600  23.7000  23.4400  23.6200     3346.24   \n",
       "3  2022-10-26 01:00:00  24.1500  24.1500  23.7600  23.8400    13772.65   \n",
       "7  2022-11-06 11:00:00  23.1300  23.6400  22.8200  23.4700   195485.06   \n",
       "10 2022-11-08 06:00:00  19.2700  19.5500  17.0400  17.9400  2324262.07   \n",
       "27 2022-12-27 07:00:00   0.3229   0.3241   0.3227   0.3232   235279.00   \n",
       "\n",
       "          rsi  stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  \\\n",
       "2   43.727673    25.327885    24.045347        -0.007431  ...   \n",
       "3   64.054892    83.517327    87.090858        -0.029809  ...   \n",
       "7   33.589815    17.450236    12.459531         0.221789  ...   \n",
       "10  16.426697    21.681996    20.663002         0.096723  ...   \n",
       "27  34.957293    35.958102    36.687176        -0.013187  ...   \n",
       "\n",
       "    linear_reg_angle_prev_96  macd_prev_96  macdsignal_prev_96  \\\n",
       "2                  -6.562901      0.035983            0.107527   \n",
       "3                 -10.369665     -0.020052           -0.052992   \n",
       "7                  -4.240717     -0.018542           -0.006710   \n",
       "10                -10.781879     -0.209272           -0.227469   \n",
       "27                  5.007458      0.001522            0.000284   \n",
       "\n",
       "    macdhist_prev_96  macd_dir_prev_96  macdsignal_dir_prev_96  atr_prev_96  \\\n",
       "2          -0.071544         -0.288045               -0.100355     0.188287   \n",
       "3           0.032940         -0.201703               -0.103966     0.133742   \n",
       "7          -0.011832          0.000000                0.075655     0.200640   \n",
       "10          0.018198          0.000000               -0.016908     0.208816   \n",
       "27          0.001238          0.396215                1.688159     0.002535   \n",
       "\n",
       "    close_smooth_prev_96  target  ttype  \n",
       "2              23.758333       1   sell  \n",
       "3              22.383333       1    buy  \n",
       "7              25.909167       1   sell  \n",
       "10             24.600833       0   sell  \n",
       "27              0.307438       1   sell  \n",
       "\n",
       "[5 rows x 505 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(7154, 505)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 96\n",
    "\n",
    "df = pd.read_pickle(f'signal_stat/train_df_{last}.pkl')\n",
    "\n",
    "df = df[df['pattern'].isin(['MACD', 'STOCH_RSI'])]\n",
    "\n",
    "display(df.head())\n",
    "display(df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split by ticker group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.1, n_splits=2, random_state = 7)\n",
    "\n",
    "split = splitter.split(df, groups=df['ticker'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_df = df.iloc[train_inds]\n",
    "test_df = df.iloc[test_inds]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_size=0.2\n",
    "\n",
    "x_data = train_df.drop(['target', 'time', 'ticker', 'pattern', 'ttype'], axis=1)\n",
    "y_data = train_df['target']\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n",
    "input_channels = x_data.shape[1]\n",
    "output_size = len(set(y_data))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train[x_train.columns] = scaler.fit_transform(x_train)\n",
    "x_valid[x_valid.columns] = scaler.transform(x_valid)\n",
    "\n",
    "x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "x_valid = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_train), type(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(SigModel, self, ).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(input_channels, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "        self.layers.add_module('sigmoid', torch.nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train).squeeze()\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train)\n",
    "    train_acc = (train_preds.round() == y_train).float().mean()\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad() # ~ model.zero_grad()\n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid).squeeze()\n",
    "        val_loss = criterion(val_preds, y_valid)\n",
    "        val_acc = (val_preds.round() == y_valid).float().mean()\n",
    "    \n",
    "    # update weights according to gradient value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# Initialize model\n",
    "if CFG.train_NN:\n",
    "    model = SigModel(input_channels).to(device)\n",
    "\n",
    "    # Number of epochs\n",
    "    epochs = 100000\n",
    "\n",
    "    # Send data to the device\n",
    "    x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "    y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "    train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    # Empty loss lists to track values\n",
    "    epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    learning_rate = 1e-6\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2000, threshold=1e-2)\n",
    "\n",
    "    # Loop through the data\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc, val_loss, val_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "        # Print progress a total of 20 times\n",
    "        if epoch % int(epochs / 20) == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.5f}\\\n",
    "                Validation Loss: {val_loss:.5f}, Val Acc: {val_acc:.5f}\\\n",
    "                    LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]}')\n",
    "\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(train_loss.cpu().detach().numpy())\n",
    "            valid_loss_values.append(val_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NN train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CFG.train_NN:\n",
    "    plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "    plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss Curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern', 'ttype']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 2000,\n",
    "          'learning_rate': 0.02,\n",
    "        #   'early_stopping_round': 100,\n",
    "          'max_depth': 10,\n",
    "          'subsample' : 0.7,\n",
    "          'colsample_bytree': 0.85,\n",
    "          'num_leaves': 24,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain',\n",
    "          'max_bin': 255,\n",
    "          'reg_alpha': 1e-6,\n",
    "          'reg_lambda': 1e-8\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "if CFG.select_features:\n",
    "    perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(df, permut=True, boruta=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.select_features:\n",
    "    perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "    boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "    feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "    fi = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "    fi = fi.groupby('Feature')['rank'].sum().sort_values().reset_index()\n",
    "    fi.to_csv('feature_importance.csv')\n",
    "else:\n",
    "    fi = pd.read_csv('feature_importance.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test split based on ticker groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 160 features\n",
      "Fold #1\n",
      "[100]\tvalid_0's binary_logloss: 0.671201\tvalid_0's average_precision: 0.634788\n",
      "[200]\tvalid_0's binary_logloss: 0.664086\tvalid_0's average_precision: 0.646405\n",
      "[300]\tvalid_0's binary_logloss: 0.661295\tvalid_0's average_precision: 0.650736\n",
      "[400]\tvalid_0's binary_logloss: 0.656768\tvalid_0's average_precision: 0.659428\n",
      "[500]\tvalid_0's binary_logloss: 0.655518\tvalid_0's average_precision: 0.6614\n",
      "[600]\tvalid_0's binary_logloss: 0.656294\tvalid_0's average_precision: 0.661311\n",
      "[700]\tvalid_0's binary_logloss: 0.654626\tvalid_0's average_precision: 0.659192\n",
      "[800]\tvalid_0's binary_logloss: 0.653843\tvalid_0's average_precision: 0.658755\n",
      "[900]\tvalid_0's binary_logloss: 0.651615\tvalid_0's average_precision: 0.662891\n",
      "[1000]\tvalid_0's binary_logloss: 0.650495\tvalid_0's average_precision: 0.666382\n",
      "[1100]\tvalid_0's binary_logloss: 0.651148\tvalid_0's average_precision: 0.663738\n",
      "[1200]\tvalid_0's binary_logloss: 0.650246\tvalid_0's average_precision: 0.666324\n",
      "[1300]\tvalid_0's binary_logloss: 0.650212\tvalid_0's average_precision: 0.665684\n",
      "[1400]\tvalid_0's binary_logloss: 0.649875\tvalid_0's average_precision: 0.668096\n",
      "[1500]\tvalid_0's binary_logloss: 0.650154\tvalid_0's average_precision: 0.668793\n",
      "[1600]\tvalid_0's binary_logloss: 0.649302\tvalid_0's average_precision: 0.672632\n",
      "[1700]\tvalid_0's binary_logloss: 0.649372\tvalid_0's average_precision: 0.668082\n",
      "[1800]\tvalid_0's binary_logloss: 0.647911\tvalid_0's average_precision: 0.672361\n",
      "[1900]\tvalid_0's binary_logloss: 0.648579\tvalid_0's average_precision: 0.67484\n",
      "[2000]\tvalid_0's binary_logloss: 0.647798\tvalid_0's average_precision: 0.675315\n",
      "[2100]\tvalid_0's binary_logloss: 0.648435\tvalid_0's average_precision: 0.674385\n",
      "[2200]\tvalid_0's binary_logloss: 0.648709\tvalid_0's average_precision: 0.672746\n",
      "Logloss: 0.6487091442093875, Confident objects precision: 0.6928327645051194, % of confident objects: 0.2738317757009346\n",
      "Fold #2\n",
      "[100]\tvalid_0's binary_logloss: 0.66335\tvalid_0's average_precision: 0.678646\n",
      "[200]\tvalid_0's binary_logloss: 0.658086\tvalid_0's average_precision: 0.68143\n",
      "[300]\tvalid_0's binary_logloss: 0.655729\tvalid_0's average_precision: 0.68127\n",
      "[400]\tvalid_0's binary_logloss: 0.65284\tvalid_0's average_precision: 0.685939\n",
      "[500]\tvalid_0's binary_logloss: 0.651302\tvalid_0's average_precision: 0.689001\n",
      "[600]\tvalid_0's binary_logloss: 0.649148\tvalid_0's average_precision: 0.698301\n",
      "[700]\tvalid_0's binary_logloss: 0.648722\tvalid_0's average_precision: 0.696627\n",
      "[800]\tvalid_0's binary_logloss: 0.64835\tvalid_0's average_precision: 0.696369\n",
      "[900]\tvalid_0's binary_logloss: 0.64705\tvalid_0's average_precision: 0.700169\n",
      "[1000]\tvalid_0's binary_logloss: 0.646597\tvalid_0's average_precision: 0.700221\n",
      "[1100]\tvalid_0's binary_logloss: 0.644915\tvalid_0's average_precision: 0.702319\n",
      "[1200]\tvalid_0's binary_logloss: 0.644279\tvalid_0's average_precision: 0.704762\n",
      "[1300]\tvalid_0's binary_logloss: 0.64446\tvalid_0's average_precision: 0.705473\n",
      "[1400]\tvalid_0's binary_logloss: 0.645327\tvalid_0's average_precision: 0.704048\n",
      "[1500]\tvalid_0's binary_logloss: 0.644561\tvalid_0's average_precision: 0.704443\n",
      "[1600]\tvalid_0's binary_logloss: 0.645635\tvalid_0's average_precision: 0.703084\n",
      "[1700]\tvalid_0's binary_logloss: 0.64582\tvalid_0's average_precision: 0.704215\n",
      "[1800]\tvalid_0's binary_logloss: 0.64581\tvalid_0's average_precision: 0.703688\n",
      "[1900]\tvalid_0's binary_logloss: 0.645968\tvalid_0's average_precision: 0.705604\n",
      "[2000]\tvalid_0's binary_logloss: 0.646047\tvalid_0's average_precision: 0.706375\n",
      "[2100]\tvalid_0's binary_logloss: 0.647529\tvalid_0's average_precision: 0.704262\n",
      "[2200]\tvalid_0's binary_logloss: 0.647359\tvalid_0's average_precision: 0.705612\n",
      "Logloss: 0.6473594088618382, Confident objects precision: 0.7338501291989664, % of confident objects: 0.3146341463414634\n",
      "Fold #3\n",
      "[100]\tvalid_0's binary_logloss: 0.659936\tvalid_0's average_precision: 0.714599\n",
      "[200]\tvalid_0's binary_logloss: 0.653095\tvalid_0's average_precision: 0.7161\n",
      "[300]\tvalid_0's binary_logloss: 0.647258\tvalid_0's average_precision: 0.720848\n",
      "[400]\tvalid_0's binary_logloss: 0.644536\tvalid_0's average_precision: 0.724632\n",
      "[500]\tvalid_0's binary_logloss: 0.642836\tvalid_0's average_precision: 0.724228\n",
      "[600]\tvalid_0's binary_logloss: 0.642831\tvalid_0's average_precision: 0.718877\n",
      "[700]\tvalid_0's binary_logloss: 0.642248\tvalid_0's average_precision: 0.714973\n",
      "[800]\tvalid_0's binary_logloss: 0.640704\tvalid_0's average_precision: 0.714633\n",
      "[900]\tvalid_0's binary_logloss: 0.640766\tvalid_0's average_precision: 0.717307\n",
      "[1000]\tvalid_0's binary_logloss: 0.640783\tvalid_0's average_precision: 0.716849\n",
      "[1100]\tvalid_0's binary_logloss: 0.63918\tvalid_0's average_precision: 0.720678\n",
      "[1200]\tvalid_0's binary_logloss: 0.640232\tvalid_0's average_precision: 0.718061\n",
      "[1300]\tvalid_0's binary_logloss: 0.640829\tvalid_0's average_precision: 0.716746\n",
      "[1400]\tvalid_0's binary_logloss: 0.641133\tvalid_0's average_precision: 0.717393\n",
      "[1500]\tvalid_0's binary_logloss: 0.641255\tvalid_0's average_precision: 0.716978\n",
      "[1600]\tvalid_0's binary_logloss: 0.642418\tvalid_0's average_precision: 0.714132\n",
      "[1700]\tvalid_0's binary_logloss: 0.642082\tvalid_0's average_precision: 0.715102\n",
      "[1800]\tvalid_0's binary_logloss: 0.643038\tvalid_0's average_precision: 0.712243\n",
      "[1900]\tvalid_0's binary_logloss: 0.643818\tvalid_0's average_precision: 0.710728\n",
      "[2000]\tvalid_0's binary_logloss: 0.644102\tvalid_0's average_precision: 0.711311\n",
      "[2100]\tvalid_0's binary_logloss: 0.645651\tvalid_0's average_precision: 0.710035\n",
      "[2200]\tvalid_0's binary_logloss: 0.646699\tvalid_0's average_precision: 0.708451\n",
      "Logloss: 0.6466986355695679, Confident objects precision: 0.7220902612826603, % of confident objects: 0.2895460797799175\n",
      "Fold #4\n",
      "[100]\tvalid_0's binary_logloss: 0.661084\tvalid_0's average_precision: 0.708742\n",
      "[200]\tvalid_0's binary_logloss: 0.654731\tvalid_0's average_precision: 0.721126\n",
      "[300]\tvalid_0's binary_logloss: 0.648\tvalid_0's average_precision: 0.725538\n",
      "[400]\tvalid_0's binary_logloss: 0.644858\tvalid_0's average_precision: 0.724362\n",
      "[500]\tvalid_0's binary_logloss: 0.641504\tvalid_0's average_precision: 0.724917\n",
      "[600]\tvalid_0's binary_logloss: 0.641296\tvalid_0's average_precision: 0.724795\n",
      "[700]\tvalid_0's binary_logloss: 0.64197\tvalid_0's average_precision: 0.7229\n",
      "[800]\tvalid_0's binary_logloss: 0.64131\tvalid_0's average_precision: 0.723647\n",
      "[900]\tvalid_0's binary_logloss: 0.638927\tvalid_0's average_precision: 0.726872\n",
      "[1000]\tvalid_0's binary_logloss: 0.637455\tvalid_0's average_precision: 0.727021\n",
      "[1100]\tvalid_0's binary_logloss: 0.635521\tvalid_0's average_precision: 0.728812\n",
      "[1200]\tvalid_0's binary_logloss: 0.634462\tvalid_0's average_precision: 0.728966\n",
      "[1300]\tvalid_0's binary_logloss: 0.634046\tvalid_0's average_precision: 0.72925\n",
      "[1400]\tvalid_0's binary_logloss: 0.634255\tvalid_0's average_precision: 0.728109\n",
      "[1500]\tvalid_0's binary_logloss: 0.633921\tvalid_0's average_precision: 0.727932\n",
      "[1600]\tvalid_0's binary_logloss: 0.633379\tvalid_0's average_precision: 0.730083\n",
      "[1700]\tvalid_0's binary_logloss: 0.632377\tvalid_0's average_precision: 0.732231\n",
      "[1800]\tvalid_0's binary_logloss: 0.63314\tvalid_0's average_precision: 0.730044\n",
      "[1900]\tvalid_0's binary_logloss: 0.632038\tvalid_0's average_precision: 0.731281\n",
      "[2000]\tvalid_0's binary_logloss: 0.631834\tvalid_0's average_precision: 0.730692\n",
      "[2100]\tvalid_0's binary_logloss: 0.632011\tvalid_0's average_precision: 0.729723\n",
      "[2200]\tvalid_0's binary_logloss: 0.63182\tvalid_0's average_precision: 0.730168\n",
      "Logloss: 0.6318195653529364, Confident objects precision: 0.723342939481268, % of confident objects: 0.2923336141533277\n",
      "Fold #5\n",
      "[100]\tvalid_0's binary_logloss: 0.657872\tvalid_0's average_precision: 0.704708\n",
      "[200]\tvalid_0's binary_logloss: 0.651539\tvalid_0's average_precision: 0.706744\n",
      "[300]\tvalid_0's binary_logloss: 0.646012\tvalid_0's average_precision: 0.708954\n",
      "[400]\tvalid_0's binary_logloss: 0.641399\tvalid_0's average_precision: 0.717048\n",
      "[500]\tvalid_0's binary_logloss: 0.640868\tvalid_0's average_precision: 0.716707\n",
      "[600]\tvalid_0's binary_logloss: 0.639545\tvalid_0's average_precision: 0.720734\n",
      "[700]\tvalid_0's binary_logloss: 0.637479\tvalid_0's average_precision: 0.720194\n",
      "[800]\tvalid_0's binary_logloss: 0.638311\tvalid_0's average_precision: 0.714467\n",
      "[900]\tvalid_0's binary_logloss: 0.636559\tvalid_0's average_precision: 0.718487\n",
      "[1000]\tvalid_0's binary_logloss: 0.635845\tvalid_0's average_precision: 0.721075\n",
      "[1100]\tvalid_0's binary_logloss: 0.634606\tvalid_0's average_precision: 0.720661\n",
      "[1200]\tvalid_0's binary_logloss: 0.634493\tvalid_0's average_precision: 0.720513\n",
      "[1300]\tvalid_0's binary_logloss: 0.633726\tvalid_0's average_precision: 0.720791\n",
      "[1400]\tvalid_0's binary_logloss: 0.63176\tvalid_0's average_precision: 0.722624\n",
      "[1500]\tvalid_0's binary_logloss: 0.630577\tvalid_0's average_precision: 0.72398\n",
      "[1600]\tvalid_0's binary_logloss: 0.63008\tvalid_0's average_precision: 0.72504\n",
      "[1700]\tvalid_0's binary_logloss: 0.629841\tvalid_0's average_precision: 0.727356\n",
      "[1800]\tvalid_0's binary_logloss: 0.629431\tvalid_0's average_precision: 0.728533\n",
      "[1900]\tvalid_0's binary_logloss: 0.629403\tvalid_0's average_precision: 0.729164\n",
      "[2000]\tvalid_0's binary_logloss: 0.62974\tvalid_0's average_precision: 0.730321\n",
      "[2100]\tvalid_0's binary_logloss: 0.629802\tvalid_0's average_precision: 0.731119\n",
      "[2200]\tvalid_0's binary_logloss: 0.629316\tvalid_0's average_precision: 0.732481\n",
      "Logloss: 0.6293157916536654, Confident objects precision: 0.7688442211055276, % of confident objects: 0.27486187845303867\n",
      "Total fold Logloss: 0.6404585637362186, Total confident objects precision: 0.7302275189599133, Total % of confident objects: 0.2889341054938175\n"
     ]
    }
   ],
   "source": [
    "def model_train(df, train_df, test_df, features, task_type, how, n_folds, low_bound, high_bound, train_test): \n",
    "    X, groups = train_df[features], train_df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(train_df[['pattern', 'ttype']], drop_first=True)], axis=1)\n",
    "    y = train_df['target']\n",
    "    \n",
    "    if train_test == 'fold':\n",
    "        oof = np.zeros([train_df['target'].shape[0], 1])\n",
    "        \n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "\n",
    "        oe_enc = OrdinalEncoder()\n",
    "        groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "        print(f\"Training with {len(features)} features\")\n",
    "        \n",
    "        if how == 'lreg':\n",
    "            scaler = StandardScaler()\n",
    "            X[X.columns] = scaler.fit_transform(X)\n",
    "        \n",
    "        for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            print(f'Fold #{fold + 1}')\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[fit_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[fit_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "            \n",
    "            models = list()\n",
    "            if how == 'lgbm':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "            elif how == 'lreg':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            prec_score, prec_obj_pct = confident_score(y_val, val_preds[:,1], low_bound, high_bound)\n",
    "            print(f'Logloss: {val_score}, Confident objects precision: {prec_score}, % of confident objects: {prec_obj_pct}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "\n",
    "        return oof, model\n",
    "    elif train_test == 'full':\n",
    "        print('Train on full data')\n",
    "        X, y = df[features], df['target']\n",
    "        X = pd.concat([X, pd.get_dummies(df[['pattern', 'ttype']], drop_first=True)], axis=1)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X, y, eval_set=[(X, y)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        return np.zeros([df.shape[0], 1]), model\n",
    "    else:\n",
    "        # fit model on full dataset and predict on test\n",
    "        print(\"Test fold\")\n",
    "        X_test, y_test = test_df[features], test_df['target']\n",
    "        X_test = pd.concat([X_test, pd.get_dummies(test_df[['pattern', 'ttype']], drop_first=True)], axis=1)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X, y, eval_set=[(X_test, y_test)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        oof_test = np.zeros([test_df['target'].shape[0], 1])\n",
    "        oof_test[:,0] = model.predict_proba(X_test)[:,1]\n",
    "        return oof_test, model\n",
    "\n",
    "def prepare_features(fi, feature_num):\n",
    "    ''' Get features, sort them by their time appearance and return for using in train and inference'''\n",
    "    fi = fi['Feature'].head(feature_num)\n",
    "    feature_dict = defaultdict(list)\n",
    "    features = list()\n",
    "\n",
    "    for f in fi:\n",
    "        period = f.split('_')\n",
    "        if period[-1].isdigit():\n",
    "            feature_dict[int(period[-1])].append('_'.join(period[:-2]))\n",
    "        else:\n",
    "            feature_dict[0].append(f)\n",
    "\n",
    "    feature_dict = dict(sorted(feature_dict.items()))\n",
    "    \n",
    "    for item in feature_dict.items():\n",
    "        if item[0] > 0:\n",
    "            features.extend([i + f'_prev_{item[0]}' for i in item[1]])\n",
    "        else:\n",
    "            features.extend([i for i in item[1]])\n",
    "\n",
    "    feature_dict['features'] = features + ['Pattern_Trend', 'STOCH_RSI']\n",
    "\n",
    "    return features, feature_dict\n",
    "\n",
    "def confident_score(y, oof, low_bound, high_bound):\n",
    "    ''' Consider only high confident objects for accuracy and precision scores calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    # pred_conf_acc = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "    pred_conf_prec = pred_conf[(oof > high_bound)]\n",
    "    # y_conf_acc = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "    y_conf_prec = y.values.reshape(-1,1)[(oof > high_bound)]\n",
    "\n",
    "    return precision_score(y_conf_prec, pred_conf_prec), y_conf_prec.shape[0]/y.shape[0]\n",
    "\n",
    "train_test = 'fold'\n",
    "low_bound, high_bound = 0.31, 0.69\n",
    "feature_num = 160\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'n_estimators': 2200,\n",
    "    'learning_rate': 0.02,\n",
    "    #   'early_stopping_round': 50,\n",
    "    'max_depth': 10,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'subsample': 0.85,\n",
    "    'subsample_freq': 1,\n",
    "    'num_leaves': 24,\n",
    "    'verbosity': -1,\n",
    "    'max_bin': 255,\n",
    "    'reg_alpha': 1e-6,\n",
    "    'reg_lambda': 1e-8,\n",
    "    'objective': 'binary',\n",
    "    # 'is_unbalance': True,\n",
    "    # 'class_weight': 'balanced',\n",
    "    'metric': 'average_precision'\n",
    "    }\n",
    "\n",
    "if CFG.train_LGBM:\n",
    "    fi = pd.read_csv('feature_importance.csv')\n",
    "    features, feature_dict = prepare_features(fi, feature_num)\n",
    "    oof, model = model_train(df, train_df, test_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound, train_test=train_test)\n",
    "\n",
    "    if train_test == 'fold':\n",
    "        y = train_df['target']\n",
    "        oof_val_score = log_loss(y, oof)\n",
    "        oof_conf_prec_score, oof_conf_obj_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "        print(f'Total fold Logloss: {oof_val_score}, Total confident objects precision: {oof_conf_prec_score}, Total % of confident objects: {oof_conf_obj_pct}')\n",
    "    elif train_test == 'test':\n",
    "        y_test = test_df['target']\n",
    "        test_val_score = log_loss(y_test, oof)\n",
    "        test_conf_prec_score, test_conf_obj_pct = confident_score(y_test, oof, low_bound, high_bound)\n",
    "        print(f'Total test Logloss: {test_val_score}, Total test confident objects precision: {test_conf_prec_score}, Total % of test confident objects: {test_conf_obj_pct}')\n",
    "    elif train_test == 'full':\n",
    "        model.booster_.save_model('lgbm.pkl')\n",
    "        joblib.dump(model, 'lgbm.pkl')\n",
    "        # save feature dictionary for further inference\n",
    "        with open(f'features.json', 'w') as f:\n",
    "            json.dump(feature_dict, f)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total fold Logloss: 0.654868571882246, Total confident objects precision: 0.7310313493866424, Total % of confident objects: 0.20181551439574547\n",
    "\n",
    "Total test Logloss: 0.6532393825806957, Total test confident objects precision: 0.7449392712550608, Total % of test confident objects: 0.17846820809248554"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count predictions according to pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3695\n",
       "0    2694\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pattern</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MACD</th>\n",
       "      <td>0.364930</td>\n",
       "      <td>4357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STOCH_RSI</th>\n",
       "      <td>0.125984</td>\n",
       "      <td>2032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean  count\n",
       "pattern                   \n",
       "MACD       0.364930   4357\n",
       "STOCH_RSI  0.125984   2032"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[:,'oof'] = oof >= high_bound\n",
    "train_df.groupby('pattern')['oof'].agg(['mean', 'count'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
