{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and set configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from optimizer.optimizer import Optimizer\n",
    "from os import environ\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, confusion_matrix, precision_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "style.use(\"fivethirtyeight\")\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "from config.config import ConfigFactory\n",
    "from indicators import indicators\n",
    "\n",
    "from api.tvdatafeed.main import TvDatafeed, Interval\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "# here we load environment variables from .env, must be called before init. class\n",
    "load_dotenv(find_dotenv('../.env'), verbose=True)\n",
    "\n",
    "# Set environment variable\n",
    "environ[\"ENV\"] = \"optimize\"\n",
    "\n",
    "tv_username = os.getenv(\"TV_USERNAME\")\n",
    "tv_password = os.getenv(\"TV_PASSWORD\")\n",
    "\n",
    "class CFG:\n",
    "    load = False\n",
    "    historical = False\n",
    "    create_dataset = False\n",
    "    update_dataset = False\n",
    "    cls_target_ratio_tp = 1.05\n",
    "    cls_target_ratio_sl = 1.05\n",
    "    # last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "    last = 272\n",
    "    # for how long time (in hours) we want to predict\n",
    "    target_offset = 128\n",
    "    ttype = \"both\"\n",
    "    patterns_to_filter = [\"STOCH_RSI_Volume24\"]\n",
    "    select_features = False\n",
    "    optimize = True\n",
    "    optimize_alpha = 0.2\n",
    "    n_repeats = 1\n",
    "    n_folds = 8\n",
    "    min_precision = 0.575\n",
    "    last_date = datetime.strptime(\"2024-09-16:12:00:00\", \"%Y-%m-%d:%H:%M:%S\")\n",
    "    agg_periods = [24, 72]\n",
    "    agg_funcs = [np.min, np.max, np.mean, np.median, np.std]\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=(FutureWarning, pd.errors.PerformanceWarning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "### Load STOCH_RSI buy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_timeframe = \"1h\"\n",
    "higher_timeframe = \"4h\"\n",
    "opt_limit = 100000\n",
    "\n",
    "ttype = \"buy\"\n",
    "pattern = [\"STOCH\", \"RSI\", \"Volume24\"]\n",
    "indicator_list = pattern\n",
    "indicator_list_higher = [\"Trend\", \"MACD\"]\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "configs[\"Indicator_list\"] = indicator_list\n",
    "configs[\"Higher_TF_indicator_list\"] = indicator_list_higher\n",
    "configs[\"Timeframes\"][\"work_timeframe\"] = work_timeframe\n",
    "configs[\"Timeframes\"][\"higher_timeframe\"] = higher_timeframe\n",
    "\n",
    "optim_dict = {\n",
    "                \"RSI\": {\n",
    "                        \"timeperiod\": [14], \n",
    "                        \"low_bound\": [35]\n",
    "                       },\n",
    "                \"STOCH\": {\n",
    "                          \"fastk_period\": [9],\n",
    "                          \"slowk_period\": [7],\n",
    "                          \"slowd_period\": [3], \n",
    "                          \"low_bound\": [25]\n",
    "                        }\n",
    "             }\n",
    "\n",
    "if CFG.load:\n",
    "  print(f\"Timeframe is {work_timeframe}/{higher_timeframe}, trade type is {ttype}\")\n",
    "  opt = Optimizer(pattern, optim_dict, clean=False, **configs)\n",
    "  min_time = datetime.now().replace(microsecond=0, second=0, minute=0) - pd.to_timedelta(365 * 10, unit=\"D\")\n",
    "  stat = opt.optimize(pattern, ttype, opt_limit, load=True, op_type=\"ml\", historical=CFG.historical, min_time=min_time)\n",
    "  display(stat) # 31060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load STOCH_RSI sell data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_timeframe = \"1h\"\n",
    "higher_timeframe = \"4h\"\n",
    "opt_limit = 100000\n",
    "\n",
    "ttype = \"sell\"\n",
    "pattern = [\"STOCH\", \"RSI\", \"Volume24\"]\n",
    "indicator_list = pattern\n",
    "indicator_list_higher = [\"Trend\", \"MACD\"]\n",
    "\n",
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "configs[\"Indicator_list\"] = indicator_list\n",
    "configs[\"Higher_TF_indicator_list\"] = indicator_list_higher\n",
    "configs[\"Timeframes\"][\"work_timeframe\"] = work_timeframe\n",
    "configs[\"Timeframes\"][\"higher_timeframe\"] = higher_timeframe\n",
    "\n",
    "optim_dict = {\n",
    "                \"RSI\": {\n",
    "                        \"timeperiod\": [14], \n",
    "                        \"low_bound\": [35]\n",
    "                       },\n",
    "                \"STOCH\": {\n",
    "                          \"fastk_period\": [9],\n",
    "                          \"slowk_period\": [7],\n",
    "                          \"slowd_period\": [3], \n",
    "                          \"low_bound\": [25]\n",
    "                        }\n",
    "             }\n",
    "\n",
    "if CFG.load:\n",
    "  print(f\"Timeframe is {work_timeframe}/{higher_timeframe}, trade type is {ttype}\")\n",
    "  opt = Optimizer(pattern, optim_dict, clean=False, **configs)\n",
    "  min_time = datetime.now().replace(microsecond=0, second=0, minute=0) - pd.to_timedelta(365 * 10, unit=\"D\")\n",
    "  stat = opt.optimize(pattern, ttype, opt_limit, load=False, op_type=\"ml\", historical=CFG.historical, min_time=min_time)\n",
    "  display(stat) # 23629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test loaded data\n",
    "\n",
    "### Remove dataframe files with the biggest number of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 \n",
    "\n",
    "tickers_1h = glob(\"../optimizer/ticker_dataframes/*_1h.pkl\")\n",
    "for ticker in tqdm(tickers_1h[3:]):\n",
    "    df = pd.read_pickle(ticker)\n",
    "    nunique = df[\"time\"].diff()[1:].nunique()\n",
    "    if nunique > 1:\n",
    "        count = df[df[\"time\"].diff().astype(\"timedelta64[h]\") != 1].shape[0]\n",
    "        if count > 200:\n",
    "            print(ticker, nunique, count)\n",
    "            os.remove(ticker)\n",
    "            os.remove(ticker[:-6] + \"4h.pkl\")\n",
    "            \n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot time vs index for some of dataframe files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = 3\n",
    "figsize = (15, 4 * n_rows)\n",
    "\n",
    "tickers = glob(\"../optimizer/ticker_dataframes/*_1h.pkl\")\n",
    "random_tickers = random.choices(tickers, k=n_cols*n_rows)\n",
    "random_tickers[:3] = [t for t in tickers if \"/BTCUSDT_1h\" in t or \"/ETHUSDT_1h\" in t or \"/LTCUSDT_1h\" in t]\n",
    "\n",
    "def plot_times(random_tickers, n_rows, n_cols):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    for idx in range(n_cols*n_rows):\n",
    "        ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        \n",
    "        df = pd.read_pickle(random_tickers[idx])\n",
    "        sns.lineplot(data=df[\"time\"])\n",
    "\n",
    "        ax.set_ylabel(\"\")\n",
    "        plt.yticks(fontsize=12) \n",
    "        ax.set_xlabel(\"\")\n",
    "        plt.xticks(fontsize=12)\n",
    "        # ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_title(f\"{random_tickers[idx].split('/')[-1][:-7]}\", loc=\"right\", weight=\"bold\", fontsize=15)\n",
    "        ax.lines[0].set_linewidth(2)\n",
    "\n",
    "    \n",
    "    fig.suptitle(f\"Time vs Index\\n\", ha=\"center\",  fontweight=\"bold\", fontsize=21)\n",
    "    # fig.legend([1, 0], loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), fontsize=21, ncol=3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_times(sorted(random_tickers),  n_rows=n_rows, n_cols=n_cols) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all dataframe files has data for both timeframes 1h and 4h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "x = glob.glob(\"../optimizer/ticker_dataframes/*.pkl\")\n",
    "y = [i[31:].split(\"_\")[0] for i in x]\n",
    "z = (\"\").join(x)\n",
    "\n",
    "for i in y:\n",
    "    if f\"{i}_1h\" not in z:\n",
    "        print(i, \"1h\")\n",
    "    if f\"{i}_4h\" not in z:\n",
    "        print(i, \"4h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and create train data\n",
    "\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configs\n",
    "configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "higher_features = [\"time\", \"linear_reg\", \"linear_reg_angle\", \"macd\", \"macdhist\", \"macdsignal\"]\n",
    "price_cols = [\"open\",\"high\", \"low\", \"close\"]\n",
    "real_price_cols = [\"real_high\", \"real_low\", \"real_close\"]\n",
    "funding_cols = [\"funding_rate\"]\n",
    "rsi_stoch_cols = [\"rsi\", \"stoch_diff\", \"stoch_slowd\", \"stoch_slowk\"]\n",
    "btcd_cols = [\"time\", \"btcd_open\", \"btcd_high\", \"btcd_low\", \"btcd_close\", \"btcd_volume\"]\n",
    "btcdom_cols = [\"time\", \"btcdom_open\", \"btcdom_high\", \"btcdom_low\", \"btcdom_close\", \"btcdom_volume\"]\n",
    "\n",
    "def get_file(ticker):\n",
    "    \"\"\" Find files buy ticker names \"\"\"\n",
    "    try:\n",
    "        tmp_df_1h = pd.read_pickle(f\"../optimizer/ticker_dataframes/{ticker}_1h.pkl\")\n",
    "        tmp_df_4h = pd.read_pickle(f\"../optimizer/ticker_dataframes/{ticker}_4h.pkl\")\n",
    "    except FileNotFoundError:\n",
    "        return None, None\n",
    "    else:\n",
    "        return tmp_df_1h, tmp_df_4h\n",
    "\n",
    "\n",
    "def add_indicators(df: pd.DataFrame, df_higher: pd.DataFrame, ttype: str, configs: dict) -> pd.DataFrame:\n",
    "    \"\"\"Create indicators and add them to the dataset\"\"\"\n",
    "    # add RSI\n",
    "    rsi = indicators.RSI(ttype, configs)\n",
    "    df = rsi.get_indicator(df, \"\", \"\", 0)\n",
    "    # add Stochastic\n",
    "    stoch = indicators.STOCH(ttype, configs)\n",
    "    df = stoch.get_indicator(df, \"\", \"\", 0)\n",
    "    # add ATR\n",
    "    atr = indicators.ATR(ttype, configs)\n",
    "    df = atr.get_indicator(df, \"\", \"\", 0)\n",
    "    # add CCI\n",
    "    cci = indicators.CCI(ttype, configs)\n",
    "    df = cci.get_indicator(df, \"\", \"\", 0)\n",
    "    # add SAR\n",
    "    sar = indicators.SAR(ttype, configs)\n",
    "    df = sar.get_indicator(df, \"\", \"\", 0)\n",
    "    # add MACD\n",
    "    macd = indicators.MACD(ttype, configs)\n",
    "    df_higher = macd.get_indicator(df_higher, \"\", \"\", 0)\n",
    "    # add Trend\n",
    "    trend = indicators.Trend(ttype, configs)\n",
    "    df_higher = trend.get_indicator(df_higher, \"\", \"\", 0)\n",
    "    # merge higher timeframe indicators with working timeframe\n",
    "    df_higher[\"time\"] = df_higher[\"time\"] + pd.to_timedelta(3, unit=\"h\")\n",
    "    df[higher_features] = pd.merge(df[[\"time\"]], df_higher[higher_features], how=\"left\", on=\"time\")\n",
    "    df = df.drop(columns=[\"close_smooth\"])\n",
    "    df = df.drop(columns=[c for c in df.columns if c.endswith(\"_dir\")])\n",
    "    # merge with BTC.D dataframe\n",
    "    df[btcd_cols] = pd.merge(df[[\"time\"]], btcd[btcd_cols], how=\"left\", on=\"time\")\n",
    "    # merge with BTCDOM dataframe\n",
    "    df[btcdom_cols] = pd.merge(df[[\"time\"]], btcdom[btcdom_cols], how=\"left\", on=\"time\")\n",
    "    df = df.ffill()\n",
    "    df[btcdom_cols] = df[btcdom_cols].fillna(df[btcdom_cols].mean().round(1))\n",
    "    # price and MACD columns to pct difference\n",
    "    df[real_price_cols] = df[[\"high\", \"low\", \"close\"]]\n",
    "    cols_to_scale = [\"open\", \"high\", \"low\", \"close\", \"macd\", \"macdhist\", \"macdsignal\", \"atr\"]\n",
    "    for c in cols_to_scale:\n",
    "        df[c] = df[c].pct_change() * 100\n",
    "    # # add aggregate values\n",
    "    # for col in [\"close\", \"btcd_close\", \"btcdom_close\"]:\n",
    "    #     for period in CFG.agg_periods:\n",
    "    #         for agg_func in CFG.agg_funcs:\n",
    "    #             df[f\"{col}_{agg_func.__name__}_{period}\"] = df[col].rolling(period, min_periods=1).agg({\"func\": agg_func})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_train_df(df, ttype, configs, target_offset, first, last, step, train_df_prev=None):\n",
    "    \"\"\" Create train dataset from signal statistics and ticker candle data\"\"\"\n",
    "    train_df = list()\n",
    "    tickers = df[\"ticker\"].unique()\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        \n",
    "        # get signals with current ticker\n",
    "        signal_df = df[df[\"ticker\"] == ticker]\n",
    "        times = signal_df[\"time\"]\n",
    "        \n",
    "        # load max time for that ticker from the previously created dataset\n",
    "        if train_df_prev is not None:\n",
    "            max_time = train_df_prev.loc[train_df_prev[\"ticker\"] == ticker, \"time\"].max()\n",
    "        else:\n",
    "            max_time = None\n",
    "        \n",
    "        # load candle history of this ticker\n",
    "        tmp_df_1h, tmp_df_4h = get_file(ticker)\n",
    "\n",
    "        # add indicators \n",
    "        try:\n",
    "            tmp_df_1h = add_indicators(tmp_df_1h, tmp_df_4h, ttype, configs)\n",
    "        except TypeError:\n",
    "            # print(f\"TypeError, ticker - {ticker}\")\n",
    "            continue\n",
    "\n",
    "        # add historical data for current ticker\n",
    "        for i, t in enumerate(times.to_list()):\n",
    "            if max_time and t <= max_time:\n",
    "                continue\n",
    "            \n",
    "            pass_cycle = False\n",
    "            pattern = signal_df.iloc[i, signal_df.columns.get_loc(\"pattern\")]\n",
    "            row = tmp_df_1h.loc[tmp_df_1h[\"time\"] == t, :].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(first, last + 1, step):\n",
    "                # collect features every 4 hours, save difference between the current feature and the lagged features\n",
    "                time_prev = t + timedelta(hours= -i)\n",
    "                try:\n",
    "                    row_tmp = tmp_df_1h.loc[tmp_df_1h[\"time\"] == time_prev, [c for c in tmp_df_1h.columns if c not in real_price_cols]].reset_index(drop=True)\n",
    "                    if i % 8 != 0:\n",
    "                        row_tmp = row_tmp.drop(columns=funding_cols)\n",
    "                    if i % 24 != 0:\n",
    "                        row_tmp = row_tmp.drop(columns=btcd_cols)\n",
    "                    row_tmp.columns = [c + f\"_prev_{i}\" for c in row_tmp.columns]\n",
    "                except IndexError:\n",
    "                    pass_cycle = True\n",
    "                    break\n",
    "                row = pd.concat([row, row_tmp.iloc[:,1:]], axis=1)\n",
    "                row[\"ticker\"] = ticker\n",
    "                row[\"pattern\"] = pattern\n",
    "                \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "\n",
    "            row[\"target\"] = 0\n",
    "            row[\"max_price_deviation\"] = 0\n",
    "            row[\"first_price\"] = 0\n",
    "            row[\"last_price\"] = 0\n",
    "            row[\"ttype\"] = ttype\n",
    "            \n",
    "            # If ttype = buy and during the selected period high price was higher than close_price * target_ratio\n",
    "            # and earlier low price wasn't lower than close_price / target_ratio, than target is True, else target is False.\n",
    "            # Similarly for ttype = sell \n",
    "            if pattern.startswith(\"MACD\"):\n",
    "                close_price = tmp_df_1h.loc[tmp_df_1h[\"time\"] == t + timedelta(hours=3), \"real_close\"]\n",
    "            else:\n",
    "                close_price = tmp_df_1h.loc[tmp_df_1h[\"time\"] == t, \"real_close\"]\n",
    "            \n",
    "\n",
    "            # move to the next ticker if can't find any data corresponding to time t\n",
    "            if close_price.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            row[\"first_price\"] = close_price.values[0]\n",
    "            row[\"close_time\"] = row[\"time\"].values[0] + pd.to_timedelta(target_offset, unit=\"h\")\n",
    "            \n",
    "            close_price = close_price.values[0]\n",
    "            higher_price = close_price * CFG.cls_target_ratio_tp\n",
    "            lower_price = close_price * (2 - CFG.cls_target_ratio_tp)\n",
    "            \n",
    "            target_buys, target_sells = [], []\n",
    "            for i in range(1, target_offset + 1):\n",
    "                \n",
    "                if pattern.startswith(\"MACD\"):\n",
    "                    time_next = t + timedelta(hours=3+i)\n",
    "                else:\n",
    "                    time_next = t + timedelta(hours=i)\n",
    "                    \n",
    "                target_buy = tmp_df_1h.loc[tmp_df_1h[\"time\"] == time_next, \"real_high\"]\n",
    "                target_sell = tmp_df_1h.loc[tmp_df_1h[\"time\"] == time_next, \"real_low\"]\n",
    "                \n",
    "                target_buys.append(target_buy)\n",
    "                target_sells.append(target_sell)\n",
    "                \n",
    "                if target_buy.shape[0] == 0 or target_sell.shape[0] == 0:\n",
    "                    if row[\"target\"].values[0] == 0:\n",
    "                        pass_cycle = True\n",
    "                    break\n",
    "                \n",
    "                target_buy = target_buy.values[0]\n",
    "                target_sell = target_sell.values[0]\n",
    "\n",
    "                # set TPs and SLs\n",
    "                target_buy_tp = 1 if target_buy > higher_price else 0\n",
    "                target_buy_sl = 1 if target_buy > higher_price else 0\n",
    "                target_sell_tp = 1 if target_sell < lower_price else 0\n",
    "                target_sell_sl = 1 if target_sell < lower_price else 0\n",
    "\n",
    "                pattern = row[\"pattern\"].values[0]\n",
    "                ttype = row[\"ttype\"].values[0]\n",
    "                \n",
    "                # set SL flag and exit cycle if price reaches stop-loss threshold before it reaches take-profit threshold\n",
    "                # (SL depends on ttype and pattern)\n",
    "                sl1 = pattern.startswith(\"STOCH\") and ttype == \"buy\" and target_buy_sl == 1\n",
    "                sl2 = pattern.startswith(\"STOCH\") and ttype == \"sell\" and target_sell_sl == 1\n",
    "\n",
    "                # set TP flag and exit cycle if price reaches take-profit threshold and doesn't reach stop-loss threshold before\n",
    "                # (TP depends on ttype and pattern)\n",
    "                tp1 = pattern.startswith(\"STOCH\") and ttype == \"buy\" and target_sell_tp == 1\n",
    "                tp2 = pattern.startswith(\"STOCH\") and ttype == \"sell\" and target_buy_tp == 1\n",
    "                    \n",
    "                # if both TP and SL flag is on - don't consider that trade\n",
    "                if (tp1 and sl1) or (tp2 and sl2):\n",
    "                    if row[\"target\"].values[0] == 0:\n",
    "                        pass_cycle = True\n",
    "                    break\n",
    "                elif sl1 or sl2:\n",
    "                    # if reach TP - write the time when the trade was closed (but only one time)\n",
    "                    row[\"close_time\"] = row[\"time\"].values[0] + pd.to_timedelta(i, unit=\"h\")\n",
    "                    break\n",
    "                elif tp1 or tp2:\n",
    "                    # if reach TP - write the time when the trade was closed (but only one time)\n",
    "                    if row[\"close_time\"].values[0] == row[\"time\"].values[0] + pd.to_timedelta(target_offset, unit=\"h\"):\n",
    "                        row[\"close_time\"] = row[\"time\"].values[0] + pd.to_timedelta(i, unit=\"h\")\n",
    "                    row[\"target\"] = 1\n",
    "                \n",
    "                # if price doesn't reaches both TP and SL thresholds but price above / below enter price for buy / sell trade - set TP flag\n",
    "                # (depends on ttype and pattern)\n",
    "                if i == target_offset: \n",
    "                    last_price = tmp_df_1h.loc[tmp_df_1h[\"time\"] == time_next, \"real_close\"].values[0]\n",
    "                    l1 = pattern.startswith(\"STOCH\") and ttype == \"buy\" and last_price < close_price\n",
    "                    l2 = pattern.startswith(\"STOCH\") and ttype == \"sell\" and last_price > close_price\n",
    "                    # if price doesn't reach both TP and SL - write its last price\n",
    "                    if row[\"target\"].values[0] != 1:\n",
    "                        row[\"last_price\"] = last_price\n",
    "\n",
    "                    if l1 or l2:\n",
    "                        row[\"target\"] = 1\n",
    "\n",
    "                # set the maximum price deviation to the correct side for the current trade period \n",
    "                if ttype == \"sell\":\n",
    "                    curr_price = (target_buy - close_price) / close_price\n",
    "                else:\n",
    "                    curr_price = (close_price - target_sell) / close_price\n",
    "                \n",
    "                row[\"max_price_deviation\"] = max(row[\"max_price_deviation\"].values[0], curr_price) \n",
    "            \n",
    "            if pass_cycle:\n",
    "                continue\n",
    "            \n",
    "            # add data to the dataset\n",
    "            train_df.append(row)\n",
    "    \n",
    "    train_df = pd.concat(train_df).reset_index(drop=True)\n",
    "    train_df = train_df.drop(columns=real_price_cols)\n",
    "    return train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.create_dataset:\n",
    "    # Get BTC dominance\n",
    "    tv = TvDatafeed(username=tv_username, password=tv_password)\n",
    "    \n",
    "    btcd = tv.get_hist(\"BTC.D\",\"CRYPTOCAP\", interval=Interval.in_daily, n_bars=7000, extended_session=True).reset_index()\n",
    "    btcd = btcd.drop(columns=\"symbol\")\n",
    "    btcd.columns = btcd_cols\n",
    "    btcd[\"time\"] = btcd[\"time\"] + pd.to_timedelta(23, unit=\"h\")\n",
    "\n",
    "    btcdom = tv.get_hist(\"BTCDOMUSDT.P\",\"BINANCE\", interval=Interval.in_4_hour, n_bars=7000, extended_session=True).reset_index()\n",
    "    btcdom = btcdom.drop(columns=\"symbol\")\n",
    "    btcdom.columns = btcdom_cols\n",
    "    btcdom[\"time\"] = btcdom[\"time\"] + pd.to_timedelta(3, unit=\"h\")\n",
    "    \n",
    "    # first previous data point to collect for model training (value represents number of hours before signal point)\n",
    "    first = 4\n",
    "    # step of previous data points collecting (total number of points to collect is (last - first + step) / step)\n",
    "    step = 4\n",
    "\n",
    "    # Buy\n",
    "    # good hours: \n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle(\"signal_stat/buy_stat_1h.pkl\")\n",
    "    df = df[df[\"time\"].dt.year > 1970]\n",
    "    df = df[df[\"pattern\"].isin(CFG.patterns_to_filter)]\n",
    "    buy_hours_to_save = [0, 1, 2, 3, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
    "    df = df[df[\"time\"].dt.hour.isin(buy_hours_to_save)]\n",
    "    # if previously generated dataset exists - update it, don't create it from scratch\n",
    "    train_df_prev = None\n",
    "    if CFG.update_dataset:\n",
    "        try:\n",
    "            train_df_prev = pd.read_pickle(f\"signal_stat/train_buy_{CFG.last}.pkl\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    # dataset for model training\n",
    "    train_buy = create_train_df(df, \"buy\", configs, CFG.target_offset, first, CFG.last, step, train_df_prev)\n",
    "    train_buy = train_buy.dropna()\n",
    "    if CFG.update_dataset and train_df_prev is not None:\n",
    "        train_buy = pd.concat([train_df_prev, train_buy]).reset_index(drop=True)\n",
    "    train_buy.to_pickle(f\"signal_stat/train_buy_{CFG.last}.pkl\")\n",
    "\n",
    "    display(train_buy.head())\n",
    "    display(train_buy.shape)\n",
    "\n",
    "    # Sell\n",
    "    # good hours \n",
    "    # dataset with the signal statistics\n",
    "    df = pd.read_pickle(\"signal_stat/sell_stat_1h.pkl\")\n",
    "    df = df[df[\"time\"].dt.year > 1970]\n",
    "    df = df[df[\"pattern\"].isin(CFG.patterns_to_filter)]\n",
    "    sell_hours_to_save = [5, 8, 9, 11, 14, 17]\n",
    "    df = df[df[\"time\"].dt.hour.isin(sell_hours_to_save)]\n",
    "    # if previously generated dataset exists - update it, don't create it from scratch\n",
    "    train_df_prev = None\n",
    "    if CFG.update_dataset:\n",
    "        try:\n",
    "            train_df_prev = pd.read_pickle(f\"signal_stat/train_sell_{CFG.last}.pkl\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    # dataset for model training\n",
    "    train_sell = create_train_df(df, \"sell\", configs, CFG.target_offset, first, CFG.last, step, train_df_prev)\n",
    "    train_sell = train_sell.dropna()\n",
    "    if CFG.update_dataset and train_df_prev is not None:\n",
    "        train_sell = pd.concat([train_df_prev, train_sell]).reset_index(drop=True)\n",
    "    train_sell.to_pickle(f\"signal_stat/train_sell_{CFG.last}.pkl\")\n",
    "\n",
    "    display(train_sell.head())\n",
    "    display(train_sell.shape)\n",
    "    \n",
    "    # this is made for tests\n",
    "    df = pd.read_pickle(\"signal_stat/buy_stat_1h.pkl\")\n",
    "    df = df[df[\"time\"].dt.year > 1970]\n",
    "    df = df[df[\"pattern\"].isin(CFG.patterns_to_filter)]\n",
    "\n",
    "    test_df_buy_1 = df[df[\"time\"].dt.hour.isin(buy_hours_to_save[0:1])]\n",
    "    test_df_buy_1 = create_train_df(test_df_buy_1, \"buy\", configs, CFG.target_offset, first, CFG.last, step)\n",
    "    test_df_buy_1 = test_df_buy_1.dropna().sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "    df = pd.read_pickle(\"signal_stat/sell_stat_1h.pkl\")\n",
    "    df = df[df[\"time\"].dt.year > 1970]\n",
    "    df = df[df[\"pattern\"].isin(CFG.patterns_to_filter)]\n",
    "    \n",
    "    test_df_sell_1 = df[df[\"time\"].dt.hour.isin(sell_hours_to_save[0:1])]\n",
    "    test_df_sell_1 = create_train_df(test_df_sell_1, \"sell\", configs, CFG.target_offset, first, CFG.last, step)\n",
    "    test_df_sell_1 = test_df_sell_1.dropna().sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "    del df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buy = pd.read_pickle(f\"signal_stat/train_buy_{CFG.last}.pkl\")\n",
    "train_sell = pd.read_pickle(f\"signal_stat/train_sell_{CFG.last}.pkl\")\n",
    "\n",
    "train_df = pd.concat([train_buy, train_sell])\n",
    "train_df = train_df.sort_values(\"time\")\n",
    "\n",
    "# do not consider the last signals - they may contain erroneus signals\n",
    "train_df = train_df[train_df[\"time\"] < train_df[\"time\"].max()].reset_index(drop=True)\n",
    "\n",
    "macd_cols = [c for c in train_df.columns if \"prev\" not in c and \"sar\" in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data tests\n",
    "\n",
    "### Test train dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "if CFG.create_dataset:\n",
    "    # check if train dataset has only columns that we expect\n",
    "    cols = set(re.sub(r\"_prev_\\d+\", \"\", c) for c in train_buy.columns)\n",
    "\n",
    "    agg_funcs = [\"amin\", \"amax\", \"mean\", \"median\", \"std\"]\n",
    "    agg_cols = [c for c in cols if len(c.split(\"_\")) > 2 and c.split(\"_\")[-2] in agg_funcs]\n",
    "\n",
    "    expected_cols = set(price_cols + higher_features + funding_cols + rsi_stoch_cols +\n",
    "                        btcd_cols + btcdom_cols + agg_cols + [\"atr\", \"cci\", \"sar\", \"volume\", \"pattern\", \"target\", \"max_price_deviation\", \"ticker\", \"ttype\", \"close_time\", \"first_price\", \"last_price\"])\n",
    "    assert expected_cols == cols\n",
    "\n",
    "    # check RSI and STOCH columns, their values must be in [0, 100] range \n",
    "    rsi_stoch_cols_ = [c for c in train_df.columns if (\"rsi\" in c or \"stoch\" in c) and \"diff\" not in c]\n",
    "    for r_s_c in rsi_stoch_cols_:\n",
    "        assert train_df[r_s_c].min() > -0.0001\n",
    "        assert train_df[r_s_c].max() < 100.0001\n",
    "\n",
    "    # check volume columns, their values must be >= 0\n",
    "    vol_cols_ = [c for c in train_df.columns if \"volume\" in c]\n",
    "    for v_c in vol_cols_:\n",
    "        assert train_df[v_c].min() >= 0\n",
    "\n",
    "    # check funding columns, their period must be 8 hours\n",
    "    funding_cols_ = [c for c in train_df.columns if c.startswith(\"funding\")]\n",
    "    for f_c in funding_cols_:\n",
    "        num = \"\".join([i for i in f_c if i.isdigit()])\n",
    "        if len(num) > 0:\n",
    "            num = int(num)\n",
    "            assert num % 8 == 0\n",
    "\n",
    "    # check BTC dominance columns, their values must be in [0, 100] range \n",
    "    # and their period must be 24 hours\n",
    "    btcd_cols_ = [c for c in train_df.columns if c.startswith(\"btcd_\") and \"volume\" not in c]\n",
    "    for b_c in btcd_cols_:\n",
    "        pass_cycle = False\n",
    "        for a_c in agg_funcs:\n",
    "            if a_c in b_c:\n",
    "                pass_cycle = True\n",
    "                break\n",
    "        if pass_cycle:\n",
    "            continue\n",
    "        # check values\n",
    "        assert train_df[b_c].min() >= 0\n",
    "        assert train_df[b_c].max() <= 100\n",
    "        # check period\n",
    "        num = \"\".join([i for i in b_c if i.isdigit()])\n",
    "        if len(num) > 0:\n",
    "            num = int(num)\n",
    "            assert num % 24 == 0\n",
    "\n",
    "    # check the rest columns, their period must be 4 hours\n",
    "    rest_cols_ = [c for c in train_df.columns if c not in funding_cols_ and c not in btcd_cols]\n",
    "    for r_c in rest_cols_:\n",
    "        num = \"\".join([i for i in r_c if i.isdigit()])\n",
    "        if len(num) > 0:\n",
    "            num = int(num)\n",
    "            assert num % 4 == 0\n",
    "\n",
    "    # check if changing of source dataframe doesn't affect the resulting train dataframe\n",
    "    test_df_buy_2 = train_buy[train_buy[\"time\"].dt.hour.isin(buy_hours_to_save[0:1])]\n",
    "    assert test_df_buy_1.shape == test_df_buy_2.shape\n",
    "    test_df_sell_2 = train_sell[train_sell[\"time\"].dt.hour.isin(sell_hours_to_save[0:1])]\n",
    "    assert test_df_sell_1.shape == test_df_sell_2.shape\n",
    "\n",
    "    # check if columns that were added for backtest and statistics are correct\n",
    "    for i, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "        if row[\"last_price\"] > 0:\n",
    "            assert row[\"close_time\"] - row[\"time\"] == pd.to_timedelta(CFG.target_offset, unit=\"h\")\n",
    "            assert row[\"max_price_deviation\"] < CFG.cls_target_ratio_tp - 1\n",
    "        else:\n",
    "            assert row[\"close_time\"] - row[\"time\"] <= pd.to_timedelta(CFG.target_offset, unit=\"h\")\n",
    "            if row[\"target\"] == 1:\n",
    "                assert row[\"max_price_deviation\"] >= CFG.cls_target_ratio_tp - 1\n",
    "            else:\n",
    "                assert row[\"max_price_deviation\"] < CFG.cls_target_ratio_tp - 1\n",
    "\n",
    "    # plot time values \n",
    "    train_df[\"time\"].plot(title=\"Signal Time distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test buy target corectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_ticker = None\n",
    "\n",
    "train_buy_ = train_buy[train_buy[\"ticker\"] == \"MILOUSDT\"]\n",
    "\n",
    "if CFG.create_dataset:\n",
    "    for i in tqdm(range(train_buy.shape[0])):\n",
    "        x = train_buy[[\"ticker\", \"ttype\", \"pattern\", \"time\", \"close\", \"target\"]]\n",
    "        y = x.iloc[i]\n",
    "        pattern, ticker, time_, target = y[\"pattern\"], y[\"ticker\"], y[\"time\"], y[\"target\"]\n",
    "        if ticker != \"MILOUSDT\":\n",
    "            continue\n",
    "\n",
    "        if ticker != prev_ticker:\n",
    "            tmp_df = pd.read_pickle(f\"../optimizer/ticker_dataframes/{ticker}_1h.pkl\")\n",
    "            prev_ticker = ticker\n",
    "\n",
    "        tmp_df_1h = tmp_df.copy()\n",
    "        idx = tmp_df_1h[tmp_df_1h[\"time\"] == time_].index[0]\n",
    "        close_price = tmp_df_1h.loc[tmp_df_1h[\"time\"] == time_, \"close\"].values[0]\n",
    "        last_idx = min(idx+CFG.target_offset, len(tmp_df_1h)-1)\n",
    "        last_price = tmp_df_1h.iloc[last_idx, tmp_df_1h.columns.get_loc(\"close\")]\n",
    "        low_price = close_price * (2 - CFG.cls_target_ratio_tp)\n",
    "        high_price = close_price * CFG.cls_target_ratio_tp\n",
    "        tmp_df_1h[\"low_price\"] = low_price\n",
    "        tmp_df_1h[\"high_price\"] = high_price\n",
    "\n",
    "        tmp_df_1h = tmp_df_1h.iloc[idx+1:idx+CFG.target_offset+1][[\"time\", \"close\", \"high\", \"high_price\", \"low\", \"low_price\"]]\n",
    "        tmp_df_1h[\"signal\"] = tmp_df_1h[\"low\"] < tmp_df_1h[\"low_price\"]\n",
    "        tmp_df_1h[\"anti_signal\"] = tmp_df_1h[\"high\"] > tmp_df_1h[\"high_price\"]\n",
    "        \n",
    "        first_signal = tmp_df_1h[\"signal\"].argmax()\n",
    "        first_anti_signal = tmp_df_1h[\"anti_signal\"].argmax()\n",
    "        \n",
    "        if tmp_df_1h[\"signal\"].max() == 0 and tmp_df_1h[\"anti_signal\"].max() == 0:\n",
    "            if pattern.startswith(\"STOCH\"): \n",
    "                if last_price < close_price:\n",
    "                    assert target == 1\n",
    "                else:\n",
    "                    assert target == 0\n",
    "        elif tmp_df_1h[\"signal\"].max() == 0:\n",
    "            assert target == 0\n",
    "        elif tmp_df_1h[\"anti_signal\"].max() == 0:\n",
    "            assert target == 1\n",
    "        else:\n",
    "            if target == 1:\n",
    "                assert first_signal < first_anti_signal\n",
    "            else:\n",
    "                try:\n",
    "                    assert first_signal > first_anti_signal\n",
    "                except AssertionError:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sell target corectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.create_dataset:\n",
    "    for i in tqdm(range(train_sell.shape[0])):\n",
    "        x = train_sell[[\"ticker\", \"ttype\", \"pattern\", \"time\", \"close\", \"target\"]]\n",
    "        y = x.iloc[i]\n",
    "        pattern, ticker, time_, target = y[\"pattern\"], y[\"ticker\"], y[\"time\"], y[\"target\"]\n",
    "\n",
    "        if ticker != prev_ticker:\n",
    "            tmp_df = pd.read_pickle(f\"../optimizer/ticker_dataframes/{ticker}_1h.pkl\")\n",
    "            prev_ticker = ticker\n",
    "\n",
    "        tmp_df_1h = tmp_df.copy()\n",
    "        idx = tmp_df_1h[tmp_df_1h[\"time\"] == time_].index[0]\n",
    "        close_price = tmp_df_1h.loc[tmp_df_1h[\"time\"] == time_, \"close\"].values[0]\n",
    "        last_idx = min(idx+CFG.target_offset, len(tmp_df_1h)-1)\n",
    "        last_price = tmp_df_1h.iloc[last_idx, tmp_df_1h.columns.get_loc(\"close\")]\n",
    "        low_price = close_price * (2 - CFG.cls_target_ratio_tp)\n",
    "        high_price = close_price * CFG.cls_target_ratio_tp\n",
    "        tmp_df_1h[\"low_price\"] = low_price\n",
    "        tmp_df_1h[\"high_price\"] = high_price\n",
    "\n",
    "        tmp_df_1h = tmp_df_1h.iloc[idx+1:idx+CFG.target_offset+1][[\"time\", \"close\", \"high\", \"high_price\", \"low\", \"low_price\"]]\n",
    "        tmp_df_1h[\"signal\"] = tmp_df_1h[\"high\"] > tmp_df_1h[\"high_price\"]\n",
    "        tmp_df_1h[\"anti_signal\"] = tmp_df_1h[\"low\"] < tmp_df_1h[\"low_price\"]\n",
    "\n",
    "        first_signal = tmp_df_1h[\"signal\"].argmax()\n",
    "        first_anti_signal = tmp_df_1h[\"anti_signal\"].argmax()\n",
    "        \n",
    "        if tmp_df_1h[\"signal\"].max() == 0 and tmp_df_1h[\"anti_signal\"].max() == 0:\n",
    "            if pattern.startswith(\"STOCH\"):\n",
    "                if last_price > close_price:\n",
    "                    assert target == 1\n",
    "                else:\n",
    "                    assert target == 0\n",
    "        elif tmp_df_1h[\"signal\"].max() == 0:\n",
    "            assert target == 0\n",
    "        elif tmp_df_1h[\"anti_signal\"].max() == 0:\n",
    "            assert target == 1\n",
    "        else:\n",
    "            if target == 1:\n",
    "                assert first_signal < first_anti_signal\n",
    "            else:\n",
    "                assert first_signal > first_anti_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pattern / target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q10(x):\n",
    "    return x.quantile(0.1)\n",
    "\n",
    "def q30(x):\n",
    "    return x.quantile(0.3)\n",
    "\n",
    "def q90(x):\n",
    "    return x.quantile(0.9)\n",
    "\n",
    "def trust_interval(row, z=1.95):\n",
    "    \"\"\" Calculate trust interval for Bernulli distribution \"\"\"\n",
    "    sum, val1 = row[\"total\"], row[\"count\"]\n",
    "    val2 = sum - val1\n",
    "    n = val1 + val2\n",
    "    p = val1 / n\n",
    "    low_bound = p - z * np.sqrt(p * (1-p) / n)\n",
    "    high_bound = p + z * np.sqrt(p * (1-p) / n)\n",
    "    return round(low_bound, 4), round(high_bound, 4)\n",
    "\n",
    "pvt = train_buy[(train_buy[\"time\"] < CFG.last_date)]\n",
    "pvt = pvt[[\"target\", \"pattern\", \"time\", \"max_price_deviation\"]]\n",
    "pvt[\"hour\"] = pvt[\"time\"].dt.hour\n",
    "pvt = pvt.pivot_table(index=[\"hour\", \"target\"], \n",
    "                      values=[\"pattern\", \"max_price_deviation\"], \n",
    "                      aggfunc={\n",
    "                          \"pattern\": \"count\",\n",
    "                          \"max_price_deviation\": [\"median\", q10, q30, q90],\n",
    "                          }).reset_index()\n",
    "pvt.columns = [\"hour\", \"target\", \"max_price_dev_q50\", \n",
    "               \"max_price_dev_q10\", \"max_price_dev_q30\", \n",
    "               \"max_price_dev_q90\", \"pattern\"]\n",
    "pvt[\"total\"] = pvt.groupby(\"hour\")[\"pattern\"].transform(\"sum\")\n",
    "pvt = pvt.rename(columns={\"pattern\": \"count\"})\n",
    "pvt[\"pct\"] = pvt[\"count\"] / pvt[\"total\"]\n",
    "pvt = pvt[pvt[\"target\"]==1]\n",
    "pvt[\"trust_interval\"] = pvt.apply(trust_interval, axis=1)\n",
    "\n",
    "print(\"Buy\")\n",
    "display(pvt)\n",
    "display(train_buy[\"ttype\"].value_counts())\n",
    "display(train_buy[[\"target\", \"pattern\"]].value_counts())\n",
    "display(train_buy[[\"target\", \"pattern\"]].value_counts(normalize=True))\n",
    "\n",
    "pvt = train_sell[(train_sell[\"time\"] < CFG.last_date)]\n",
    "pvt = pvt[[\"target\", \"pattern\", \"time\", \"max_price_deviation\"]]\n",
    "pvt[\"hour\"] = pvt[\"time\"].dt.hour\n",
    "pvt = pvt.pivot_table(index=[\"hour\", \"target\"], \n",
    "                      values=[\"pattern\", \"max_price_deviation\"], \n",
    "                      aggfunc={\n",
    "                          \"pattern\": \"count\",\n",
    "                          \"max_price_deviation\": [\"median\", q10, q30, q90],\n",
    "                          }).reset_index()\n",
    "pvt.columns = [\"hour\", \"target\", \"max_price_dev_q50\", \n",
    "               \"max_price_dev_q10\", \"max_price_dev_q30\", \n",
    "               \"max_price_dev_q90\", \"pattern\"]\n",
    "pvt[\"total\"] = pvt.groupby(\"hour\")[\"pattern\"].transform(\"sum\")\n",
    "pvt = pvt.rename(columns={\"pattern\": \"count\"})\n",
    "pvt[\"pct\"] = pvt[\"count\"] / pvt[\"total\"]\n",
    "pvt = pvt[pvt[\"target\"]==1]\n",
    "pvt[\"trust_interval\"] = pvt.apply(trust_interval, axis=1)\n",
    "\n",
    "print(\"Sell\")\n",
    "display(pvt)\n",
    "display(train_sell[\"ttype\"].value_counts())\n",
    "display(train_sell[[\"target\", \"pattern\"]].value_counts())\n",
    "display(train_sell[[\"target\", \"pattern\"]].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization\n",
    "\n",
    "### Plot ratio of class 1 for every week day\n",
    "\n",
    "Monday = 0, Sunday = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "group_df = train_df[(train_df[\"time\"].dt.year >= 2020)]\n",
    "group_df[\"weekday\"] = group_df[\"time\"].dt.weekday\n",
    "ax = group_df.groupby(\"weekday\")[\"target\"].mean().plot()\n",
    "ax.lines[0].set_linewidth(2)\n",
    "ax.set_xlabel(\"Weekday\")\n",
    "ax.set_ylabel(\"Class 1 ratio\")\n",
    "ax.set_title(\"Class 1 ratio vs Weekday\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ratio of class 1 for every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "group_df = train_df[(train_df[\"time\"].dt.year >= 2020)]\n",
    "group_df[\"month\"] = group_df[\"time\"].dt.month\n",
    "ax = group_df.groupby(\"month\")[\"target\"].mean().plot()\n",
    "ax.lines[0].set_linewidth(2)\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Class 1 ratio\")\n",
    "ax.set_title(\"Class 1 ratio vs Month\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ratio of class 1 for every day of last months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "group_df = train_df[(train_df[\"time\"].dt.year >= 2024) & (train_df[\"time\"].dt.month >= 5)]\n",
    "group_df[\"day\"] = group_df[\"time\"].dt.to_period(\"D\")\n",
    "ax = group_df.groupby(\"day\")[\"target\"].mean().plot()\n",
    "ax.lines[0].set_linewidth(2)\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"Class 1 ratio\")\n",
    "ax.set_title(\"Class 1 ratio vs Day\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize buy trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "\n",
    "if CFG.create_dataset:\n",
    "\n",
    "    plt_num = 3\n",
    "    j = 1\n",
    "    fig = plt.figure(figsize=(30, 6 * plt_num))\n",
    "\n",
    "    buy_idxs = train_buy.index\n",
    "    test_buy = train_buy.sample(plt_num, axis=0)\n",
    "\n",
    "\n",
    "    for i, row in test_buy.iterrows():\n",
    "        ticker = row[\"ticker\"]\n",
    "        time = row[\"time\"]\n",
    "        target = row[\"target\"]\n",
    "        ttype = row[\"ttype\"]\n",
    "        pattern = row[\"pattern\"]\n",
    "\n",
    "        df_1h, _ = get_file(ticker)\n",
    "        df_1h = df_1h[(df_1h[\"time\"] >= time) & (df_1h[\"time\"] <= time + timedelta(hours=CFG.target_offset))]\n",
    "\n",
    "        ohlc = df_1h[[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"]].set_index(\"time\")\n",
    "        \n",
    "        price = df_1h.iloc[0][\"close\"]\n",
    "        high_price = price * CFG.cls_target_ratio_tp\n",
    "        low_price = price * (2 - CFG.cls_target_ratio_tp)\n",
    "\n",
    "        ax = fig.add_subplot(plt_num, 1, j)\n",
    "        mpf.plot(ohlc, type=\"candle\", warn_too_much_data=1001, style=\"yahoo\", ylabel=\"\", tz_localize=True, ax=ax)\n",
    "        \n",
    "        if ttype == \"buy\":\n",
    "            ax.axhline(high_price, color=\"g\")\n",
    "            ax.axhline(low_price, color=\"r\")\n",
    "        else:\n",
    "            ax.axhline(high_price, color=\"r\")\n",
    "            ax.axhline(low_price, color=\"g\")\n",
    "        ax.set_title(f\"Ticker: {ticker}, price: {price}, pattern {pattern}, target: {target}\", fontsize=20)\n",
    "        j += 1\n",
    "        \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sell trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.create_dataset:\n",
    "    plt_num = 3\n",
    "    j = 1\n",
    "    fig = plt.figure(figsize=(30, 6 * plt_num))\n",
    "\n",
    "    buy_idxs = train_sell.index\n",
    "    test_sell = train_sell.sample(plt_num, axis=0)\n",
    "\n",
    "    for i, row in test_sell.iterrows():\n",
    "        ticker = row[\"ticker\"]\n",
    "        time = row[\"time\"]\n",
    "        target = row[\"target\"]\n",
    "        ttype = row[\"ttype\"]\n",
    "        pattern = row[\"pattern\"]\n",
    "\n",
    "        df_1h, _ = get_file(ticker)\n",
    "        df_1h = df_1h[(df_1h[\"time\"] >= time) & (df_1h[\"time\"] <= time + timedelta(hours=CFG.target_offset))]\n",
    "        \n",
    "        ohlc = df_1h[[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"]].set_index(\"time\")\n",
    "        \n",
    "        price = df_1h.iloc[0][\"close\"]\n",
    "        high_price = price * CFG.cls_target_ratio_tp\n",
    "        low_price = price * (2 - CFG.cls_target_ratio_tp)\n",
    "\n",
    "        ax = fig.add_subplot(plt_num, 1, j)\n",
    "        mpf.plot(ohlc, type=\"candle\", warn_too_much_data=1001, style=\"yahoo\", ylabel=\"\", tz_localize=True, ax=ax)\n",
    "        \n",
    "        if ttype == \"buy\":\n",
    "            ax.axhline(high_price, color=\"g\")\n",
    "            ax.axhline(low_price, color=\"r\")\n",
    "        else:\n",
    "            ax.axhline(high_price, color=\"r\")\n",
    "            ax.axhline(low_price, color=\"g\")\n",
    "        ax.set_title(f\"Ticker: {ticker}, price: {price}, pattern {pattern}, target: {target}\", fontsize=20)\n",
    "        j += 1\n",
    "        \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.read_csv(f\"model/feature_importance.csv\")[\"Feature\"].to_list()\n",
    "cols = [c for c in fi if \"prev\" not in c]\n",
    "figsize = (20, 30)\n",
    "\n",
    "def plot_target_violine(df, df_cols, n_rows, n_cols, target):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    for idx, col in enumerate(df_cols):\n",
    "        ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        try:\n",
    "            sns.violinplot(x=target, y=col, data=df)\n",
    "        except ValueError:\n",
    "            print(f\"Can't find {col} in the dataframe\")\n",
    "\n",
    "        ax.set_ylabel(\"\"); ax.spines[\"top\"].set_visible(False), \n",
    "        ax.set_xlabel(\"\"); ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_title(f\"{col}\", loc=\"right\", weight=\"bold\", fontsize=21)\n",
    "\n",
    "    \n",
    "    fig.suptitle(f\"Features vs Target ({target})\\n\\n\\n\", ha=\"center\",  fontweight=\"bold\", fontsize=21)\n",
    "    # fig.legend([1, 0], loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), fontsize=21, ncol=3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "n_cols = 3\n",
    "n_rows = len(cols) // n_cols + 1\n",
    "plot_target_violine(train_df, cols, n_rows=n_rows, n_cols=n_cols, target=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the last signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_pickle(\"signal_stat/buy_stat_1h.pkl\")\n",
    "x[\"ttype\"] = \"buy\"\n",
    "y = pd.read_pickle(\"signal_stat/sell_stat_1h.pkl\")\n",
    "y[\"ttype\"] = \"sell\"\n",
    "x = pd.concat([x, y]).sort_values(\"time\").reset_index(drop=True)\n",
    "x.loc[x[\"pattern\"] == \"STOCH_RSI_Volume24\", [\"time\", \"ticker\", \"ttype\", \"pattern\"]].tail(50\n",
    "                                                                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "### Select features with BORUTA feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shaphypetune import BoostBoruta\n",
    "  \n",
    "params = {\n",
    "          \"boosting_type\": \"dart\",\n",
    "          \"n_estimators\": 1000,\n",
    "          \"learning_rate\": 0.02,\n",
    "          \"max_depth\": 10,\n",
    "          \"subsample\" : 0.7,\n",
    "          \"colsample_bytree\": 0.85,\n",
    "          \"num_leaves\": 24,\n",
    "          \"verbosity\": -1,\n",
    "          \"importance_type\": \"gain\",\n",
    "          \"max_bin\": 255,\n",
    "          \"reg_alpha\": 1e-6,\n",
    "          \"reg_lambda\": 1e-7,\n",
    "          \"objective\": \"binary\",\n",
    "          \"metric\": \"auc\"\n",
    "        }\n",
    "features = [c for c in train_df.columns if c not in [\"time\", \"target\", \"ticker\", \"pattern\", \"ttype\", \"max_price_deviation\"]]\n",
    "\n",
    "\n",
    "def ppv_npv_acc(y_true, y_pred):\n",
    "    \"\"\" Calculate confusion matrix and return harmonic mean score of Positive Predictive Value (PPV, precisoin) and Negative Predictive Value (NPV) \"\"\"\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    return (tp + tn) / (tp + fp + tn + fn + 1e-8)\n",
    "\n",
    "def ppv_npv_acc_lgbm(y_true, y_pred):\n",
    "    \"\"\" Metric for LGBM \"\"\"\n",
    "    return \"ppv_npv_acc\", ppv_npv_acc(y_true, y_pred), False\n",
    "\n",
    "\n",
    "def boruta_selction(df):\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "\n",
    "    X, y, time = df[features], df[\"target\"], df[\"time\"]\n",
    "\n",
    "    tss = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=CFG.n_folds, test_size=len(X) // int(CFG.n_folds * 1.5))\n",
    "    eval_metric = \"logloss\"\n",
    "\n",
    "    # Stratify based on Class and Alpha (3 types of conditions)\n",
    "    for fold, (train_idx, val_idx) in enumerate(tss.split(time)):\n",
    "\n",
    "        print(f\"Fold: {fold}\")\n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**params)\n",
    "        model = BoostBoruta(clf, importance_type=\"shap_importances\", train_importance=False, max_iter=1000)\n",
    "        try:\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                      eval_metric=\"logloss\", \n",
    "                      callbacks=[lgb.log_evaluation(100)])\n",
    "        except RuntimeError:\n",
    "            break\n",
    "\n",
    "        boruta_importance_df = pd.DataFrame({\"importance\": model.ranking_}, index=X_train.columns).sort_index()\n",
    "        if boruta_df_.shape[0] == 0:\n",
    "            boruta_df_ = boruta_importance_df.copy()\n",
    "        else:\n",
    "            boruta_df_ += boruta_importance_df\n",
    "\n",
    "    boruta_df_ = boruta_df_.sort_values(\"importance\")\n",
    "    boruta_df_ = boruta_df_.reset_index().rename({\"index\": \"Feature\"}, axis=1)\n",
    "\n",
    "    return boruta_df_\n",
    "\n",
    "\n",
    "if CFG.select_features:\n",
    "    boruta_df_ = boruta_selction(train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features with permutation importance and GBM feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [\"#302c36\", \"#037d97\", \"#E4591E\", \"#C09741\",\n",
    "           \"#EC5B6D\", \"#90A6B1\", \"#6ca957\", \"#D8E3E2\"]\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "# load the list of Bybit tickers\n",
    "with open(f\"model/bybit_tickers.json\", \"r\") as f:\n",
    "    bybit_tickers = json.load(f)\n",
    "\n",
    "def lgbm_tuning(df, permut=False):\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "\n",
    "    X, y, time = df[features], df[\"target\"], df[\"time\"]\n",
    "    \n",
    "    for fold in range(CFG.n_repeats):\n",
    "        print(f\"Repeat {blu}#{fold+1}\")\n",
    "\n",
    "        tss = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=CFG.n_folds, test_size=len(X) // int(CFG.n_folds * 1.5))\n",
    "        \n",
    "        oof = np.zeros(len(df))\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (fit_idx, val_idx) in enumerate(tss.split(time)):\n",
    "            if fold == 0:\n",
    "                first_val_idx = val_idx[0]\n",
    "            \n",
    "            # select only those val_idxs that correspond to \n",
    "            # time = max train dataset time + 96 hours to prevent data leakage\n",
    "            max_train_time = time[fit_idx].max() + pd.to_timedelta(96, unit=\"h\")\n",
    "            max_val_time = time[val_idx].max()\n",
    "            # also select only tickers from ByBit for validation because we trade on ByBit only\n",
    "            val_idx = time[(time > max_train_time) & (time <= max_val_time) & (train_df[\"ticker\"].isin(bybit_tickers))].index.tolist()\n",
    "            \n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[fit_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[fit_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            clf = lgb.LGBMClassifier(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                    eval_metric=\"logloss\", \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            val_preds = clf.predict_proba(X_val)[:,1]\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f\"Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}\")\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                      columns=[\"Value\",\"Feature\"])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_[\"Value\"] += f_i[\"Value\"]\n",
    "            \n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({\"importance\": perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "        outer_cv = log_loss(y[first_val_idx:], oof[first_val_idx:])\n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f\"{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}\")\n",
    "    print(f\"{'*' * 50}\\n\")\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values(\"importance\", ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({\"index\": \"Feature\"}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values(\"Value\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "if CFG.select_features:\n",
    "    perm_df_, feature_importances_, outer_cv_score = lgbm_tuning(train_df, permut=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def rfe_selection(df):\n",
    "    params = {\n",
    "          \"penalty\": \"l2\",\n",
    "          \"max_iter\": 10000,\n",
    "          \"C\": 1\n",
    "        }\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X, y = df[features], df[\"target\"]\n",
    "    X = scaler.fit_transform(X)\n",
    "        \n",
    "    estimator = LogisticRegression(**params)\n",
    "    selector = RFECV(estimator, min_features_to_select=50, step=0.025, cv=5, verbose=1)\n",
    "    selector = selector.fit(X, y)\n",
    "    rfe_df_ = pd.DataFrame({\"importance\": selector.ranking_}, index=features).sort_index()\n",
    "    rfe_df_ = rfe_df_.reset_index().rename({\"index\": \"Feature\"}, axis=1)\n",
    "    return rfe_df_\n",
    "\n",
    "if CFG.select_features:\n",
    "    rfe_df_ = rfe_selection(train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.select_features:\n",
    "    boruta_df_[\"rank\"] = boruta_df_[\"importance\"].rank()\n",
    "    perm_df_[\"rank\"] = perm_df_[\"importance\"].rank(ascending=False)\n",
    "    rfe_df_[\"rank\"] = rfe_df_[\"importance\"]\n",
    "    feature_importances_[\"rank\"] = feature_importances_[\"Value\"].rank(ascending=False)\n",
    "\n",
    "    fi = pd.concat([perm_df_[[\"Feature\",\"rank\"]], feature_importances_[[\"Feature\",\"rank\"]], rfe_df_[[\"Feature\",\"rank\"]],\n",
    "                    boruta_df_[[\"Feature\",\"rank\"]]])\n",
    "    fi = fi.groupby(\"Feature\")[\"rank\"].sum().sort_values().reset_index()\n",
    "    fi.to_csv(f\"model/feature_importance.csv\", index=False)\n",
    "else:\n",
    "    fi = pd.read_csv(f\"model/feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "### Load selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stoch_diff',\n",
       " 'linear_reg_angle',\n",
       " 'btcdom_close',\n",
       " 'high',\n",
       " 'close',\n",
       " 'open',\n",
       " 'volume_prev_4',\n",
       " 'high_prev_4',\n",
       " 'close_prev_4',\n",
       " 'low_prev_12',\n",
       " 'btcdom_volume_prev_20',\n",
       " 'stoch_slowk_prev_20',\n",
       " 'open_prev_24',\n",
       " 'high_prev_24',\n",
       " 'atr_prev_24',\n",
       " 'stoch_diff_prev_28',\n",
       " 'close_prev_28',\n",
       " 'low_prev_28',\n",
       " 'high_prev_32',\n",
       " 'rsi_prev_36',\n",
       " 'low_prev_36',\n",
       " 'btcdom_volume_prev_40',\n",
       " 'cci_prev_44',\n",
       " 'close_prev_44',\n",
       " 'high_prev_44',\n",
       " 'low_prev_44',\n",
       " 'high_prev_48',\n",
       " 'btcdom_volume_prev_56',\n",
       " 'rsi_prev_56',\n",
       " 'low_prev_60',\n",
       " 'high_prev_60',\n",
       " 'rsi_prev_68',\n",
       " 'low_prev_68',\n",
       " 'stoch_slowk_prev_72',\n",
       " 'high_prev_76',\n",
       " 'high_prev_80',\n",
       " 'stoch_slowk_prev_80',\n",
       " 'atr_prev_84',\n",
       " 'low_prev_84',\n",
       " 'btcdom_volume_prev_88',\n",
       " 'low_prev_88',\n",
       " 'stoch_slowk_prev_88',\n",
       " 'atr_prev_92',\n",
       " 'low_prev_96',\n",
       " 'atr_prev_96',\n",
       " 'close_prev_96',\n",
       " 'stoch_diff_prev_96',\n",
       " 'close_prev_104',\n",
       " 'btcdom_volume_prev_108',\n",
       " 'cci_prev_108',\n",
       " 'low_prev_108',\n",
       " 'close_prev_108',\n",
       " 'low_prev_112',\n",
       " 'btcdom_volume_prev_116',\n",
       " 'low_prev_116',\n",
       " 'linear_reg_angle_prev_116',\n",
       " 'stoch_slowk_prev_120',\n",
       " 'high_prev_124',\n",
       " 'low_prev_128',\n",
       " 'rsi_prev_132',\n",
       " 'low_prev_132',\n",
       " 'close_prev_132',\n",
       " 'stoch_diff_prev_136',\n",
       " 'btcdom_volume_prev_140',\n",
       " 'cci_prev_140',\n",
       " 'atr_prev_140',\n",
       " 'close_prev_144',\n",
       " 'stoch_diff_prev_144',\n",
       " 'cci_prev_148',\n",
       " 'close_prev_152',\n",
       " 'low_prev_156',\n",
       " 'rsi_prev_160',\n",
       " 'low_prev_160',\n",
       " 'stoch_diff_prev_164',\n",
       " 'cci_prev_168',\n",
       " 'btcdom_volume_prev_172',\n",
       " 'stoch_slowd_prev_176',\n",
       " 'high_prev_176',\n",
       " 'close_prev_180',\n",
       " 'atr_prev_188',\n",
       " 'btcd_volume_prev_192',\n",
       " 'btcdom_volume_prev_196',\n",
       " 'stoch_diff_prev_196',\n",
       " 'low_prev_200',\n",
       " 'cci_prev_204',\n",
       " 'high_prev_204',\n",
       " 'low_prev_204',\n",
       " 'rsi_prev_208',\n",
       " 'close_prev_208',\n",
       " 'stoch_diff_prev_212',\n",
       " 'btcdom_volume_prev_216',\n",
       " 'low_prev_220',\n",
       " 'low_prev_224',\n",
       " 'stoch_diff_prev_224',\n",
       " 'cci_prev_224',\n",
       " 'btcdom_volume_prev_232',\n",
       " 'high_prev_232',\n",
       " 'atr_prev_240',\n",
       " 'rsi_prev_240',\n",
       " 'close_prev_244',\n",
       " 'stoch_slowd_prev_244',\n",
       " 'low_prev_244',\n",
       " 'btcdom_volume_prev_248',\n",
       " 'low_prev_256',\n",
       " 'high_prev_256',\n",
       " 'atr_prev_256',\n",
       " 'stoch_slowk_prev_260',\n",
       " 'btcdom_volume_prev_260',\n",
       " 'close_prev_264',\n",
       " 'high_prev_264',\n",
       " 'rsi_prev_268',\n",
       " 'stoch_diff_prev_268',\n",
       " 'macd_prev_272',\n",
       " 'weekday',\n",
       " 'hour']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def exclude_corr_features(features, corr_thresh):\n",
    "    features_to_select = features.copy()\n",
    "    correlations = train_df.loc[:, features_to_select].corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "    correlations = correlations[correlations[\"level_0\"] != correlations[\"level_1\"]] \n",
    "    correlations.columns = [\"feature_1\", \"feature_2\", \"corr\"]\n",
    "\n",
    "    correlations = pd.merge(left=correlations, right=fi, how=\"left\", left_on=\"feature_1\", right_on=\"Feature\")\n",
    "    correlations = correlations.drop(columns=\"Feature\")\n",
    "    correlations = correlations.sort_values([\"corr\", \"rank\"], ascending=[False, True])\n",
    "    correlations = correlations[::2]\n",
    "\n",
    "    features_to_exclude = set()\n",
    "    correlations = correlations[correlations[\"corr\"] > corr_thresh]\n",
    "\n",
    "    for _, row in correlations.iterrows():\n",
    "        feature_1 = row[\"feature_1\"]\n",
    "        feature_2 = row[\"feature_2\"]\n",
    "\n",
    "        if feature_1 in features_to_exclude:\n",
    "            continue\n",
    "\n",
    "        features_to_exclude.add(feature_2)\n",
    "\n",
    "    return features_to_exclude\n",
    "\n",
    "\n",
    "def prepare_features(fi, feature_num, corr_thresh):\n",
    "    \"\"\" Get features, sort them by their time appearance and return for using in train and inference\"\"\"\n",
    "    # exclude some features \n",
    "    fi = fi[\"Feature\"]\n",
    "    fi = fi[:feature_num]\n",
    "    feature_dict = defaultdict(list)\n",
    "    features = list()\n",
    "    \n",
    "    for f in fi:\n",
    "        if f == \"volume_24\":\n",
    "            feature_dict[0].append(f)\n",
    "            continue\n",
    "        period = f.split(\"_\")\n",
    "        if period[-1].isdigit() and period[-2] == \"prev\":\n",
    "            feature_dict[int(period[-1])].append(\"_\".join(period[:-2]))\n",
    "        else:\n",
    "            feature_dict[0].append(f)\n",
    "\n",
    "    feature_dict = dict(sorted(feature_dict.items()))\n",
    "    \n",
    "    for item in feature_dict.items():\n",
    "        if item[0] > 0:\n",
    "            features.extend([i + f\"_prev_{item[0]}\" for i in item[1]])\n",
    "        else:\n",
    "            features.extend([i for i in item[1]])\n",
    "    \n",
    "    # select only features with low correlation\n",
    "    features_to_exclude = exclude_corr_features(features, corr_thresh)\n",
    "    features = [f for f in features if f not in features_to_exclude]\n",
    "    features = list(features) + [\"weekday\", \"hour\"]\n",
    "    \n",
    "    # remove highly correlated features from the feature dict\n",
    "    feature_dict[\"features\"] = features\n",
    "\n",
    "    for item in feature_dict.items():\n",
    "        if not isinstance(item[0], int):\n",
    "            continue\n",
    "\n",
    "        features_to_remove = list()\n",
    "\n",
    "        for f in item[1]:\n",
    "            if item[0] > 0:\n",
    "                f_ = f\"{f}_prev_{item[0]}\"\n",
    "            else:\n",
    "                f_ = f\n",
    "            \n",
    "            if f_ not in features:\n",
    "                assert f_ in features_to_exclude\n",
    "                features_to_remove.append(f)\n",
    "        \n",
    "        feature_dict[item[0]] = [f for f in feature_dict[item[0]] if f not in features_to_remove]\n",
    "\n",
    "    # remove empty lists from feature_dict\n",
    "    empty_list_keys = list()\n",
    "    \n",
    "    for key in feature_dict:\n",
    "        if not feature_dict[key]:\n",
    "            empty_list_keys.append(key)\n",
    "            \n",
    "    for key in empty_list_keys:\n",
    "        del feature_dict[key]\n",
    "\n",
    "    return features, feature_dict\n",
    "\n",
    "feature_num = 212\n",
    "corr_thresh = 0.544954\t\n",
    "\n",
    "# feature_num = 58\n",
    "# corr_thresh = 0.5324216723399987\n",
    "\n",
    "train_df[\"hour\"] = train_df[\"time\"].dt.hour\n",
    "train_df[\"weekday\"] = train_df[\"time\"].dt.weekday\n",
    "fi = pd.read_csv(\"model/feature_importance.csv\")\n",
    "features, feature_dict = prepare_features(fi, feature_num, corr_thresh)\n",
    "assert len(features) == len(set(features))\n",
    "\n",
    "display(features, len(features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "\n",
    "def conf_ppv_npv_acc_score(y: np.ndarray, oof: np.ndarray, \n",
    "                           low_bound: float, high_bound: float) -> Tuple[float, float, float]:\n",
    "    \"\"\" \n",
    "    Consider only high confident objects and low confident \n",
    "    objects for PPV and NPV score calculation \n",
    "    \"\"\"\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof >= high_bound] = 1\n",
    "    pred_conf = pred_conf[(oof >= high_bound)]\n",
    "    y_conf = y.values.reshape(-1,1)[(oof >= high_bound)]\n",
    "    if y_conf.shape[0] == 0:\n",
    "        return 0, 0, 0\n",
    "    return precision_score(y_conf, pred_conf), y_conf.shape[0], y_conf.shape[0]/y.shape[0]\n",
    "\n",
    "def model_train(df: pd.DataFrame, features: list, params: dict, sample_weight: Union[list, None],\n",
    "                n_folds: int, low_bound: float, high_bound: float, train_test: str, \n",
    "                max_train_size: int = None, verbose: bool = False) -> Tuple[np.ndarray, lgb.LGBMClassifier, list] : \n",
    "    \"\"\"\n",
    "    Train/validate model, return: \n",
    "        - model\n",
    "        - list of precisions for confident objects by folds (if train_test == \"fold\")\n",
    "        - list of profitable objects by folds (if train_test == \"fold\")\n",
    "    \"\"\"\n",
    "    X, time = df[features], df[\"time\"]\n",
    "    y = df[\"target\"]\n",
    "    val_idxs = []\n",
    "    conf_scores =[]\n",
    "    conf_object_nums = []\n",
    "    max_train_size = int(len(df) * max_train_size)\n",
    "    \n",
    "    if train_test == \"fold\":\n",
    "        oof = np.zeros([len(df), 1])\n",
    "\n",
    "        tss = TimeSeriesSplit(gap=0, max_train_size=max_train_size, n_splits=n_folds, test_size=(len(df) * 2) // (n_folds * 3))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Training with {len(features)} features\")\n",
    "        \n",
    "        for fold, (fit_idx, val_idx) in enumerate(tss.split(time)):\n",
    "            # select only those val_idxs that correspond to \n",
    "            # time = max train dataset time + 96 hours to prevent data leakage\n",
    "            max_train_time = time[fit_idx].max() + pd.to_timedelta(96, unit=\"h\")\n",
    "            max_val_time = time[val_idx].max()\n",
    "            \n",
    "            # also select only tickers from ByBit for validation because we trade on ByBit only\n",
    "            val_idx = time[(time > max_train_time) & (time <= max_val_time) & (train_df[\"ticker\"].isin(bybit_tickers))].index.tolist()\n",
    "            val_idxs.extend(val_idx)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Fold #{fold + 1}\")\n",
    "            \n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[fit_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[fit_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if verbose:\n",
    "                display(y_val.value_counts(normalize=True))\n",
    "                display(df.loc[val_idx[0], \"time\"])\n",
    "                display(df.loc[val_idx[-1], \"time\"])\n",
    "\n",
    "                plt.plot(train_df.index[fit_idx], [fold + 1] * len(fit_idx), label=f'Train {fold + 1}', color='blue')\n",
    "                plt.plot(train_df.index[val_idx], [fold + 1] * len(val_idx), label=f'Test {fold + 1}', color='red')\n",
    "                \n",
    "                callbacks = [lgb.log_evaluation(100)]\n",
    "            else:\n",
    "                callbacks = []\n",
    "            \n",
    "            if sample_weight is not None:\n",
    "                sample_weight = df.loc[fit_idx, \"weight\"]\n",
    "            \n",
    "            model_lgb = lgb.LGBMClassifier(**params)\n",
    "            model_lgb.fit(X_train, y_train, sample_weight=sample_weight, \n",
    "                          eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                          eval_metric=\"logloss\", callbacks=callbacks)\n",
    "\n",
    "            val_preds = model_lgb.predict_proba(X_val)\n",
    "            oof[val_idx, 0] = val_preds[:, 1]\n",
    "        \n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            conf_score, conf_obj_num, conf_obj_pct = conf_ppv_npv_acc_score(y_val, val_preds[:, 1], low_bound, high_bound)\n",
    "            conf_scores.append(conf_score)\n",
    "            conf_object_nums.append(conf_obj_num)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Logloss: {val_score}, Confident objects score: {conf_score}\\n\"\n",
    "                      f\"Number of confident objects {conf_obj_num}, % of confident objects: {conf_obj_pct}\\n\"\n",
    "                      f\"Number of profitable objects: {round((2 * conf_score -  1) * conf_obj_num)}\")\n",
    "        \n",
    "        if verbose:\n",
    "            plt.ylim(0.5, n_folds + 0.5)\n",
    "            plt.xlabel('Index')\n",
    "            plt.ylabel('Fold')\n",
    "            plt.title('Train/Test Distribution for Time-Series Split')\n",
    "            plt.legend(['Train', 'Test'], loc='lower right')\n",
    "            plt.show()\n",
    "        \n",
    "        return model_lgb, conf_scores, conf_object_nums, oof, val_idxs\n",
    "    elif train_test == \"inference\":\n",
    "        print(\"Train on full data\")\n",
    "        X, y = df.iloc[-max_train_size:][features], df.iloc[-max_train_size:][\"target\"]\n",
    "        if sample_weight is not None:\n",
    "                sample_weight = df.iloc[-max_train_size:][\"weight\"]\n",
    "        model_lgb = lgb.LGBMClassifier(**params)\n",
    "        model_lgb.fit(X, y, sample_weight=sample_weight, eval_set=[(X, y)], \n",
    "                      eval_metric=\"logloss\", callbacks = [lgb.log_evaluation(100)])\n",
    "        \n",
    "        return model_lgb, [], [], np.array([]), np.array([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0.08 # real take profit that we set in our trade\n",
    "SL = 0.05\n",
    "risk = 0.003 # what part of deposit we use in one trade\n",
    "leverage = 4\n",
    "price_change = SL * leverage\n",
    "\n",
    "open_comission = 0.00036 # Bybit comission for trade opening\n",
    "close_comission = 0.001 # Bybit comission for trade closing\n",
    "\n",
    "# consider trade slippage, let it be ~ 0.2%\n",
    "slippage = 0.002\n",
    "TP -= slippage\n",
    "SL += slippage\n",
    "\n",
    "def calculate_profit(target, quantity, first_price, last_price):\n",
    "    \"\"\"Calculate profit change according to trade success and comissions\"\"\"\n",
    "    if target == 1:\n",
    "        if last_price > 0: \n",
    "            # if price doesn't reach TP or SL - calculate profit\n",
    "            # from price of entering and exiting of trade\n",
    "            trade_profit = quantity * (1 + (last_price - first_price) / first_price * leverage)\n",
    "        else:\n",
    "            # TP reached\n",
    "            trade_profit = quantity * (1 + TP * leverage)\n",
    "    else:\n",
    "        # SL reached\n",
    "        trade_profit = quantity * (1 - SL * leverage)\n",
    "        \n",
    "    # subtract Bybit close comission \n",
    "    trade_profit *=  (1 - close_comission)\n",
    "    # subtract possible slippage\n",
    "\n",
    "    profit = trade_profit - quantity\n",
    "    return profit, trade_profit\n",
    "\n",
    "\n",
    "def backtest(df: pd.DataFrame, oof: pd.Series, val_idxs: pd.Series, high_bound: float, show_progress: bool = False) -> float:\n",
    "    \"\"\"Function for model backtest\"\"\"\n",
    "    # select only trades in which model is confident\n",
    "    backtest_df = df.loc[val_idxs, [\"target\", \"max_price_deviation\", \"time\", \"close_time\", \"first_price\", \"last_price\"]]\n",
    "    backtest_df[\"pred\"] = oof\n",
    "    backtest_df = backtest_df[backtest_df[\"pred\"] >= high_bound].reset_index(drop=True)\n",
    "    backtest_df[\"balance\"] = 1\n",
    "    backtest_df[\"free_balance\"] = 1\n",
    "    backtest_df[\"profit\"] = 0\n",
    "    backtest_df[\"trade_profit\"] = 0\n",
    "    backtest_df[\"quantity\"] = 0\n",
    "    backtest_df[\"profit_count\"] = 0\n",
    "    backtest_df[\"is_closed\"] = 0\n",
    "    \n",
    "    if show_progress:\n",
    "        generator = tqdm(backtest_df.iterrows(), total=len(backtest_df))\n",
    "    else:\n",
    "        generator = backtest_df.iterrows()\n",
    "\n",
    "    # move through the dataset row-by-row\n",
    "    for i, row in generator:\n",
    "        j = i - 1\n",
    "        # take balance from previous signal \n",
    "        # or if signal is first - set balance to 1\n",
    "        if j >= 0:\n",
    "            balance = backtest_df.loc[j, \"balance\"]\n",
    "            free_balance = backtest_df.loc[j, \"free_balance\"]\n",
    "        else:\n",
    "            balance = free_balance = 1\n",
    "        \n",
    "        signal_time = row[\"time\"]\n",
    "        target = row[\"target\"]\n",
    "        first_price = row[\"first_price\"]\n",
    "        last_price = row[\"last_price\"]\n",
    "        \n",
    "        # update balance with previous signals info\n",
    "        while j >= 0:\n",
    "            prev_signal_time = backtest_df.loc[j, \"time\"]\n",
    "            prev_signal_close_time = backtest_df.loc[j, \"close_time\"]\n",
    "            # if previous signal was closed - add its profit to the current balance \n",
    "            if signal_time >= prev_signal_close_time:\n",
    "                # if trade wasn't processed before\n",
    "                if  backtest_df.loc[j, \"is_closed\"] == 0:\n",
    "                    free_balance += backtest_df.loc[j, \"trade_profit\"]\n",
    "                    backtest_df.loc[i, \"trade_profit\"] += backtest_df.loc[j, \"trade_profit\"]\n",
    "                    backtest_df.loc[i, \"profit_count\"] += 1\n",
    "                    backtest_df.loc[j, \"is_closed\"] = 1\n",
    "            if signal_time - prev_signal_time > pd.to_timedelta(CFG.target_offset * 10, unit=\"h\"):\n",
    "                break\n",
    "            j -= 1\n",
    "\n",
    "        # calculate quantity to use in trade\n",
    "        quantity = free_balance * risk / price_change\n",
    "        \n",
    "        # subtact quantity + open comission from balance\n",
    "        balance -= quantity * open_comission\n",
    "        free_balance -= (quantity * (1 + open_comission))\n",
    "\n",
    "        # get profit from the current trade\n",
    "        profit, trade_profit = calculate_profit(target, quantity, first_price, last_price)\n",
    "\n",
    "        # write balance and profit to the dataset\n",
    "        backtest_df.loc[i, \"quantity\"] = quantity\n",
    "        backtest_df.loc[i, \"balance\"] = balance + profit\n",
    "        backtest_df.loc[i, \"free_balance\"] = free_balance\n",
    "        backtest_df.loc[i, \"profit\"] = profit\n",
    "        backtest_df.loc[i, \"trade_profit\"] = trade_profit\n",
    "\n",
    "    result = round(backtest_df['balance'].iloc[-1] * 100, 2)\n",
    "    return result, backtest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial) -> float:\n",
    "    \"\"\"Optimize hyperparameters using Optuna\"\"\"\n",
    "    params = {\n",
    "        # Main parameters\n",
    "#                     \"device\": \"gpu\",\n",
    "#                     \"gpu_platform_id\": 0,\n",
    "#                     \"gpu_device_id\": 0,\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"average_precison\",\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"dart\", \"goss\", \"gbdt\"]), \n",
    "        # Hyperparamters (in order of importance decreasing)\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 3000),  # max number of trees in model\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 3e-1),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True), # L1,  alias: lambda_l1\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True), # L2, alias: lambda_l2\n",
    "         # decrease to deal with overfit\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10), # tree max depth \n",
    "         # decrease to deal with overfit\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 4, 512),  # Max number of leaves in one tree\n",
    "                                                                # should be ~ 2**(max_depth-1)\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 0.9), # Randomly select a subset of features \n",
    "                                                                   # if colsample_bytree < 1.0\n",
    "                                                                   # alias:feature_fraction\n",
    "        # increase for accuracy, decrease to deal with overfit\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 32, 255), # Max number of bins that feature values will be bucketed in\n",
    "        # increase to deal with overfit\n",
    "        \"subsample_freq\": 1, # Perform bagging at every k iteration, alias: bagging_freq\n",
    "\n",
    "        # \"subsample_for_bin\": 200000, # Number of data that sampled to construct feature discrete bins; setting this \n",
    "                                       # to larger value will give better training result but may increase train time \n",
    "        # \"cat_smooth\": trial.suggest_float(\"cat_smooth\", 10.0, 100.0),  # this can reduce the effect of noises in \n",
    "                                                                         # categorical features, especially for \n",
    "                                                                         # categories with few data                                  \n",
    "        \"is_unbalance\": trial.suggest_categorical(\"is_unbalance\", [True, False]),                                            \n",
    "        \"verbose\": -1,\n",
    "        \"importance_type\": \"gain\",\n",
    "        \"high_bound\": trial.suggest_float(\"high_bound\", 0.5, 0.65),\n",
    "        \"low_bound\": trial.suggest_float(\"low_bound\", 0.0, 0.1),\n",
    "        \"feature_num\": trial.suggest_int(\"feature_num\", 30, 600),\n",
    "        \"corr_thresh\": trial.suggest_float(\"corr_thresh\", 0.5, 0.99),\n",
    "        \"max_train_size\": trial.suggest_float(\"max_train_size\", 0.5, 1),\n",
    "        \"sample_weight\": trial.suggest_categorical(\"sample_weight\", [None, \"cos\", \"linear\"])\n",
    "    }\n",
    "    \n",
    "    if params[\"boosting_type\"] != \"goss\":\n",
    "        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.3, 0.9)\n",
    "\n",
    "    if params[\"is_unbalance\"] == \"True\":\n",
    "        params[\"class_weight\"] = trial.suggest_categorical(\"class_weight\", [\"balanced\", None]), \n",
    "\n",
    "    high_bound = params[\"high_bound\"]\n",
    "    del params[\"high_bound\"]\n",
    "\n",
    "    low_bound = params[\"low_bound\"]\n",
    "    del params[\"low_bound\"]\n",
    "\n",
    "    corr_thresh = params[\"corr_thresh\"]\n",
    "    del params[\"corr_thresh\"]\n",
    "\n",
    "    # set max train size for TSS fold\n",
    "    max_train_size = params[\"max_train_size\"]\n",
    "    del params[\"max_train_size\"]\n",
    "\n",
    "    # add object weights\n",
    "    sample_weight = params[\"sample_weight\"]\n",
    "    del params[\"sample_weight\"]\n",
    "    \n",
    "    if sample_weight:\n",
    "        train_df[\"weight\"] = train_df[\"time\"].astype(np.int64) / int(1e6)\n",
    "        if sample_weight == \"cos\":\n",
    "            train_df[\"weight\"] = (train_df[\"weight\"].max() - train_df[\"weight\"]) /\\\n",
    "                  (train_df[\"weight\"].max() - train_df[\"weight\"].min()) * np.pi / 2\n",
    "            train_df[\"weight\"] = np.cos(train_df[\"weight\"])\n",
    "        else:\n",
    "            train_df[\"weight\"] = (train_df[\"weight\"].max() - train_df[\"weight\"]) /\\\n",
    "                  (train_df[\"weight\"].max() - train_df[\"weight\"].min())\n",
    "        sample_weight = True\n",
    "    \n",
    "    fi = pd.read_csv(f\"model/feature_importance.csv\")\n",
    "    feature_num = params[\"feature_num\"]\n",
    "    del params[\"feature_num\"]\n",
    "    \n",
    "    train_df[\"weekday\"] = train_df[\"time\"].dt.weekday\n",
    "    features, _ = prepare_features(fi, feature_num, corr_thresh)\n",
    "    \n",
    "    # train model, get results\n",
    "    _, conf_scores, conf_object_nums,oof, val_idxs = model_train(train_df[train_df[\"time\"] < CFG.last_date], features, params, sample_weight,\n",
    "                                                                 n_folds=10, low_bound=low_bound, high_bound=high_bound, \n",
    "                                                                 train_test=\"fold\", max_train_size=max_train_size, verbose=False)\n",
    "    \n",
    "    # calculate total number of confident objects and total precison for the confident objects\n",
    "    y = train_df[\"target\"][val_idxs]\n",
    "    oof = oof[val_idxs]\n",
    "    oof_conf_score, oof_conf_obj_num, _ = conf_ppv_npv_acc_score(y, oof, low_bound, high_bound)\n",
    "    # result = oof_conf_obj_num * (oof_conf_score - CFG.min_precision) * 100\n",
    "    result, _ = backtest(train_df, oof, val_idxs, high_bound)\n",
    "    \n",
    "    # prepare result list for t-test\n",
    "    results = [conf_object_num * (conf_score - CFG.min_precision) * 100 for conf_object_num, conf_score in zip(conf_object_nums, conf_scores)]\n",
    "    \n",
    "    # load dataframe with additional information\n",
    "    df_optuna_more_info = pd.read_csv(\"optuna/optuna_lgbm_info.csv\")\n",
    "    profit_objects = round(oof_conf_obj_num * (2 * oof_conf_score - 1))\n",
    "\n",
    "    # if current result is better than the best result - compare scores of current model \n",
    "    # with scores of the best model using t-test for dependent samples\n",
    "    # if p-value is less than alpha - than the result is significant and we can use it\n",
    "    # else set result as best_result - 1\n",
    "    if df_optuna_more_info.shape[0] > 0:\n",
    "        best_result, best_scores = df_optuna_more_info.query(\"value == value.max()\")[[\"value\", \"values\"]].values[0]\n",
    "        best_scores = best_scores[1:-1].split(\", \")\n",
    "        best_scores = [float(b) for b in best_scores]\n",
    "        if result > best_result:\n",
    "            p_value = ttest_rel(best_scores, results).pvalue\n",
    "            if p_value >= CFG.optimize_alpha:\n",
    "                print(f\"avg conf score {result} is better than best score {best_result}, \"\n",
    "                      f\"but p-value {p_value} is more than alpha {CFG.optimize_alpha}\")\n",
    "                # increase result but discount the difference betweeen current result\n",
    "                # and previous best result on (1 - p_value) \n",
    "                result = best_result + (result - best_result) * (1 - p_value)\n",
    "    \n",
    "    tmp = pd.DataFrame({\n",
    "                        \"value\": [result], \n",
    "                        \"total_conf_score\": [oof_conf_score],\n",
    "                        \"values\": [(results)], \n",
    "                        \"profit_objects\": [profit_objects],\n",
    "                        \"total_conf_obj_num\": [oof_conf_obj_num] \n",
    "                        })\n",
    "    \n",
    "    # save new data to dataframe with additional information\n",
    "    df_optuna_more_info = pd.concat([df_optuna_more_info, tmp])\n",
    "    df_optuna_more_info.to_csv(\"optuna/optuna_lgbm_info.csv\", index=False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / optimize process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-20 11:54:19,523] A new study created in memory with name: no-name-760c5f78-d38e-4aea-94b7-b715707447a1\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"n_estimators\": 964,\n",
    "    \"learning_rate\": 0.003568,\n",
    "    \"reg_alpha\": 0.002782,\n",
    "    \"reg_lambda\": 0.00005,\n",
    "    \"max_depth\": 4,\n",
    "    \"num_leaves\": 16,\n",
    "    \"colsample_bytree\": 0.431844,\n",
    "    \"max_bin\": 135,\n",
    "    \"is_unbalance\": True,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"subsample\": 0.673918,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"objective\": \"binary\",\n",
    "    \"importance_type\": \"gain\",\n",
    "    \"metric\": \"average_precison\",\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "# params = {\n",
    "#     \"boosting_type\": \"dart\",\n",
    "#     \"n_estimators\": 1269,\n",
    "#     \"learning_rate\": 0.004870260614106017,\n",
    "#     \"reg_alpha\": 0.000140973748579361,\n",
    "#     \"reg_lambda\": 1.376889751304579e-07,\n",
    "#     \"max_depth\": 6,\n",
    "#     \"num_leaves\": 417,\n",
    "#     \"colsample_bytree\": 0.7348495404154777,\n",
    "#     \"max_bin\": 209,\n",
    "#     \"is_unbalance\": False,\n",
    "#     \"class_weight\": \"balanced\",\n",
    "#     \"subsample\": 0.8688384439011413,\n",
    "#     \"subsample_freq\": 1,\n",
    "#     \"objective\": \"binary\",\n",
    "#     \"importance_type\": \"gain\",\n",
    "#     \"metric\": \"average_precison\",\n",
    "#     \"verbosity\": -1\n",
    "# }\n",
    "\n",
    "train_test = \"fold\"         # fold - train and validate data on TSS fold scheme\n",
    "                            # inference - train model on all available data and save it\n",
    "                            # test - test model in production on the new data\n",
    "max_train_size = 0.640379\t\n",
    "# max_train_size = 0.8176437724917641\n",
    "\n",
    "# set high and low bound for model predictions\n",
    "# p > high_bound -> 1, p < low_bound -> 0\n",
    "high_bound = 0.53726\n",
    "# high_bound = 0.5480965958942431\n",
    "\n",
    "low_bound = 0\n",
    "\n",
    "# add object weights\n",
    "train_df[\"weight\"] = train_df[\"time\"].astype(np.int64) / int(1e6)\n",
    "train_df[\"weight\"] = (train_df[\"weight\"].max() - train_df[\"weight\"]) / (train_df[\"weight\"].max() - train_df[\"weight\"].min()) * np.pi / 2\n",
    "train_df[\"weight\"] = np.cos(train_df[\"weight\"])\n",
    "sample_weight = True\n",
    "\n",
    "\n",
    "if CFG.optimize:\n",
    "    df_optuna_more_info = pd.DataFrame(columns=[\"value\", \"oof_conf_score\", \"profit_objects\", \n",
    "                                                \"oof_val_score\", \"oof_conf_obj_num\", \"oof_conf_obj_pct\"])\n",
    "    df_optuna_more_info.to_csv(\"optuna/optuna_lgbm_info.csv\", index=False)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=750)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    df_optuna = study.trials_dataframe()\n",
    "    df_optuna = df_optuna.sort_values(\"value\", ascending=False)\n",
    "    df_optuna.to_csv(f\"optuna/optuna_lgbm.csv\")\n",
    "\n",
    "    display(df_optuna.head(10))\n",
    "elif train_test == \"fold\":\n",
    "    model, conf_scores, conf_object_nums, oof, val_idxs = model_train(train_df, features, params, sample_weight, \n",
    "                                                                      n_folds=8, low_bound=low_bound, \n",
    "                                                                      high_bound=high_bound, train_test=\"fold\", \n",
    "                                                                      max_train_size=max_train_size, verbose=True)\n",
    "    y = train_df[\"target\"][val_idxs]\n",
    "    oof = oof[val_idxs]\n",
    "    oof_conf_score, oof_conf_obj_num, oof_conf_obj_pct = conf_ppv_npv_acc_score(y, oof, low_bound, high_bound)\n",
    "    \n",
    "    # oof_conf_obj_num = round(np.mean(conf_object_nums))\n",
    "    # oof_conf_score = sum([score * object_num for score, object_num in zip(conf_scores, conf_object_nums)]) / sum(conf_object_nums)\n",
    "\n",
    "    print(80 * \"=\")\n",
    "    print(f\"Total confident object score: {oof_conf_score}\\n\"\n",
    "          f\"Total number of confident objects {oof_conf_obj_num}\\n\"\n",
    "          f\"Total number of profitable objects: {round((2 * oof_conf_score -  1) * oof_conf_obj_num)}\")\n",
    "    \n",
    "    results = [conf_object_num * (conf_score - CFG.min_precision) * 100 for conf_object_num, conf_score in zip(conf_object_nums, conf_scores)]\n",
    "    print(f\"Results: {results}\")\n",
    "elif train_test == \"inference\": \n",
    "    model, _, _, _, _ = model_train(train_df, features, params, sample_weight, \n",
    "                                    n_folds=8, low_bound=low_bound, \n",
    "                                    high_bound=high_bound, train_test=\"inference\", \n",
    "                                    max_train_size=max_train_size, verbose=False)\n",
    "    joblib.dump(model, f\"model/lgbm.pkl\")\n",
    "    # save feature dictionary for further inference\n",
    "    with open(f\"model/features.json\", \"w\") as f:\n",
    "        json.dump(feature_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test result of the new model against the result of the old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSS fold results with sample weight\n",
    "a =  [2432.5000000000005, -1047.4999999999998, -1624.9999999999995, 1867.5, 2687.5000000000014, 3915.000000000002, 7850.000000000002, 1050.000000000001]\n",
    "# TSS fold results with threshold 0.5\n",
    "b =  [3512.5, -4344.999999999999, -1599.9999999999957, 2172.5, 11310.000000000004, 885.0000000000025, 8282.5, 157.49999999999997]\n",
    "\n",
    "ttest_rel(a, b, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.optimize and train_test == \"fold\":\n",
    "    sns.lineplot(x=list(range(len(conf_scores))), y=conf_scores, linewidth=2)\n",
    "\n",
    "    plt.title(\"Model score by folds\")\n",
    "    plt.xlabel(\"Folds\")\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtest model\n",
    "\n",
    "I don't consider funding fees because the can be both positive and negative. I can do it because typical funding fee is positive and majority of my trade are shorts, so I can even earn on funding. But I think it's better not to consider it at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, backtest_df = backtest(train_df, oof, val_idxs, high_bound, show_progress=True)\n",
    "    \n",
    "# plot the balance\n",
    "backtest_plot = backtest_df[\"balance\"].reset_index(drop=True)\n",
    "backtest_plot = backtest_plot * 100\n",
    "\n",
    "print(f\"Result balance increase is {result}%\")\n",
    "\n",
    "plt.xlabel('Number of trades')\n",
    "plt.ylabel('Balance, %')\n",
    "plt.title('Trade balance vs Number of trades')\n",
    "backtest_plot.plot(linewidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of model prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.optimize and train_test == \"fold\":\n",
    "    sns.displot(oof)\n",
    "\n",
    "    plt.xlabel(\"Model prediction scores\")\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.ylabel(\"Number of predictions\")\n",
    "    plt.yticks(fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display PR curve for fold predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "if not CFG.optimize and train_test == \"fold\":\n",
    "    disp = PrecisionRecallDisplay.from_predictions(\n",
    "        y.values, oof, name=\"PR AUC\"\n",
    "    )\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    _ = disp.ax_.set_title(\"2-class Precision-Recall curve\")\n",
    "    disp.ax_.lines[0].set_linewidth(2)\n",
    "\n",
    "# vol 1e6 AP=0.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best threshold for fold predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "\n",
    "if not CFG.optimize and train_test == \"fold\":\n",
    "    figsize = (10, 5)\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    score_list = list()\n",
    "    obj_num_list = list()\n",
    "    obj_pct_list = list()\n",
    "    obj_profit_list = list()\n",
    "    max_obj_profit = 0\n",
    "    for hb in np.arange(0.41, 0.61, 0.001):\n",
    "        score, obj_num, obj_pct = conf_ppv_npv_acc_score(y.reset_index(drop=True), oof, 0, hb)\n",
    "        if score == 0:\n",
    "            obj_num = 0\n",
    "            obj_pct = 0\n",
    "        bound, score, obj_num, obj_pct = round(hb, 4), round(score, 5), round(obj_num, 2), round(obj_pct, 2)\n",
    "        obj_profit = round((2 * score - 1) * obj_num)\n",
    "        score_list.append(score)\n",
    "        obj_num_list.append(obj_num)\n",
    "        obj_pct_list.append(obj_pct)\n",
    "        obj_profit_list.append(obj_profit)\n",
    "        max_obj_profit = max(max_obj_profit, obj_profit)\n",
    "        ic(bound, score, obj_num, obj_pct, obj_profit)\n",
    "\n",
    "    obj_profit_list = [o / max_obj_profit for o in obj_profit_list]\n",
    "    line1 = plt.plot(np.arange(0.41, 0.61, 0.001), score_list, label=\"precison score\", linewidth=2)\n",
    "    line2 = plt.plot(np.arange(0.41, 0.61, 0.001), obj_pct_list, label=\"object pct\", linewidth=2)\n",
    "    line3 = plt.plot(np.arange(0.41, 0.61, 0.001), obj_profit_list, label=\"number of profit objects\", linewidth=2)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model\n",
    "\n",
    "### Test model predictions on the new data\n",
    "\n",
    "List of possible reasons of inconsistence between real and train predictions of tickers\n",
    "\n",
    "- ticker price doesn't reach the cls_target_ratio_tp or cls_target_ratio_sl thresholds yet\n",
    "\n",
    "- errors in the data preparation pipeline make model give different predictions\n",
    "\n",
    "- bot was used in debug mode\n",
    "\n",
    "- too small volume\n",
    "\n",
    "- ticker is not presented in bybit_ticker_list due to errors\n",
    "\n",
    "- ticker was added to exchange not too much time ago\n",
    "\n",
    "- different high_bound threshold\n",
    "\n",
    "- there are NaNs in the ticker dataframe\n",
    "\n",
    "- signal was one of the last in the dataframe and was deleted due to signal error protection measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.optimize:\n",
    "    if train_test == \"test\":\n",
    "        model = joblib.load(\"model/lgbm.pkl\")\n",
    "        with open(f\"model/features.json\", \"r\") as f:\n",
    "            features = json.load(f)[\"features\"]\n",
    "\n",
    "    with open(f\"model/bybit_tickers.json\", \"r\") as f:\n",
    "        bybit_tickers = json.load(f)\n",
    "\n",
    "    X = train_df[(train_df[\"ticker\"].isin(bybit_tickers))]\n",
    "    X[\"pred_proba\"] = model.predict_proba(X[features])[:,1]\n",
    "    res = X.loc[((X[\"pred_proba\"] >= high_bound)) & (X[\"time\"] > CFG.last_date), [\"time\", \"ticker\", \"ttype\", \"pred_proba\", \"target\", \"pattern\"]]\n",
    "    res = res.reset_index(drop=True)\n",
    "    display(res[\"time\"].min(), res[\"time\"].max())\n",
    "    display(res[\"target\"].value_counts())\n",
    "    display(res[\"target\"].value_counts(normalize=True))\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model, importance_type=\"gain\", figsize=(5, 24), title=\"LightGBM Feature Importance (Gain)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot rolling mean ppv_npv_acc score of the model predictions for the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.optimize:\n",
    "    figsize = (12, 5)\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    tmp = res[res[\"target\"] == 1]\n",
    "    \n",
    "    if len(tmp) > 0:\n",
    "        tmp[\"ppv_npv_acc\"] = tmp[\"target\"].rolling(len(tmp), min_periods=1).count() / (tmp.index + 1)\n",
    "\n",
    "        ax = sns.lineplot(x=tmp[\"time\"], y=tmp[\"ppv_npv_acc\"].values)\n",
    "        ax.lines[0].set_linewidth(2)\n",
    "        plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare indicator / signal values for bot and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from signals.find_signal import SignalFactory\n",
    "\n",
    "# ttype = \"sell\"\n",
    "# ticker = \"BADGERUSDT\"\n",
    "# month = 7\n",
    "# day = 15\n",
    "# hour = 23\n",
    "# configs = ConfigFactory.factory(environ).configs\n",
    "\n",
    "# x = pd.read_csv(f\"../bot/ticker_dataframes/{ticker}_1h_{ttype}_{month}_{day}_{hour}.csv\").drop(columns=[\"Unnamed: 0\"]).reset_index(drop=True)\n",
    "# y = pd.read_csv(f\"../bot/ticker_dataframes/{ticker}_4h_{ttype}_{month}_{day}_{hour}.csv\").drop(columns=[\"Unnamed: 0\"]).reset_index(drop=True)\n",
    "\n",
    "# # add Volume24\n",
    "# vol24 = indicators.Volume24(ttype, configs)\n",
    "# x = vol24.get_indicator(x, \"\", \"1h\", 0)\n",
    "# # add Pattern\n",
    "# pattern = indicators.Pattern(ttype, configs)\n",
    "# x = pattern.get_indicator(x, \"\", \"\", 0)\n",
    "# # add trend\n",
    "# trend = indicators.Trend(ttype, configs)\n",
    "# y = trend.get_indicator(y, \"\", \"\", 0)\n",
    "\n",
    "# # cols = [\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"rsi\", \"stoch_slowk\", \"stoch_slowd\", \"linear_reg\", \"linear_reg_angle\", \"macd\", \"macdsignal\", \"macdhist\"]\n",
    "# cols = [\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"linear_reg\", \"linear_reg_angle\", \"high_max\", \"low_min\", \"volume_24\"]\n",
    "\n",
    "# higher_features = [\"time_4h\", \"linear_reg\", \"linear_reg_angle\", \"macd\", \"macdhist\",  \"macd_dir\", \n",
    "#                    \"macdsignal\", \"macdsignal_dir\"]\n",
    "# x[\"time\"] = pd.to_datetime(x[\"time\"])\n",
    "# y[\"time\"] = pd.to_datetime(y[\"time\"])\n",
    "# y[\"time_4h\"] = y[\"time\"] + pd.to_timedelta(3, unit=\"h\")\n",
    "# x[[\"time\"] + higher_features] = pd.merge(x[[\"time\"]], y[higher_features], how=\"left\", left_on=\"time\", right_on=\"time_4h\")\n",
    "\n",
    "# # x = x.drop(columns=[\"time_4h\"])\n",
    "# # y = y.drop(columns=[\"time_4h\"])\n",
    "# x = x.ffill()\n",
    "# x = x.reset_index(drop=True)\n",
    "\n",
    "# # get Swing pattern\n",
    "# pattern = SignalFactory().factory(\"Pattern\", ttype, configs)\n",
    "# pattern_points = pattern.find_signal(x)\n",
    "# trend = SignalFactory().factory(\"Trend\", ttype, configs)\n",
    "# trend_points = trend.find_signal(x)\n",
    "# idxs = np.where((pattern_points > 0) & (trend_points > 0))\n",
    "# display(x.loc[idxs[0], cols])\n",
    "\n",
    "# z = pd.read_pickle(f\"../optimizer/ticker_dataframes/{ticker}_1h.pkl\")\n",
    "# v = pd.read_pickle(f\"../optimizer/ticker_dataframes/{ticker}_4h.pkl\")\n",
    "\n",
    "# # add Volume24\n",
    "# vol24 = indicators.Volume24(ttype, configs)\n",
    "# z = vol24.get_indicator(z, \"\", \"1h\", 0)\n",
    "# # add Pattern\n",
    "# pattern = indicators.Pattern(ttype, configs)\n",
    "# z = pattern.get_indicator(z, \"\", \"\", 0)\n",
    "# # add Trend\n",
    "# trend = indicators.Trend(ttype, configs)\n",
    "# v = trend.get_indicator(v, \"\", \"\", 0)\n",
    "# z.tail(48)\n",
    "\n",
    "# v[\"time_4h\"] = v[\"time\"] + pd.to_timedelta(3, unit=\"h\")\n",
    "# z[[\"time\"] + higher_features] = pd.merge(z[[\"time\"]], y[higher_features], how=\"left\", left_on=\"time\", right_on=\"time_4h\")\n",
    "\n",
    "# z = z.drop(columns=[\"time_4h\"])\n",
    "# v = v.drop(columns=[\"time_4h\"])\n",
    "# z = z.ffill()\n",
    "# z = z.reset_index(drop=True)\n",
    "\n",
    "# # get Swing pattern\n",
    "# pattern = SignalFactory().factory(\"Pattern\", ttype, configs)\n",
    "# pattern_points = pattern.find_signal(z)\n",
    "# trend = SignalFactory().factory(\"Trend\", ttype, configs)\n",
    "# trend_points = trend.find_signal(z)\n",
    "# idxs = np.where((pattern_points > 0) & (trend_points > 0))\n",
    "# display(z.loc[idxs[0], cols])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
