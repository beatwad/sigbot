{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import log_loss, mean_squared_error, precision_score\n",
    "\n",
    "import torch\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    select_features = True\n",
    "    train_NN = False\n",
    "    train_LGBM = True\n",
    "    n_repeats = 1\n",
    "    n_folds = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and add indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>stoch_slowk_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>linear_reg_angle_prev_96</th>\n",
       "      <th>macd_prev_96</th>\n",
       "      <th>macdsignal_prev_96</th>\n",
       "      <th>macdhist_prev_96</th>\n",
       "      <th>macd_dir_prev_96</th>\n",
       "      <th>macdsignal_dir_prev_96</th>\n",
       "      <th>atr_prev_96</th>\n",
       "      <th>close_smooth_prev_96</th>\n",
       "      <th>target</th>\n",
       "      <th>ttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-18 23:00:00</td>\n",
       "      <td>23.46</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.44</td>\n",
       "      <td>23.62</td>\n",
       "      <td>3346.24</td>\n",
       "      <td>43.727673</td>\n",
       "      <td>25.327885</td>\n",
       "      <td>24.045347</td>\n",
       "      <td>-0.007431</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.562901</td>\n",
       "      <td>0.035983</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>-0.071544</td>\n",
       "      <td>-0.288045</td>\n",
       "      <td>-0.100355</td>\n",
       "      <td>0.188287</td>\n",
       "      <td>23.758333</td>\n",
       "      <td>1</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-26 01:00:00</td>\n",
       "      <td>24.15</td>\n",
       "      <td>24.15</td>\n",
       "      <td>23.76</td>\n",
       "      <td>23.84</td>\n",
       "      <td>13772.65</td>\n",
       "      <td>64.054892</td>\n",
       "      <td>83.517327</td>\n",
       "      <td>87.090858</td>\n",
       "      <td>-0.029809</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.369665</td>\n",
       "      <td>-0.020052</td>\n",
       "      <td>-0.052992</td>\n",
       "      <td>0.032940</td>\n",
       "      <td>-0.201703</td>\n",
       "      <td>-0.103966</td>\n",
       "      <td>0.133742</td>\n",
       "      <td>22.383333</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-26 05:00:00</td>\n",
       "      <td>24.04</td>\n",
       "      <td>24.10</td>\n",
       "      <td>23.98</td>\n",
       "      <td>24.03</td>\n",
       "      <td>8515.96</td>\n",
       "      <td>68.293118</td>\n",
       "      <td>54.118435</td>\n",
       "      <td>61.220840</td>\n",
       "      <td>-0.108235</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.869113</td>\n",
       "      <td>-0.017024</td>\n",
       "      <td>-0.033512</td>\n",
       "      <td>0.016488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.106127</td>\n",
       "      <td>0.121932</td>\n",
       "      <td>22.381250</td>\n",
       "      <td>1</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-06 09:00:00</td>\n",
       "      <td>23.06</td>\n",
       "      <td>23.24</td>\n",
       "      <td>23.05</td>\n",
       "      <td>23.14</td>\n",
       "      <td>68509.23</td>\n",
       "      <td>23.173086</td>\n",
       "      <td>9.373380</td>\n",
       "      <td>10.391639</td>\n",
       "      <td>-0.032943</td>\n",
       "      <td>...</td>\n",
       "      <td>2.351254</td>\n",
       "      <td>0.015162</td>\n",
       "      <td>-0.003651</td>\n",
       "      <td>0.018813</td>\n",
       "      <td>10.982025</td>\n",
       "      <td>-0.386205</td>\n",
       "      <td>0.190773</td>\n",
       "      <td>25.949167</td>\n",
       "      <td>0</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-06 11:00:00</td>\n",
       "      <td>23.13</td>\n",
       "      <td>23.64</td>\n",
       "      <td>22.82</td>\n",
       "      <td>23.47</td>\n",
       "      <td>195485.06</td>\n",
       "      <td>33.589815</td>\n",
       "      <td>17.450236</td>\n",
       "      <td>12.459531</td>\n",
       "      <td>0.221789</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.240717</td>\n",
       "      <td>-0.018542</td>\n",
       "      <td>-0.006710</td>\n",
       "      <td>-0.011832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075655</td>\n",
       "      <td>0.200640</td>\n",
       "      <td>25.909167</td>\n",
       "      <td>1</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time   open   high    low  close     volume        rsi  \\\n",
       "0 2022-10-18 23:00:00  23.46  23.70  23.44  23.62    3346.24  43.727673   \n",
       "1 2022-10-26 01:00:00  24.15  24.15  23.76  23.84   13772.65  64.054892   \n",
       "2 2022-10-26 05:00:00  24.04  24.10  23.98  24.03    8515.96  68.293118   \n",
       "3 2022-11-06 09:00:00  23.06  23.24  23.05  23.14   68509.23  23.173086   \n",
       "4 2022-11-06 11:00:00  23.13  23.64  22.82  23.47  195485.06  33.589815   \n",
       "\n",
       "   stoch_slowk  stoch_slowd  stoch_slowk_dir  ...  linear_reg_angle_prev_96  \\\n",
       "0    25.327885    24.045347        -0.007431  ...                 -6.562901   \n",
       "1    83.517327    87.090858        -0.029809  ...                -10.369665   \n",
       "2    54.118435    61.220840        -0.108235  ...                -11.869113   \n",
       "3     9.373380    10.391639        -0.032943  ...                  2.351254   \n",
       "4    17.450236    12.459531         0.221789  ...                 -4.240717   \n",
       "\n",
       "   macd_prev_96  macdsignal_prev_96  macdhist_prev_96  macd_dir_prev_96  \\\n",
       "0      0.035983            0.107527         -0.071544         -0.288045   \n",
       "1     -0.020052           -0.052992          0.032940         -0.201703   \n",
       "2     -0.017024           -0.033512          0.016488          0.000000   \n",
       "3      0.015162           -0.003651          0.018813         10.982025   \n",
       "4     -0.018542           -0.006710         -0.011832          0.000000   \n",
       "\n",
       "   macdsignal_dir_prev_96  atr_prev_96  close_smooth_prev_96  target  ttype  \n",
       "0               -0.100355     0.188287             23.758333       1   sell  \n",
       "1               -0.103966     0.133742             22.383333       1    buy  \n",
       "2               -0.106127     0.121932             22.381250       1    buy  \n",
       "3               -0.386205     0.190773             25.949167       0   sell  \n",
       "4                0.075655     0.200640             25.909167       1   sell  \n",
       "\n",
       "[5 rows x 505 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(12528, 505)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# last previous data point to collect for model training (value represents number of hours before signal point)\n",
    "last = 96\n",
    "\n",
    "df = pd.read_pickle(f'signal_stat/train_df_{last}.pkl')\n",
    "\n",
    "display(df.head())\n",
    "display(df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split by ticker group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.1, n_splits=2, random_state = 7)\n",
    "\n",
    "split = splitter.split(df, groups=df['ticker'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_df = df.iloc[train_inds]\n",
    "test_df = df.iloc[test_inds]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_size=0.2\n",
    "\n",
    "x_data = train_df.drop(['target', 'time', 'ticker', 'pattern', 'ttype'], axis=1)\n",
    "y_data = train_df['target']\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=True)\n",
    "input_channels = x_data.shape[1]\n",
    "output_size = len(set(y_data))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train[x_train.columns] = scaler.fit_transform(x_train)\n",
    "x_valid[x_valid.columns] = scaler.transform(x_valid)\n",
    "\n",
    "x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "x_valid = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "\n",
    "display(type(x_train), type(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n"
     ]
    }
   ],
   "source": [
    "# find available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigModel(torch.nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(SigModel, self, ).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.add_module('lin1', torch.nn.Linear(input_channels, 64))\n",
    "        self.layers.add_module('relu1', torch.nn.ReLU())\n",
    "        self.layers.add_module('do1', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin2', torch.nn.Linear(64, 128))\n",
    "        self.layers.add_module('relu2', torch.nn.ReLU())\n",
    "        self.layers.add_module('do2', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin3', torch.nn.Linear(128, 96))\n",
    "        self.layers.add_module('relu3', torch.nn.ReLU())\n",
    "        self.layers.add_module('do3', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin4', torch.nn.Linear(96, 32))\n",
    "        self.layers.add_module('relu4', torch.nn.ReLU())\n",
    "        self.layers.add_module('do4', torch.nn.Dropout(p=0.25))\n",
    "        self.layers.add_module('lin5', torch.nn.Linear(32, 1))\n",
    "        self.layers.add_module('sigmoid', torch.nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# train function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_loader\n",
    "\n",
    "    # get output of the model\n",
    "    train_preds = model(x_train).squeeze()\n",
    "    # calculate train loss\n",
    "    train_loss = criterion(train_preds, y_train)\n",
    "    train_acc = (train_preds.round() == y_train).float().mean()\n",
    "    \n",
    "    # set gradient to zero to prevent it accumulation\n",
    "    optimizer.zero_grad() # ~ model.zero_grad()\n",
    "    # calculate gradient\n",
    "    train_loss.backward() \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(x_valid).squeeze()\n",
    "        val_loss = criterion(val_preds, y_valid)\n",
    "        val_acc = (val_preds.round() == y_valid).float().mean()\n",
    "    \n",
    "    # update weights according to gradient value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# Initialize model\n",
    "if CFG.train_NN:\n",
    "    model = SigModel(input_channels).to(device)\n",
    "\n",
    "    # Number of epochs\n",
    "    epochs = 100000\n",
    "\n",
    "    # Send data to the device\n",
    "    x_train, x_valid = x_train.to(device), x_valid.to(device)\n",
    "    y_train, y_valid = y_train.to(device), y_valid.to(device)\n",
    "    train_loader = x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    # Empty loss lists to track values\n",
    "    epoch_count, train_loss_values, valid_loss_values = [], [], []\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    learning_rate = 1e-6\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2000, threshold=1e-2)\n",
    "\n",
    "    # Loop through the data\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc, val_loss, val_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "        # Print progress a total of 20 times\n",
    "        if epoch % int(epochs / 20) == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.5f}\\\n",
    "                Validation Loss: {val_loss:.5f}, Val Acc: {val_acc:.5f}\\\n",
    "                    LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]}')\n",
    "\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(train_loss.cpu().detach().numpy())\n",
    "            valid_loss_values.append(val_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NN train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CFG.train_NN:\n",
    "    plt.plot(epoch_count, train_loss_values, label='Training Loss')\n",
    "    plt.plot(epoch_count, valid_loss_values, label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss Curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat \u001b[1m\u001b[34m#1\n",
      "[100]\ttraining's binary_logloss: 0.651038\tvalid_1's binary_logloss: 0.679076\n",
      "[200]\ttraining's binary_logloss: 0.632084\tvalid_1's binary_logloss: 0.675079\n",
      "[300]\ttraining's binary_logloss: 0.612133\tvalid_1's binary_logloss: 0.67211\n",
      "[400]\ttraining's binary_logloss: 0.59403\tvalid_1's binary_logloss: 0.670357\n",
      "[500]\ttraining's binary_logloss: 0.575251\tvalid_1's binary_logloss: 0.669354\n",
      "[600]\ttraining's binary_logloss: 0.5672\tvalid_1's binary_logloss: 0.669277\n",
      "[700]\ttraining's binary_logloss: 0.55283\tvalid_1's binary_logloss: 0.669212\n",
      "[800]\ttraining's binary_logloss: 0.543162\tvalid_1's binary_logloss: 0.66903\n",
      "[900]\ttraining's binary_logloss: 0.529085\tvalid_1's binary_logloss: 0.667802\n",
      "[1000]\ttraining's binary_logloss: 0.519267\tvalid_1's binary_logloss: 0.667077\n",
      "[1100]\ttraining's binary_logloss: 0.507892\tvalid_1's binary_logloss: 0.666785\n",
      "[1200]\ttraining's binary_logloss: 0.50051\tvalid_1's binary_logloss: 0.666186\n",
      "[1300]\ttraining's binary_logloss: 0.491103\tvalid_1's binary_logloss: 0.665723\n",
      "[1400]\ttraining's binary_logloss: 0.478078\tvalid_1's binary_logloss: 0.664973\n",
      "[1500]\ttraining's binary_logloss: 0.472152\tvalid_1's binary_logloss: 0.664532\n",
      "[1600]\ttraining's binary_logloss: 0.461106\tvalid_1's binary_logloss: 0.664123\n",
      "[1700]\ttraining's binary_logloss: 0.449968\tvalid_1's binary_logloss: 0.663987\n",
      "[1800]\ttraining's binary_logloss: 0.440943\tvalid_1's binary_logloss: 0.663764\n",
      "[1900]\ttraining's binary_logloss: 0.431684\tvalid_1's binary_logloss: 0.663558\n",
      "[2000]\ttraining's binary_logloss: 0.42391\tvalid_1's binary_logloss: 0.663614\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| loss: \u001b[1m\u001b[34m0.66361\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.651163\tvalid_1's binary_logloss: 0.676226\n",
      "[200]\ttraining's binary_logloss: 0.632748\tvalid_1's binary_logloss: 0.672208\n",
      "[300]\ttraining's binary_logloss: 0.613151\tvalid_1's binary_logloss: 0.669462\n",
      "[400]\ttraining's binary_logloss: 0.595188\tvalid_1's binary_logloss: 0.66711\n",
      "[500]\ttraining's binary_logloss: 0.576889\tvalid_1's binary_logloss: 0.665948\n",
      "[600]\ttraining's binary_logloss: 0.569402\tvalid_1's binary_logloss: 0.665516\n",
      "[700]\ttraining's binary_logloss: 0.55542\tvalid_1's binary_logloss: 0.66472\n",
      "[800]\ttraining's binary_logloss: 0.545927\tvalid_1's binary_logloss: 0.663906\n",
      "[900]\ttraining's binary_logloss: 0.532436\tvalid_1's binary_logloss: 0.662965\n",
      "[1000]\ttraining's binary_logloss: 0.522448\tvalid_1's binary_logloss: 0.662364\n",
      "[1100]\ttraining's binary_logloss: 0.510921\tvalid_1's binary_logloss: 0.661517\n",
      "[1200]\ttraining's binary_logloss: 0.503837\tvalid_1's binary_logloss: 0.66164\n",
      "[1300]\ttraining's binary_logloss: 0.494583\tvalid_1's binary_logloss: 0.661473\n",
      "[1400]\ttraining's binary_logloss: 0.481864\tvalid_1's binary_logloss: 0.660768\n",
      "[1500]\ttraining's binary_logloss: 0.475981\tvalid_1's binary_logloss: 0.660064\n",
      "[1600]\ttraining's binary_logloss: 0.465568\tvalid_1's binary_logloss: 0.659213\n",
      "[1700]\ttraining's binary_logloss: 0.454484\tvalid_1's binary_logloss: 0.658739\n",
      "[1800]\ttraining's binary_logloss: 0.445732\tvalid_1's binary_logloss: 0.658234\n",
      "[1900]\ttraining's binary_logloss: 0.43591\tvalid_1's binary_logloss: 0.657971\n",
      "[2000]\ttraining's binary_logloss: 0.427894\tvalid_1's binary_logloss: 0.657454\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| loss: \u001b[1m\u001b[34m0.65745\u001b[0m| Best iteration: \u001b[1m\u001b[34m   0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    }
   ],
   "source": [
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "def lgbm_tuning(df, permut=False, boruta=False):\n",
    "    features = [c for c in df.columns if c not in ['time', 'target', 'ticker', 'pattern', 'ttype']]\n",
    "    groups = df['ticker']\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        if task_type == 'cls':\n",
    "            y_fold = df['target']\n",
    "            kf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=180820231)\n",
    "            eval_metric = 'logloss'\n",
    "        else:\n",
    "            y_fold = (df['target'] - df['close']) / df['close']\n",
    "            kf = GroupKFold(n_splits=CFG.n_folds)\n",
    "            eval_metric = 'mse'\n",
    "\n",
    "        X, y = df[features], y_fold\n",
    "        oof = np.zeros(len(df))\n",
    "        models_ = [] # Used to store models trained in the inner loop.\n",
    "        \n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "            else:\n",
    "                clf = lgb.LGBMRegressor(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=eval_metric, \n",
    "                    callbacks=[lgb.log_evaluation(100)])\n",
    "\n",
    "            models_.append(clf)\n",
    "\n",
    "            if task_type == 'cls':\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                val_score = log_loss(y_val, val_preds)\n",
    "            else:\n",
    "                val_preds = clf.predict(X_val)\n",
    "                val_score = mean_squared_error(y_val, val_preds, squared=False)\n",
    "            \n",
    "            oof[val_idx] = val_preds\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(f'Fold: {blu}{fold + 1:>3}{res}| loss: {blu}{val_score:.5f}{res}| Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # permutation importance\n",
    "            if permut:\n",
    "                perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                             random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                    index=X_val.columns).sort_index()\n",
    "\n",
    "                if perm_df_.shape[0] == 0:\n",
    "                    perm_df_ = perm_importance_df.copy()\n",
    "                else:\n",
    "                    perm_df_ += perm_importance_df\n",
    "\n",
    "            # gboost feature importance\n",
    "            f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                      reverse=True, key=lambda x: x[1]), \n",
    "                                columns=['Value','Feature'])\n",
    "\n",
    "            if feature_importances_.shape[0] == 0:\n",
    "                feature_importances_ = f_i.copy()\n",
    "            else:\n",
    "                feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "            # BORUTA importance\n",
    "            if boruta:\n",
    "                model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                try:\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                            eval_metric=eval_metric, \n",
    "                            callbacks=[lgb.log_evaluation(100)])\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "                \n",
    "                boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                        index=X_train.columns).sort_index()\n",
    "                if boruta_df_.shape[0] == 0:\n",
    "                    boruta_df_ = boruta_importance_df.copy()\n",
    "                else:\n",
    "                    boruta_df_ += boruta_importance_df\n",
    "\n",
    "        if task_type == 'cls':\n",
    "            outer_cv = log_loss(y, oof)\n",
    "        else:\n",
    "            outer_cv = mean_squared_error(y, oof, squared=False)\n",
    "        \n",
    "        outer_cv_score.append(outer_cv)\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} log_loss: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        perm_df_ = perm_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "        boruta_df_ = boruta_df_.reset_index().rename({'index': 'Feature'}, axis=1)\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(outer_cv_score)\n",
    "\n",
    "\n",
    "params = {\n",
    "          'n_estimators': 2000,\n",
    "          'learning_rate': 0.02,\n",
    "        #   'early_stopping_round': 100,\n",
    "          'max_depth': 10,\n",
    "          'subsample' : 0.7,\n",
    "          'colsample_bytree': 0.85,\n",
    "          'num_leaves': 24,\n",
    "          'verbosity': -1,\n",
    "          'importance_type': 'gain',\n",
    "          'max_bin': 255,\n",
    "          'reg_alpha': 1e-6,\n",
    "          'reg_lambda': 1e-8\n",
    "        }\n",
    "\n",
    "task_type = 'cls'\n",
    "\n",
    "if task_type == 'cls':\n",
    "    params['boosting_type'] = 'dart'\n",
    "    params['objective'] = 'binary'\n",
    "else:\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "\n",
    "if CFG.select_features:\n",
    "    perm_df_, feature_importances_, boruta_df_, outer_cv_score = lgbm_tuning(df, permut=True, boruta=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine importances and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.select_features:\n",
    "    perm_df_['rank'] = perm_df_['importance'].rank(ascending=False)\n",
    "    boruta_df_['rank'] = boruta_df_['importance'].rank()\n",
    "    feature_importances_['rank'] = feature_importances_['Value'].rank(ascending=False)\n",
    "\n",
    "    fi = pd.concat([perm_df_[['Feature','rank']], boruta_df_[['Feature','rank']], feature_importances_[['Feature','rank']]])\n",
    "    fi = fi.groupby('Feature')['rank'].sum().sort_values().reset_index()\n",
    "    fi.to_csv('feature_importance.csv')\n",
    "else:\n",
    "    fi = pd.read_csv('feature_importance.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test split based on ticker groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on full data\n",
      "[100]\ttraining's binary_logloss: 0.656938\ttraining's average_precision: 0.765095\n",
      "[200]\ttraining's binary_logloss: 0.642633\ttraining's average_precision: 0.786316\n",
      "[300]\ttraining's binary_logloss: 0.627202\ttraining's average_precision: 0.815493\n",
      "[400]\ttraining's binary_logloss: 0.612746\ttraining's average_precision: 0.838319\n",
      "[500]\ttraining's binary_logloss: 0.597795\ttraining's average_precision: 0.864011\n",
      "[600]\ttraining's binary_logloss: 0.591616\ttraining's average_precision: 0.881408\n",
      "[700]\ttraining's binary_logloss: 0.580192\ttraining's average_precision: 0.895498\n",
      "[800]\ttraining's binary_logloss: 0.572037\ttraining's average_precision: 0.909923\n",
      "[900]\ttraining's binary_logloss: 0.561172\ttraining's average_precision: 0.92286\n",
      "[1000]\ttraining's binary_logloss: 0.553348\ttraining's average_precision: 0.933125\n",
      "[1100]\ttraining's binary_logloss: 0.544003\ttraining's average_precision: 0.941459\n",
      "[1200]\ttraining's binary_logloss: 0.538054\ttraining's average_precision: 0.948075\n",
      "[1300]\ttraining's binary_logloss: 0.530354\ttraining's average_precision: 0.954477\n",
      "[1400]\ttraining's binary_logloss: 0.519903\ttraining's average_precision: 0.961033\n",
      "[1500]\ttraining's binary_logloss: 0.514868\ttraining's average_precision: 0.965523\n",
      "[1600]\ttraining's binary_logloss: 0.505866\ttraining's average_precision: 0.970672\n",
      "[1700]\ttraining's binary_logloss: 0.497017\ttraining's average_precision: 0.97478\n",
      "[1800]\ttraining's binary_logloss: 0.489384\ttraining's average_precision: 0.979028\n",
      "[1900]\ttraining's binary_logloss: 0.481027\ttraining's average_precision: 0.982166\n",
      "[2000]\ttraining's binary_logloss: 0.474628\ttraining's average_precision: 0.984494\n",
      "[2100]\ttraining's binary_logloss: 0.464848\ttraining's average_precision: 0.986509\n",
      "[2200]\ttraining's binary_logloss: 0.457518\ttraining's average_precision: 0.98852\n"
     ]
    }
   ],
   "source": [
    "def model_train(df, train_df, test_df, features, task_type, how, n_folds, low_bound, high_bound, train_test): \n",
    "    X, groups = train_df[features], train_df['ticker']\n",
    "    X = pd.concat([X, pd.get_dummies(train_df['pattern'], drop_first=True)], axis=1)\n",
    "    y = train_df['target']\n",
    "    \n",
    "    if train_test == 'fold':\n",
    "        oof = np.zeros([train_df['target'].shape[0], 1])\n",
    "        \n",
    "        kf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=180820231)\n",
    "\n",
    "        oe_enc = OrdinalEncoder()\n",
    "        groups = oe_enc.fit_transform(groups.values.reshape(-1, 1))\n",
    "\n",
    "        print(f\"Training with {len(features)} features\")\n",
    "        \n",
    "        if how == 'lreg':\n",
    "            scaler = StandardScaler()\n",
    "            X[X.columns] = scaler.fit_transform(X)\n",
    "        \n",
    "        for fold, (fit_idx, val_idx) in enumerate(kf.split(X, y, groups)):\n",
    "            print(f'Fold #{fold + 1}')\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[fit_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[fit_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "            \n",
    "            models = list()\n",
    "            if how == 'lgbm':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                        eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "            elif how == 'lreg':\n",
    "                model = LogisticRegression(C=0.1, max_iter=100000)#, class_weight='balanced')\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_score = log_loss(y_val, val_preds)\n",
    "            prec_score, prec_obj_pct = confident_score(y_val, val_preds[:,1], low_bound, high_bound)\n",
    "            print(f'Logloss: {val_score}, Confident objects precision: {prec_score}, % of confident objects: {prec_obj_pct}')\n",
    "            oof[val_idx, 0] = val_preds[:,1]\n",
    "\n",
    "        return oof, model\n",
    "    elif train_test == 'full':\n",
    "        print('Train on full data')\n",
    "        X, y = df[features], df['target']\n",
    "        X = pd.concat([X, pd.get_dummies(df['pattern'], drop_first=True)], axis=1)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X, y, eval_set=[(X, y)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        return np.zeros([df.shape[0], 1]), model\n",
    "    else:\n",
    "        # fit model on full dataset and predict on test\n",
    "        print(\"Test fold\")\n",
    "        X_test, y_test = test_df[features], test_df['target']\n",
    "        X_test = pd.concat([X_test, pd.get_dummies(test_df['pattern'], drop_first=True)], axis=1)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X, y, eval_set=[(X_test, y_test)], eval_metric='logloss', callbacks = [lgb.log_evaluation(100)])\n",
    "        oof_test = np.zeros([test_df['target'].shape[0], 1])\n",
    "        oof_test[:,0] = model.predict_proba(X_test)[:,1]\n",
    "        return oof_test, model\n",
    "\n",
    "def prepare_features(fi, feature_num):\n",
    "    ''' Get features, sort them by their time appearance and return for using in train and inference'''\n",
    "    fi = fi['Feature'].head(feature_num)\n",
    "    feature_dict = defaultdict(list)\n",
    "    features = list()\n",
    "\n",
    "    for f in fi:\n",
    "        period = f.split('_')\n",
    "        if period[-1].isdigit():\n",
    "            feature_dict[int(period[-1])].append('_'.join(period[:-2]))\n",
    "        else:\n",
    "            feature_dict[0].append(f)\n",
    "\n",
    "    feature_dict = dict(sorted(feature_dict.items()))\n",
    "    \n",
    "    for item in feature_dict.items():\n",
    "        if item[0] > 0:\n",
    "            features.extend([i + f'_prev_{item[0]}' for i in item[1]])\n",
    "        else:\n",
    "            features.extend([i for i in item[1]])\n",
    "\n",
    "    feature_dict['features'] = features + ['Pattern_Trend', 'STOCH_RSI']\n",
    "\n",
    "    return features, feature_dict\n",
    "\n",
    "def confident_score(y, oof, low_bound, high_bound):\n",
    "    ''' Consider only high confident objects for accuracy and precision scores calculation;\n",
    "        object probability must be lower than low_bound or higher than high_bound '''\n",
    "    pred_conf = np.zeros_like(oof)\n",
    "    pred_conf[oof > high_bound] = 1\n",
    "    pred_conf[oof < low_bound] = 0\n",
    "    # pred_conf_acc = pred_conf[(oof < low_bound) | (oof > high_bound)]\n",
    "    pred_conf_prec = pred_conf[(oof > high_bound)]\n",
    "    # y_conf_acc = y.values.reshape(-1,1)[(oof < low_bound) | (oof > high_bound)]\n",
    "    y_conf_prec = y.values.reshape(-1,1)[(oof > high_bound)]\n",
    "\n",
    "    return precision_score(y_conf_prec, pred_conf_prec), y_conf_prec.shape[0]/y.shape[0]\n",
    "\n",
    "train_test = 'full'\n",
    "low_bound, high_bound = 0.37, 0.63\n",
    "feature_num = 150\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'n_estimators': 2200,\n",
    "    'learning_rate': 0.02,\n",
    "    #   'early_stopping_round': 50,\n",
    "    'max_depth': 10,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'subsample': 0.85,\n",
    "    'subsample_freq': 1,\n",
    "    'num_leaves': 24,\n",
    "    'verbosity': -1,\n",
    "    'max_bin': 255,\n",
    "    'reg_alpha': 1e-6,\n",
    "    'reg_lambda': 1e-8,\n",
    "    'objective': 'binary',\n",
    "    # 'is_unbalance': True,\n",
    "    # 'class_weight': 'balanced',\n",
    "    'metric': 'average_precision'\n",
    "    }\n",
    "\n",
    "if CFG.train_LGBM:\n",
    "    fi = pd.read_csv('feature_importance.csv')\n",
    "    features, feature_dict = prepare_features(fi, feature_num)\n",
    "    oof, model = model_train(df, train_df, test_df, features, task_type=task_type, how='lgbm', n_folds=5, low_bound=low_bound, high_bound=high_bound, train_test=train_test)\n",
    "\n",
    "    if train_test == 'fold':\n",
    "        y = train_df['target']\n",
    "        oof_val_score = log_loss(y, oof)\n",
    "        oof_conf_prec_score, oof_conf_obj_pct = confident_score(y, oof, low_bound, high_bound)\n",
    "        print(f'Total fold Logloss: {oof_val_score}, Total confident objects precision: {oof_conf_prec_score}, Total % of confident objects: {oof_conf_obj_pct}')\n",
    "    elif train_test == 'test':\n",
    "        y_test = test_df['target']\n",
    "        test_val_score = log_loss(y_test, oof)\n",
    "        test_conf_prec_score, test_conf_obj_pct = confident_score(y_test, oof, low_bound, high_bound)\n",
    "        print(f'Total test Logloss: {test_val_score}, Total test confident objects precision: {test_conf_prec_score}, Total % of test confident objects: {test_conf_obj_pct}')\n",
    "    elif train_test == 'full':\n",
    "        model.booster_.save_model('lgbm.pkl')\n",
    "        joblib.dump(model, 'lgbm.pkl')\n",
    "        # save feature dictionary for further inference\n",
    "        with open(f'features.json', 'w') as f:\n",
    "            json.dump(feature_dict, f)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total fold Logloss: 0.6573018917857856, Total confident objects precision: 0.708091498805053, Total % of confident objects: 0.25252176911802743\n",
    "\n",
    "Total test Logloss: 0.6700240792619139, Total test confident objects precision: 0.688212927756654, Total % of test confident objects: 0.24374420759962928"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
